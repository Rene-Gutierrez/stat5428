<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Introduction to Probability | STAT 5428/6428 Spring 2026</title>
  <meta name="description" content="5 Introduction to Probability | STAT 5428/6428 Spring 2026" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Introduction to Probability | STAT 5428/6428 Spring 2026" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Introduction to Probability | STAT 5428/6428 Spring 2026" />
  
  
  

<meta name="author" content="Rene Gutierrez University of Texas at El Paso" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-description.html"/>
<link rel="next" href="from-probability-to-statistics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Stat 5428/6428 Spring 2026</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#weekly-course-outline"><i class="fa fa-check"></i><b>1.1</b> Weekly Course Outline</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#topical-learning-outcomes-aligned-with-blooms-taxonomy"><i class="fa fa-check"></i><b>1.2</b> Topical Learning Outcomes (Aligned with Bloom’s Taxonomy)</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#course-overview"><i class="fa fa-check"></i><b>1.3</b> Course Overview</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#section-i-data-description-and-intrinsic-properties"><i class="fa fa-check"></i><b>1.3.1</b> Section I: Data — Description and Intrinsic Properties</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#section-ii-data-modeling-probability-as-a-generative-mechanism"><i class="fa fa-check"></i><b>1.3.2</b> Section II: Data Modeling — Probability as a Generative Mechanism</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#section-iii-statistical-inference-learning-from-data-under-a-model"><i class="fa fa-check"></i><b>1.3.3</b> Section III: Statistical Inference — Learning from Data Under a Model</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#section-iv-data-remodeling-model-assessment-revision-and-alternatives"><i class="fa fa-check"></i><b>1.3.4</b> Section IV: Data Remodeling — Model Assessment, Revision, and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#course-perspective"><i class="fa fa-check"></i><b>1.4</b> Course Perspective</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro-to-r.html"><a href="intro-to-r.html"><i class="fa fa-check"></i><b>2</b> Intro to R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro-to-r.html"><a href="intro-to-r.html#basic-r-operations-and-concepts"><i class="fa fa-check"></i><b>2.1</b> Basic R Operations and Concepts</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro-to-r.html"><a href="intro-to-r.html#the-console-and-scripts"><i class="fa fa-check"></i><b>2.1.1</b> The Console and Scripts</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro-to-r.html"><a href="intro-to-r.html#comments"><i class="fa fa-check"></i><b>2.1.2</b> Comments</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro-to-r.html"><a href="intro-to-r.html#printing-and-output"><i class="fa fa-check"></i><b>2.1.3</b> Printing and Output</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro-to-r.html"><a href="intro-to-r.html#arithmetic"><i class="fa fa-check"></i><b>2.2</b> Arithmetic</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="intro-to-r.html"><a href="intro-to-r.html#order-of-operations"><i class="fa fa-check"></i><b>2.2.1</b> Order of Operations</a></li>
<li class="chapter" data-level="2.2.2" data-path="intro-to-r.html"><a href="intro-to-r.html#logical-comparisons"><i class="fa fa-check"></i><b>2.2.2</b> Logical Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="intro-to-r.html"><a href="intro-to-r.html#assignment-object-names-and-data-types"><i class="fa fa-check"></i><b>2.3</b> Assignment, Object Names, and Data Types</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="intro-to-r.html"><a href="intro-to-r.html#assignment"><i class="fa fa-check"></i><b>2.3.1</b> Assignment</a></li>
<li class="chapter" data-level="2.3.2" data-path="intro-to-r.html"><a href="intro-to-r.html#object-names"><i class="fa fa-check"></i><b>2.3.2</b> Object Names</a></li>
<li class="chapter" data-level="2.3.3" data-path="intro-to-r.html"><a href="intro-to-r.html#common-data-types"><i class="fa fa-check"></i><b>2.3.3</b> Common Data Types</a></li>
<li class="chapter" data-level="2.3.4" data-path="intro-to-r.html"><a href="intro-to-r.html#missing-values"><i class="fa fa-check"></i><b>2.3.4</b> 3.4 Missing Values</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="intro-to-r.html"><a href="intro-to-r.html#vectors"><i class="fa fa-check"></i><b>2.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro-to-r.html"><a href="intro-to-r.html#creating-vectors-with-c"><i class="fa fa-check"></i><b>2.4.1</b> Creating Vectors with <code>c()</code></a></li>
<li class="chapter" data-level="2.4.2" data-path="intro-to-r.html"><a href="intro-to-r.html#vector-length"><i class="fa fa-check"></i><b>2.4.2</b> Vector Length</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro-to-r.html"><a href="intro-to-r.html#vector-arithmetic-vectorized-operations"><i class="fa fa-check"></i><b>2.4.3</b> Vector Arithmetic (Vectorized Operations)</a></li>
<li class="chapter" data-level="2.4.4" data-path="intro-to-r.html"><a href="intro-to-r.html#sequences-with-and-seq"><i class="fa fa-check"></i><b>2.4.4</b> Sequences with <code>:</code> and <code>seq()</code></a></li>
<li class="chapter" data-level="2.4.5" data-path="intro-to-r.html"><a href="intro-to-r.html#repetition-with-rep"><i class="fa fa-check"></i><b>2.4.5</b> Repetition with <code>rep()</code></a></li>
<li class="chapter" data-level="2.4.6" data-path="intro-to-r.html"><a href="intro-to-r.html#indexing-vectors"><i class="fa fa-check"></i><b>2.4.6</b> Indexing Vectors</a></li>
<li class="chapter" data-level="2.4.7" data-path="intro-to-r.html"><a href="intro-to-r.html#named-vectors"><i class="fa fa-check"></i><b>2.4.7</b> Named Vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="intro-to-r.html"><a href="intro-to-r.html#functions-and-expressions"><i class="fa fa-check"></i><b>2.5</b> Functions and Expressions</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="intro-to-r.html"><a href="intro-to-r.html#functions"><i class="fa fa-check"></i><b>2.5.1</b> Functions</a></li>
<li class="chapter" data-level="2.5.2" data-path="intro-to-r.html"><a href="intro-to-r.html#expressions-and-nesting"><i class="fa fa-check"></i><b>2.5.2</b> Expressions and Nesting</a></li>
<li class="chapter" data-level="2.5.3" data-path="intro-to-r.html"><a href="intro-to-r.html#creating-your-own-function"><i class="fa fa-check"></i><b>2.5.3</b> Creating Your Own Function</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="intro-to-r.html"><a href="intro-to-r.html#getting-help"><i class="fa fa-check"></i><b>2.6</b> Getting Help</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="intro-to-r.html"><a href="intro-to-r.html#help-pages"><i class="fa fa-check"></i><b>2.6.1</b> Help Pages</a></li>
<li class="chapter" data-level="2.6.2" data-path="intro-to-r.html"><a href="intro-to-r.html#examples-in-help-files"><i class="fa fa-check"></i><b>2.6.2</b> Examples in Help Files</a></li>
<li class="chapter" data-level="2.6.3" data-path="intro-to-r.html"><a href="intro-to-r.html#searching-for-a-function"><i class="fa fa-check"></i><b>2.6.3</b> Searching for a Function</a></li>
<li class="chapter" data-level="2.6.4" data-path="intro-to-r.html"><a href="intro-to-r.html#inspecting-objects"><i class="fa fa-check"></i><b>2.6.4</b> Inspecting Objects</a></li>
<li class="chapter" data-level="2.6.5" data-path="intro-to-r.html"><a href="intro-to-r.html#getting-package-help"><i class="fa fa-check"></i><b>2.6.5</b> Getting Package Help</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html"><i class="fa fa-check"></i><b>3</b> Observational and Experimental Studies</a>
<ul>
<li class="chapter" data-level="3.1" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#surveys"><i class="fa fa-check"></i><b>3.2</b> Surveys</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#populations-and-sampling-frames"><i class="fa fa-check"></i><b>3.2.1</b> Populations and Sampling Frames</a></li>
<li class="chapter" data-level="3.2.2" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#sampling-methods"><i class="fa fa-check"></i><b>3.2.2</b> Sampling Methods</a></li>
<li class="chapter" data-level="3.2.3" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#sources-of-survey-bias"><i class="fa fa-check"></i><b>3.2.3</b> Sources of Survey Bias</a></li>
<li class="chapter" data-level="3.2.4" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#questionnaire-design-principles"><i class="fa fa-check"></i><b>3.2.4</b> Questionnaire Design Principles</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#experimental-studies"><i class="fa fa-check"></i><b>3.3</b> Experimental Studies</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#key-components-of-experimental-design"><i class="fa fa-check"></i><b>3.3.1</b> Key Components of Experimental Design</a></li>
<li class="chapter" data-level="3.3.2" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#confounding"><i class="fa fa-check"></i><b>3.3.2</b> Confounding</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#observational-vs-experimental-studies"><i class="fa fa-check"></i><b>3.4</b> Observational vs Experimental Studies</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#comparative-characteristics"><i class="fa fa-check"></i><b>3.4.1</b> Comparative Characteristics</a></li>
<li class="chapter" data-level="3.4.2" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#interpretational-considerations"><i class="fa fa-check"></i><b>3.4.2</b> Interpretational Considerations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#data-quality-and-measurement"><i class="fa fa-check"></i><b>3.5</b> Data Quality and Measurement</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#validity-and-reliability"><i class="fa fa-check"></i><b>3.5.1</b> Validity and Reliability</a></li>
<li class="chapter" data-level="3.5.2" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#sources-of-variability"><i class="fa fa-check"></i><b>3.5.2</b> Sources of Variability</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="observational-and-experimental-studies.html"><a href="observational-and-experimental-studies.html#summary"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>4</b> Data Description</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-description.html"><a href="data-description.html#data-simulation"><i class="fa fa-check"></i><b>4.1</b> Data Simulation</a></li>
<li class="chapter" data-level="4.2" data-path="data-description.html"><a href="data-description.html#types-of-data"><i class="fa fa-check"></i><b>4.2</b> Types of Data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="data-description.html"><a href="data-description.html#quantitative-vs.-qualitative-data"><i class="fa fa-check"></i><b>4.2.1</b> Quantitative vs. Qualitative Data</a></li>
<li class="chapter" data-level="4.2.2" data-path="data-description.html"><a href="data-description.html#quantitative-data"><i class="fa fa-check"></i><b>4.2.2</b> Quantitative Data</a></li>
<li class="chapter" data-level="4.2.3" data-path="data-description.html"><a href="data-description.html#qualitative-data"><i class="fa fa-check"></i><b>4.2.3</b> Qualitative Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="data-description.html"><a href="data-description.html#why-do-we-have-to-describe-data"><i class="fa fa-check"></i><b>4.3</b> Why Do We Have to Describe Data?</a></li>
<li class="chapter" data-level="4.4" data-path="data-description.html"><a href="data-description.html#types-of-data-description"><i class="fa fa-check"></i><b>4.4</b> Types of Data Description</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="data-description.html"><a href="data-description.html#numerical-description"><i class="fa fa-check"></i><b>4.4.1</b> Numerical Description</a></li>
<li class="chapter" data-level="4.4.2" data-path="data-description.html"><a href="data-description.html#graphical-description"><i class="fa fa-check"></i><b>4.4.2</b> Graphical Description</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="data-description.html"><a href="data-description.html#techniques-to-describe-categorical-data"><i class="fa fa-check"></i><b>4.5</b> Techniques to Describe Categorical Data</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="data-description.html"><a href="data-description.html#graphical-techniques"><i class="fa fa-check"></i><b>4.5.1</b> Graphical Techniques</a></li>
<li class="chapter" data-level="4.5.2" data-path="data-description.html"><a href="data-description.html#pie-charts"><i class="fa fa-check"></i><b>4.5.2</b> Pie Charts</a></li>
<li class="chapter" data-level="4.5.3" data-path="data-description.html"><a href="data-description.html#numerical-techniques"><i class="fa fa-check"></i><b>4.5.3</b> Numerical Techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="data-description.html"><a href="data-description.html#techniques-to-describe-quantitative-data"><i class="fa fa-check"></i><b>4.6</b> Techniques to Describe Quantitative Data</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="data-description.html"><a href="data-description.html#displaying-all-the-data"><i class="fa fa-check"></i><b>4.6.1</b> Displaying All the Data</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="data-description.html"><a href="data-description.html#frequency-table"><i class="fa fa-check"></i><b>4.7</b> Frequency Table</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="data-description.html"><a href="data-description.html#class-frequency"><i class="fa fa-check"></i><b>4.7.1</b> Class Frequency</a></li>
<li class="chapter" data-level="4.7.2" data-path="data-description.html"><a href="data-description.html#relative-frequency"><i class="fa fa-check"></i><b>4.7.2</b> Relative Frequency</a></li>
<li class="chapter" data-level="4.7.3" data-path="data-description.html"><a href="data-description.html#histograms"><i class="fa fa-check"></i><b>4.7.3</b> Histograms</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="data-description.html"><a href="data-description.html#general-guidelines-for-successful-graphics"><i class="fa fa-check"></i><b>4.8</b> General Guidelines for Successful Graphics</a></li>
<li class="chapter" data-level="4.9" data-path="data-description.html"><a href="data-description.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>4.9</b> Measures of Central Tendency</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="data-description.html"><a href="data-description.html#mode"><i class="fa fa-check"></i><b>4.9.1</b> Mode</a></li>
<li class="chapter" data-level="4.9.2" data-path="data-description.html"><a href="data-description.html#median"><i class="fa fa-check"></i><b>4.9.2</b> Median</a></li>
<li class="chapter" data-level="4.9.3" data-path="data-description.html"><a href="data-description.html#mean"><i class="fa fa-check"></i><b>4.9.3</b> Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="data-description.html"><a href="data-description.html#measures-of-variability"><i class="fa fa-check"></i><b>4.10</b> Measures of Variability</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="data-description.html"><a href="data-description.html#range"><i class="fa fa-check"></i><b>4.10.1</b> Range</a></li>
<li class="chapter" data-level="4.10.2" data-path="data-description.html"><a href="data-description.html#quantiles"><i class="fa fa-check"></i><b>4.10.2</b> Quantiles</a></li>
<li class="chapter" data-level="4.10.3" data-path="data-description.html"><a href="data-description.html#quantile-plot"><i class="fa fa-check"></i><b>4.10.3</b> Quantile Plot</a></li>
<li class="chapter" data-level="4.10.4" data-path="data-description.html"><a href="data-description.html#percentiles"><i class="fa fa-check"></i><b>4.10.4</b> Percentiles</a></li>
<li class="chapter" data-level="4.10.5" data-path="data-description.html"><a href="data-description.html#quartiles"><i class="fa fa-check"></i><b>4.10.5</b> Quartiles</a></li>
<li class="chapter" data-level="4.10.6" data-path="data-description.html"><a href="data-description.html#deviation"><i class="fa fa-check"></i><b>4.10.6</b> Deviation</a></li>
<li class="chapter" data-level="4.10.7" data-path="data-description.html"><a href="data-description.html#variance"><i class="fa fa-check"></i><b>4.10.7</b> Variance</a></li>
<li class="chapter" data-level="4.10.8" data-path="data-description.html"><a href="data-description.html#standard-deviation"><i class="fa fa-check"></i><b>4.10.8</b> Standard Deviation</a></li>
<li class="chapter" data-level="4.10.9" data-path="data-description.html"><a href="data-description.html#absolute-deviation"><i class="fa fa-check"></i><b>4.10.9</b> Absolute Deviation</a></li>
<li class="chapter" data-level="4.10.10" data-path="data-description.html"><a href="data-description.html#median-absolute-deviation-mad"><i class="fa fa-check"></i><b>4.10.10</b> Median Absolute Deviation (MAD)</a></li>
<li class="chapter" data-level="4.10.11" data-path="data-description.html"><a href="data-description.html#and-99.7-empirical-rule"><i class="fa fa-check"></i><b>4.10.11</b> 68%, 95%, and 99.7% Empirical Rule</a></li>
<li class="chapter" data-level="4.10.12" data-path="data-description.html"><a href="data-description.html#coefficient-of-variation"><i class="fa fa-check"></i><b>4.10.12</b> Coefficient of Variation</a></li>
<li class="chapter" data-level="4.10.13" data-path="data-description.html"><a href="data-description.html#the-boxplot"><i class="fa fa-check"></i><b>4.10.13</b> The Boxplot</a></li>
<li class="chapter" data-level="4.10.14" data-path="data-description.html"><a href="data-description.html#outliers"><i class="fa fa-check"></i><b>4.10.14</b> Outliers</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="data-description.html"><a href="data-description.html#techniques-for-two-variables"><i class="fa fa-check"></i><b>4.11</b> Techniques for Two Variables</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="data-description.html"><a href="data-description.html#contingency-tables"><i class="fa fa-check"></i><b>4.11.1</b> Contingency Tables</a></li>
<li class="chapter" data-level="4.11.2" data-path="data-description.html"><a href="data-description.html#stacked-bar-graph"><i class="fa fa-check"></i><b>4.11.2</b> Stacked Bar Graph</a></li>
<li class="chapter" data-level="4.11.3" data-path="data-description.html"><a href="data-description.html#cluster-bar-graph"><i class="fa fa-check"></i><b>4.11.3</b> Cluster Bar Graph</a></li>
<li class="chapter" data-level="4.11.4" data-path="data-description.html"><a href="data-description.html#scatter-plot"><i class="fa fa-check"></i><b>4.11.4</b> Scatter Plot</a></li>
<li class="chapter" data-level="4.11.5" data-path="data-description.html"><a href="data-description.html#correlation-coefficient"><i class="fa fa-check"></i><b>4.11.5</b> Correlation Coefficient</a></li>
<li class="chapter" data-level="4.11.6" data-path="data-description.html"><a href="data-description.html#block-boxplots"><i class="fa fa-check"></i><b>4.11.6</b> Block Boxplots</a></li>
<li class="chapter" data-level="4.11.7" data-path="data-description.html"><a href="data-description.html#block-scatter-plots"><i class="fa fa-check"></i><b>4.11.7</b> Block Scatter Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html"><i class="fa fa-check"></i><b>5</b> Introduction to Probability</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#probability-as-a-model-for-uncertain-and-complex-phenomena"><i class="fa fa-check"></i><b>5.1</b> Probability as a Model for Uncertain and Complex Phenomena</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#probability-as-a-dataproducing-machine"><i class="fa fa-check"></i><b>5.1.1</b> Probability as a Data‑Producing Machine</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#example-of-an-uncertain-phenomenon"><i class="fa fa-check"></i><b>5.1.2</b> Example of an Uncertain Phenomenon</a></li>
<li class="chapter" data-level="5.1.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#example-of-a-complex-phenomenon"><i class="fa fa-check"></i><b>5.1.3</b> Example of a Complex Phenomenon</a></li>
<li class="chapter" data-level="5.1.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#using-models-to-explain-phenomena"><i class="fa fa-check"></i><b>5.1.4</b> Using Models to Explain Phenomena</a></li>
<li class="chapter" data-level="5.1.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#final-idea-inference-without-a-model"><i class="fa fa-check"></i><b>5.1.5</b> Final Idea: Inference Without a Model</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#interpretations-of-probability"><i class="fa fa-check"></i><b>5.2</b> Interpretations of Probability</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#classical-interpretation"><i class="fa fa-check"></i><b>5.2.1</b> Classical Interpretation</a></li>
<li class="chapter" data-level="5.2.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#relative-frequency-interpretation"><i class="fa fa-check"></i><b>5.2.2</b> Relative Frequency Interpretation</a></li>
<li class="chapter" data-level="5.2.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#subjective-interpretation"><i class="fa fa-check"></i><b>5.2.3</b> Subjective Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#how-to-build-probability-models"><i class="fa fa-check"></i><b>5.3</b> How to Build Probability Models</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#finding-the-probability-of-an-event"><i class="fa fa-check"></i><b>5.3.1</b> Finding the Probability of an Event</a></li>
<li class="chapter" data-level="5.3.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#basic-event-relations-and-probability-laws"><i class="fa fa-check"></i><b>5.3.2</b> Basic Event Relations and Probability Laws</a></li>
<li class="chapter" data-level="5.3.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#properties-of-probability"><i class="fa fa-check"></i><b>5.3.3</b> Properties of Probability</a></li>
<li class="chapter" data-level="5.3.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#conditional-probability"><i class="fa fa-check"></i><b>5.3.4</b> Conditional Probability</a></li>
<li class="chapter" data-level="5.3.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#law-of-total-probability"><i class="fa fa-check"></i><b>5.3.5</b> Law of Total Probability</a></li>
<li class="chapter" data-level="5.3.6" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#bayes-formula"><i class="fa fa-check"></i><b>5.3.6</b> Bayes’ Formula</a></li>
<li class="chapter" data-level="5.3.7" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#modeling-perspective-summary"><i class="fa fa-check"></i><b>5.3.7</b> Modeling Perspective Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#summary-first-part-intro-to-probability"><i class="fa fa-check"></i><b>5.4</b> Summary First Part: Intro to Probability</a></li>
<li class="chapter" data-level="5.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#independence-of-events"><i class="fa fa-check"></i><b>5.5</b> Independence of events</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#definition"><i class="fa fa-check"></i><b>5.5.1</b> Definition</a></li>
<li class="chapter" data-level="5.5.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#example-1-independent-events"><i class="fa fa-check"></i><b>5.5.2</b> Example 1: Independent Events</a></li>
<li class="chapter" data-level="5.5.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#example-2-independent-events-card-draws"><i class="fa fa-check"></i><b>5.5.3</b> Example 2: Independent Events (Card Draws)</a></li>
<li class="chapter" data-level="5.5.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#example-of-dependent-events"><i class="fa fa-check"></i><b>5.5.4</b> Example of Dependent Events</a></li>
<li class="chapter" data-level="5.5.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#independence-vs.-mutually-exclusive"><i class="fa fa-check"></i><b>5.5.5</b> Independence vs. Mutually Exclusive</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#random-variables"><i class="fa fa-check"></i><b>5.6</b> Random Variables</a></li>
<li class="chapter" data-level="5.7" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>5.7</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#introducing-rvs-as-convenient-labels-for-outcomes"><i class="fa fa-check"></i><b>5.7.1</b> Introducing RVs as Convenient Labels for Outcomes</a></li>
<li class="chapter" data-level="5.7.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#qualitative-random-variable"><i class="fa fa-check"></i><b>5.7.2</b> Qualitative Random Variable</a></li>
<li class="chapter" data-level="5.7.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#quantitative-random-variables"><i class="fa fa-check"></i><b>5.7.3</b> Quantitative Random Variables</a></li>
<li class="chapter" data-level="5.7.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#probability-distribution-for-a-discrete-random-variable"><i class="fa fa-check"></i><b>5.7.4</b> Probability Distribution for a Discrete Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#important-discrete-distributions"><i class="fa fa-check"></i><b>5.8</b> Important Discrete Distributions</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#the-binomial-distribution"><i class="fa fa-check"></i><b>5.8.1</b> The Binomial Distribution</a></li>
<li class="chapter" data-level="5.8.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#the-poisson-distribution"><i class="fa fa-check"></i><b>5.8.2</b> The Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>5.9</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#from-discrete-to-continuous"><i class="fa fa-check"></i><b>5.9.1</b> From Discrete to Continuous</a></li>
<li class="chapter" data-level="5.9.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#pdf-and-cdf"><i class="fa fa-check"></i><b>5.9.2</b> PDF and CDF</a></li>
<li class="chapter" data-level="5.9.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#interpreting-areas-as-probabilities"><i class="fa fa-check"></i><b>5.9.3</b> Interpreting Areas as Probabilities</a></li>
<li class="chapter" data-level="5.9.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#why-the-pdf-can-be-greater-than-1"><i class="fa fa-check"></i><b>5.9.4</b> Why the PDF Can Be Greater Than 1</a></li>
<li class="chapter" data-level="5.9.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#from-simulation-to-density"><i class="fa fa-check"></i><b>5.9.5</b> From Simulation to Density</a></li>
<li class="chapter" data-level="5.9.6" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#computing-interval-probabilities-in-r"><i class="fa fa-check"></i><b>5.9.6</b> Computing Interval Probabilities in R</a></li>
<li class="chapter" data-level="5.9.7" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#key-conceptual-shift"><i class="fa fa-check"></i><b>5.9.7</b> Key Conceptual Shift</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#important-continuous-distributions"><i class="fa fa-check"></i><b>5.10</b> Important Continuous Distributions</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#the-uniform-distribution"><i class="fa fa-check"></i><b>5.10.1</b> The Uniform Distribution</a></li>
<li class="chapter" data-level="5.10.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#the-normal-distribution"><i class="fa fa-check"></i><b>5.10.2</b> The Normal Distribution</a></li>
<li class="chapter" data-level="5.10.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#chi-squared-distribution"><i class="fa fa-check"></i><b>5.10.3</b> Chi-Squared Distribution</a></li>
<li class="chapter" data-level="5.10.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#t-distribution"><i class="fa fa-check"></i><b>5.10.4</b> t Distribution</a></li>
<li class="chapter" data-level="5.10.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#f-distribution"><i class="fa fa-check"></i><b>5.10.5</b> F Distribution</a></li>
<li class="chapter" data-level="5.10.6" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#summary-1"><i class="fa fa-check"></i><b>5.10.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#the-mean-of-a-random-variable"><i class="fa fa-check"></i><b>5.11</b> The Mean of a Random Variable</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#interpreting-the-mean"><i class="fa fa-check"></i><b>5.11.1</b> Interpreting the Mean</a></li>
<li class="chapter" data-level="5.11.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#mean-as-a-distribution-parameter-vs-not"><i class="fa fa-check"></i><b>5.11.2</b> Mean as a Distribution Parameter vs Not</a></li>
<li class="chapter" data-level="5.11.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#simulation-example-mean-of-a-binomial-distribution"><i class="fa fa-check"></i><b>5.11.3</b> Simulation Example: Mean of a Binomial Distribution</a></li>
<li class="chapter" data-level="5.11.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#the-mean-as-a-weighted-average"><i class="fa fa-check"></i><b>5.11.4</b> The Mean as a Weighted Average</a></li>
<li class="chapter" data-level="5.11.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#properties-of-the-mean-operator"><i class="fa fa-check"></i><b>5.11.5</b> Properties of the Mean Operator</a></li>
<li class="chapter" data-level="5.11.6" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#why-the-mean-matters"><i class="fa fa-check"></i><b>5.11.6</b> Why the Mean Matters</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#the-variance-of-a-random-variable"><i class="fa fa-check"></i><b>5.12</b> The Variance of a Random Variable</a>
<ul>
<li class="chapter" data-level="5.12.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#interpreting-variance"><i class="fa fa-check"></i><b>5.12.1</b> Interpreting Variance</a></li>
<li class="chapter" data-level="5.12.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#variance-as-a-distribution-parameter-vs-derived-quantity"><i class="fa fa-check"></i><b>5.12.2</b> Variance as a Distribution Parameter vs Derived Quantity</a></li>
<li class="chapter" data-level="5.12.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#simulation-based-understanding-of-variance"><i class="fa fa-check"></i><b>5.12.3</b> Simulation-Based Understanding of Variance</a></li>
<li class="chapter" data-level="5.12.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#properties-of-variance"><i class="fa fa-check"></i><b>5.12.4</b> Properties of Variance</a></li>
<li class="chapter" data-level="5.12.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#summary-2"><i class="fa fa-check"></i><b>5.12.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#conditional-expectation"><i class="fa fa-check"></i><b>5.13</b> Conditional Expectation</a>
<ul>
<li class="chapter" data-level="5.13.1" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#discrete-case"><i class="fa fa-check"></i><b>5.13.1</b> Discrete Case</a></li>
<li class="chapter" data-level="5.13.2" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#continuous-case"><i class="fa fa-check"></i><b>5.13.2</b> Continuous Case</a></li>
<li class="chapter" data-level="5.13.3" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#interpretation"><i class="fa fa-check"></i><b>5.13.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.13.4" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#simple-example"><i class="fa fa-check"></i><b>5.13.4</b> Simple Example</a></li>
<li class="chapter" data-level="5.13.5" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#simulation-illustration"><i class="fa fa-check"></i><b>5.13.5</b> Simulation Illustration</a></li>
<li class="chapter" data-level="5.13.6" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#key-properties"><i class="fa fa-check"></i><b>5.13.6</b> Key Properties</a></li>
<li class="chapter" data-level="5.13.7" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#conceptual-importance"><i class="fa fa-check"></i><b>5.13.7</b> Conceptual Importance</a></li>
<li class="chapter" data-level="5.13.8" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#law-of-total-expectation"><i class="fa fa-check"></i><b>5.13.8</b> Law of Total Expectation</a></li>
<li class="chapter" data-level="5.13.9" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#simulation-example-2"><i class="fa fa-check"></i><b>5.13.9</b> Simulation Example</a></li>
<li class="chapter" data-level="5.13.10" data-path="introduction-to-probability.html"><a href="introduction-to-probability.html#key-insight"><i class="fa fa-check"></i><b>5.13.10</b> Key Insight</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html"><i class="fa fa-check"></i><b>6</b> From Probability to Statistics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#population-as-a-distribution"><i class="fa fa-check"></i><b>6.1</b> Population as a Distribution</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#the-structural-framework"><i class="fa fa-check"></i><b>6.1.1</b> The Structural Framework</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#examples-1"><i class="fa fa-check"></i><b>6.2</b> Examples</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#example-1-a-normal-population"><i class="fa fa-check"></i><b>6.2.1</b> Example 1: A Normal Population</a></li>
<li class="chapter" data-level="6.2.2" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#example-2-a-discrete-population"><i class="fa fa-check"></i><b>6.2.2</b> Example 2: A Discrete Population</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#what-does-bias-mean-under-this-framework"><i class="fa fa-check"></i><b>6.3</b> What Does Bias Mean Under This Framework?</a></li>
<li class="chapter" data-level="6.4" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#role-of-independence"><i class="fa fa-check"></i><b>6.4</b> Role of Independence</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#independent-sampling-with-replacement"><i class="fa fa-check"></i><b>6.4.1</b> Independent Sampling (With Replacement)</a></li>
<li class="chapter" data-level="6.4.2" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#deterministic-sequence-no-randomness"><i class="fa fa-check"></i><b>6.4.2</b> Deterministic Sequence (No Randomness)</a></li>
<li class="chapter" data-level="6.4.3" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#dependent-example-random-walk"><i class="fa fa-check"></i><b>6.4.3</b> Dependent Example: Random Walk</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#sampling-as-drawing-random-variables"><i class="fa fa-check"></i><b>6.5</b> Sampling as Drawing Random Variables</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#i.i.d.-assumption"><i class="fa fa-check"></i><b>6.5.1</b> i.i.d. Assumption</a></li>
<li class="chapter" data-level="6.5.2" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#sampling-without-replacement"><i class="fa fa-check"></i><b>6.5.2</b> Sampling Without Replacement</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#parameters-and-statistics"><i class="fa fa-check"></i><b>6.6</b> Parameters and Statistics</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#simulation-illustration-1"><i class="fa fa-check"></i><b>6.6.1</b> Simulation Illustration</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#example-random-sample-as-simple-random-sampling"><i class="fa fa-check"></i><b>6.7</b> Example: Random Sample as Simple Random Sampling</a></li>
<li class="chapter" data-level="6.8" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#sampling-distribution-of-a-mean"><i class="fa fa-check"></i><b>6.8</b> Sampling Distribution of a Mean</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#fundamental-properties"><i class="fa fa-check"></i><b>6.8.1</b> Fundamental Properties</a></li>
<li class="chapter" data-level="6.8.2" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#small-vs-large-sample-size"><i class="fa fa-check"></i><b>6.8.2</b> Small vs Large Sample Size</a></li>
<li class="chapter" data-level="6.8.3" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#big-vs-small-variance"><i class="fa fa-check"></i><b>6.8.3</b> Big vs Small Variance</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#shape-of-the-sampling-distribution"><i class="fa fa-check"></i><b>6.9</b> Shape of the Sampling Distribution</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#binomial-example"><i class="fa fa-check"></i><b>6.9.1</b> Binomial Example</a></li>
<li class="chapter" data-level="6.9.2" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#poisson-example"><i class="fa fa-check"></i><b>6.9.2</b> Poisson Example</a></li>
<li class="chapter" data-level="6.9.3" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#normal-example"><i class="fa fa-check"></i><b>6.9.3</b> Normal Example</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#sampling-distribution-of-a-variance"><i class="fa fa-check"></i><b>6.10</b> Sampling Distribution of a Variance</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#small-vs-large-sample-size-1"><i class="fa fa-check"></i><b>6.10.1</b> Small vs Large Sample Size</a></li>
<li class="chapter" data-level="6.10.2" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#big-vs-small-variance-1"><i class="fa fa-check"></i><b>6.10.2</b> Big vs Small Variance</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#concentration-with-larger-samples"><i class="fa fa-check"></i><b>6.11</b> Concentration with Larger Samples</a></li>
<li class="chapter" data-level="6.12" data-path="from-probability-to-statistics.html"><a href="from-probability-to-statistics.html#central-insight"><i class="fa fa-check"></i><b>6.12</b> Central Insight</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 5428/6428 Spring 2026</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-probability" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Introduction to Probability<a href="introduction-to-probability.html#introduction-to-probability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Probability provides a <strong>mathematical framework</strong> for reasoning about uncertainty and complexity.<br />
In this lecture, we develop probability ideas using <strong>simulation and computation</strong>, treating probability models as <em>data‑producing mechanisms</em>. This approach emphasizes understanding over memorization and mirrors how probability is used in modern data analysis.</p>
<hr />
<div id="probability-as-a-model-for-uncertain-and-complex-phenomena" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Probability as a Model for Uncertain and Complex Phenomena<a href="introduction-to-probability.html#probability-as-a-model-for-uncertain-and-complex-phenomena" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="probability-as-a-dataproducing-machine" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Probability as a Data‑Producing Machine<a href="introduction-to-probability.html#probability-as-a-dataproducing-machine" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:prob-model" class="definition"><strong>Definition 5.1  (Probability Model) </strong></span>A probability model is a mathematical description of a process that produces data under uncertainty, assigning probabilities to possible outcomes.</p>
</div>
<p>Rather than viewing probability as an abstract formula, we interpret it as a <strong>machine that generates data</strong>.<br />
Once a model is specified, we can simulate outcomes and study their long‑run behavior.</p>
<div id="example-a-simple-dataproducing-machine" class="section level4 hasAnchor" number="5.1.1.1">
<h4><span class="header-section-number">5.1.1.1</span> Example: A Simple Data‑Producing Machine<a href="introduction-to-probability.html#example-a-simple-dataproducing-machine" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="introduction-to-probability.html#cb176-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb176-2"><a href="introduction-to-probability.html#cb176-2" tabindex="-1"></a><span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&quot;Heads&quot;</span>, <span class="st">&quot;Tails&quot;</span>), <span class="at">size =</span> <span class="dv">10</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>))</span></code></pre></div>
<pre><code>##  [1] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot;</code></pre>
<p>Each run produces different data, yet the underlying mechanism remains fixed.</p>
<hr />
</div>
</div>
<div id="example-of-an-uncertain-phenomenon" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Example of an Uncertain Phenomenon<a href="introduction-to-probability.html#example-of-an-uncertain-phenomenon" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Uncertain phenomena are characterized by <strong>random variation</strong>, even when conditions appear identical.</p>
<p>Example:</p>
<ul>
<li>Tossing a fair coin</li>
<li>Rolling a die</li>
<li>Drawing a card from a shuffled deck</li>
</ul>
<hr />
</div>
<div id="example-of-a-complex-phenomenon" class="section level3 hasAnchor" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Example of a Complex Phenomenon<a href="introduction-to-probability.html#example-of-a-complex-phenomenon" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Complex phenomena involve many interacting components:</p>
<ul>
<li>Student performance</li>
<li>Financial markets</li>
<li>Weather systems</li>
</ul>
<p>Even if deterministic rules exist, the combined system behaves unpredictably.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="introduction-to-probability.html#cb178-1" tabindex="-1"></a><span class="co"># Function Describing Population Growth</span></span>
<span id="cb178-2"><a href="introduction-to-probability.html#cb178-2" tabindex="-1"></a>populationGrowth <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">x =</span> <span class="dv">1</span>, <span class="at">r =</span> <span class="dv">1</span>, <span class="at">t =</span> <span class="dv">1</span>){</span>
<span id="cb178-3"><a href="introduction-to-probability.html#cb178-3" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>t){</span>
<span id="cb178-4"><a href="introduction-to-probability.html#cb178-4" tabindex="-1"></a>    x <span class="ot">&lt;-</span> r <span class="sc">*</span> x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> x)</span>
<span id="cb178-5"><a href="introduction-to-probability.html#cb178-5" tabindex="-1"></a>  }</span>
<span id="cb178-6"><a href="introduction-to-probability.html#cb178-6" tabindex="-1"></a>  <span class="fu">return</span>(x)</span>
<span id="cb178-7"><a href="introduction-to-probability.html#cb178-7" tabindex="-1"></a>}</span>
<span id="cb178-8"><a href="introduction-to-probability.html#cb178-8" tabindex="-1"></a><span class="co"># Samples obtained at different initial states for 10 periods</span></span>
<span id="cb178-9"><a href="introduction-to-probability.html#cb178-9" tabindex="-1"></a>griSta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="fl">0.9</span>, <span class="at">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb178-10"><a href="introduction-to-probability.html#cb178-10" tabindex="-1"></a>samPop <span class="ot">&lt;-</span> <span class="fu">populationGrowth</span>(<span class="at">x =</span> griSta, <span class="at">r =</span> <span class="dv">4</span>, <span class="at">t =</span> <span class="dv">100</span>)</span>
<span id="cb178-11"><a href="introduction-to-probability.html#cb178-11" tabindex="-1"></a><span class="co"># Print Table</span></span>
<span id="cb178-12"><a href="introduction-to-probability.html#cb178-12" tabindex="-1"></a><span class="fu">names</span>(samPop) <span class="ot">&lt;-</span> griSta</span>
<span id="cb178-13"><a href="introduction-to-probability.html#cb178-13" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(samPop, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>##   0.1 0.101 0.102 0.103 0.104 0.105 0.106 0.107 0.108 0.109  0.11 0.111 0.112 0.113 0.114 0.115 0.116 0.117 0.118 0.119  0.12 0.121 
##  0.37  0.44  1.00  0.15  0.68  0.01  0.17  0.43  0.52  0.09  0.88  0.39  0.09  0.65  0.56  0.00  0.86  0.55  0.45  0.29  0.27  0.07 
## 0.122 0.123 0.124 0.125 0.126 0.127 0.128 0.129  0.13 0.131 0.132 0.133 0.134 0.135 0.136 0.137 0.138 0.139  0.14 0.141 0.142 0.143 
##  0.99  0.84  0.43  0.78  0.82  1.00  0.66  0.93  0.96  0.96  0.26  1.00  0.19  0.48  0.54  1.00  0.06  0.08  0.77  0.52  0.16  0.89 
## 0.144 0.145 0.146 0.147 0.148 0.149  0.15 0.151 0.152 0.153 0.154 0.155 0.156 0.157 0.158 0.159  0.16 0.161 0.162 0.163 0.164 0.165 
##  0.12  0.14  0.96  0.71  0.94  0.75  0.64  0.87  0.00  0.35  0.38  0.80  0.93  0.10  0.27  0.83  0.74  0.22  0.03  0.16  0.41  0.79 
## 0.166 0.167 0.168 0.169  0.17 0.171 0.172 0.173 0.174 0.175 0.176 0.177 0.178 0.179  0.18 0.181 0.182 0.183 0.184 0.185 0.186 0.187 
##  0.00  0.70  0.37  0.14  0.50  0.94  0.14  0.32  0.31  0.59  0.58  0.99  0.33  0.71  0.92  0.96  0.60  0.86  0.09  0.59  0.90  0.97 
## 0.188 0.189  0.19 0.191 0.192 0.193 0.194 0.195 0.196 0.197 0.198 0.199   0.2 0.201 0.202 0.203 0.204 0.205 0.206 0.207 0.208 0.209 
##  0.02  0.17  0.14  0.28  0.01  0.46  0.82  1.00  0.51  0.96  0.97  1.00  0.88  0.20  0.72  0.60  0.02  0.02  0.13  0.01  0.56  0.94 
##  0.21 0.211 0.212 0.213 0.214 0.215 0.216 0.217 0.218 0.219  0.22 0.221 0.222 0.223 0.224 0.225 0.226 0.227 0.228 0.229  0.23 0.231 
##  0.12  0.96  0.32  0.86  0.34  0.27  0.79  0.34  0.22  0.26  0.71  0.94  0.27  0.00  0.78  0.59  0.24  0.78  0.03  0.04  0.07  0.22 
## 0.232 0.233 0.234 0.235 0.236 0.237 0.238 0.239  0.24 0.241 0.242 0.243 0.244 0.245 0.246 0.247 0.248 0.249  0.25 0.251 0.252 0.253 
##  0.79  1.00  0.32  0.43  0.08  0.23  0.99  0.18  0.76  0.78  0.32  0.92  0.00  0.45  0.65  0.73  1.00  0.97  0.75  0.84  0.86  0.16 
## 0.254 0.255 0.256 0.257 0.258 0.259  0.26 0.261 0.262 0.263 0.264 0.265 0.266 0.267 0.268 0.269  0.27 0.271 0.272 0.273 0.274 0.275 
##  0.85  0.00  0.44  0.46  1.00  0.34  0.69  1.00  0.04  0.00  0.76  0.37  0.00  0.94  0.76  0.01  0.99  0.43  0.02  0.78  0.42  0.97 
## 0.276 0.277 0.278 0.279  0.28 0.281 0.282 0.283 0.284 0.285 0.286 0.287 0.288 0.289  0.29 0.291 0.292 0.293 0.294 0.295 0.296 0.297 
##  0.83  0.96  0.97  0.02  0.54  0.37  0.34  0.48  0.90  0.17  0.37  0.80  0.88  0.01  0.29  0.92  0.81  1.00  0.44  0.24  0.07  0.42 
## 0.298 0.299   0.3 0.301 0.302 0.303 0.304 0.305 0.306 0.307 0.308 0.309  0.31 0.311 0.312 0.313 0.314 0.315 0.316 0.317 0.318 0.319 
##  0.93  0.80  0.75  0.16  0.08  0.16  0.04  0.18  0.30  0.85  1.00  1.00  0.02  0.83  0.03  0.03  0.95  0.51  0.03  0.97  0.81  0.33 
##  0.32 0.321 0.322 0.323 0.324 0.325 0.326 0.327 0.328 0.329  0.33 0.331 0.332 0.333 0.334 0.335 0.336 0.337 0.338 0.339  0.34 0.341 
##  0.34  0.84  0.11  0.16  0.40  0.46  0.03  0.53  0.02  0.05  0.00  0.63  0.18  0.70  0.45  0.99  0.41  0.49  0.39  0.24  0.10  0.38 
## 0.342 0.343 0.344 0.345 0.346 0.347 0.348 0.349  0.35 0.351 0.352 0.353 0.354 0.355 0.356 0.357 0.358 0.359  0.36 0.361 0.362 0.363 
##  0.84  1.00  0.90  0.00  0.13  0.05  0.76  0.73  1.00  0.58  0.31  0.00  0.56  0.41  0.56  0.81  1.00  0.61  0.93  0.88  0.23  1.00 
## 0.364 0.365 0.366 0.367 0.368 0.369  0.37 0.371 0.372 0.373 0.374 0.375 0.376 0.377 0.378 0.379  0.38 0.381 0.382 0.383 0.384 0.385 
##  0.36  0.00  0.98  0.01  0.36  0.67  0.97  0.26  0.00  0.79  0.98  0.29  0.48  0.87  0.15  0.08  0.86  0.61  0.30  0.12  0.07  0.17 
## 0.386 0.387 0.388 0.389  0.39 0.391 0.392 0.393 0.394 0.395 0.396 0.397 0.398 0.399   0.4 0.401 0.402 0.403 0.404 0.405 0.406 0.407 
##  0.14  0.69  0.79  0.47  0.76  0.44  0.58  1.00  0.69  0.08  0.73  1.00  0.23  0.92  0.24  0.86  0.07  0.52  0.91  0.91  0.20  0.56 
## 0.408 0.409  0.41 0.411 0.412 0.413 0.414 0.415 0.416 0.417 0.418 0.419  0.42 0.421 0.422 0.423 0.424 0.425 0.426 0.427 0.428 0.429 
##  0.47  0.40  0.21  0.60  0.41  0.11  0.44  0.97  0.30  0.87  0.67  0.93  0.11  0.70  0.43  0.74  0.23  0.66  0.52  0.85  0.83  0.64 
##  0.43 0.431 0.432 0.433 0.434 0.435 0.436 0.437 0.438 0.439  0.44 0.441 0.442 0.443 0.444 0.445 0.446 0.447 0.448 0.449  0.45 0.451 
##  0.68  0.10  0.15  0.91  0.74  0.03  0.90  0.97  1.00  0.11  0.86  0.01  0.58  0.61  0.59  0.17  0.68  0.28  0.27  0.13  0.06  0.66 
## 0.452 0.453 0.454 0.455 0.456 0.457 0.458 0.459  0.46 0.461 0.462 0.463 0.464 0.465 0.466 0.467 0.468 0.469  0.47 0.471 0.472 0.473 
##  0.94  0.92  0.07  0.01  0.17  0.35  0.26  0.07  0.45  0.04  0.00  0.00  0.99  0.86  0.98  0.95  0.61  0.69  0.17  0.59  0.53  0.03 
## 0.474 0.475 0.476 0.477 0.478 0.479  0.48 0.481 0.482 0.483 0.484 0.485 0.486 0.487 0.488 0.489  0.49 0.491 0.492 0.493 0.494 0.495 
##  0.46  0.02  0.05  0.25  0.97  0.95  0.79  0.55  0.04  0.59  0.02  0.00  0.39  0.00  0.29  0.89  0.92  0.11  0.60  0.27  1.00  0.16 
## 0.496 0.497 0.498 0.499   0.5 0.501 0.502 0.503 0.504 0.505 0.506 0.507 0.508 0.509  0.51 0.511 0.512 0.513 0.514 0.515 0.516 0.517 
##  0.93  0.40  0.94  0.01  0.00  0.01  0.94  0.40  0.93  0.16  1.00  0.27  0.60  0.11  0.92  0.89  0.29  0.00  0.39  0.00  0.02  0.59 
## 0.518 0.519  0.52 0.521 0.522 0.523 0.524 0.525 0.526 0.527 0.528 0.529  0.53 0.531 0.532 0.533 0.534 0.535 0.536 0.537 0.538 0.539 
##  0.04  0.55  0.79  0.95  0.97  0.25  0.05  0.02  0.46  0.03  0.53  0.59  0.17  0.69  0.61  0.95  0.98  0.86  0.99  0.00  0.00  0.04 
##  0.54 0.541 0.542 0.543 0.544 0.545 0.546 0.547 0.548 0.549  0.55 0.551 0.552 0.553 0.554 0.555 0.556 0.557 0.558 0.559  0.56 0.561 
##  0.45  0.07  0.26  0.35  0.17  0.01  0.07  0.92  0.94  0.75  0.06  0.13  0.61  0.28  0.68  0.17  0.59  0.61  0.58  0.96  0.04  0.11 
## 0.562 0.563 0.564 0.565 0.566 0.567 0.568 0.569  0.57 0.571 0.572 0.573 0.574 0.575 0.576 0.577 0.578 0.579  0.58 0.581 0.582 0.583 
##  0.54  0.97  0.58  0.03  0.74  0.32  0.34  0.06  0.94  0.44  0.89  0.85  0.50  0.18  0.23  0.63  0.43  0.68  0.11  0.93  0.67  0.87 
## 0.584 0.585 0.586 0.587 0.588 0.589  0.59 0.591 0.592 0.593 0.594 0.595 0.596 0.597 0.598 0.599   0.6 0.601 0.602 0.603 0.604 0.605 
##  0.30  0.97  0.44  0.11  0.41  0.60  0.21  0.40  0.47  0.56  0.20  0.91  0.91  0.52  0.07  0.86  0.24  0.92  0.23  1.00  0.73  0.08 
## 0.606 0.607 0.608 0.609  0.61 0.611 0.612 0.613 0.614 0.615 0.616 0.617 0.618 0.619  0.62 0.621 0.622 0.623 0.624 0.625 0.626 0.627 
##  0.69  1.00  0.58  0.44  0.76  0.47  0.79  0.69  0.14  0.17  0.07  0.12  0.30  0.61  0.86  0.08  0.15  0.87  0.48  0.29  0.98  0.79 
## 0.628 0.629  0.63 0.631 0.632 0.633 0.634 0.635 0.636 0.637 0.638 0.639  0.64 0.641 0.642 0.643 0.644 0.645 0.646 0.647 0.648 0.649 
##  0.00  0.26  0.97  0.67  0.36  0.01  0.98  0.00  0.36  1.00  0.23  0.88  0.93  0.61  1.00  0.81  0.56  0.41  0.56  0.00  0.31  0.58 
##  0.65 0.651 0.652 0.653 0.654 0.655 0.656 0.657 0.658 0.659  0.66 0.661 0.662 0.663 0.664 0.665 0.666 0.667 0.668 0.669  0.67 0.671 
##  1.00  0.73  0.76  0.05  0.13  0.00  0.90  1.00  0.84  0.38  0.10  0.41  0.39  0.71  0.41  0.82  0.45  0.38  0.18  0.00  0.00  0.97 
## 0.672 0.673 0.674 0.675 0.676 0.677 0.678 0.679  0.68 0.681 0.682 0.683 0.684 0.685 0.686 0.687 0.688 0.689  0.69 0.691 0.692 0.693 
##  0.02  0.98  0.03  0.46  0.40  0.16  0.02  0.84  0.34  0.33  0.02  0.97  0.09  0.51  0.25  0.03  0.21  0.83  1.00  1.00  1.00  0.85 
## 0.694 0.695 0.696 0.697 0.698 0.699   0.7 0.701 0.702 0.703 0.704 0.705 0.706 0.707 0.708 0.709  0.71 0.711 0.712 0.713 0.714 0.715 
##  0.30  0.18  0.04  0.16  0.08  0.16  0.75  0.80  0.93  0.42  0.07  0.24  0.44  1.00  0.81  0.92  0.29  0.01  0.88  0.80  0.37  0.17 
## 0.716 0.717 0.718 0.719  0.72 0.721 0.722 0.723 0.724 0.725 0.726 0.727 0.728 0.729  0.73 0.731 0.732 0.733 0.734 0.735 0.736 0.737 
##  0.90  0.48  0.34  0.37  0.54  0.02  0.97  0.96  0.83  0.97  0.42  0.78  0.02  0.43  0.99  0.01  0.76  0.94  0.00  0.37  0.76  0.00 
## 0.738 0.739  0.74 0.741 0.742 0.743 0.744 0.745 0.746 0.747 0.748 0.749  0.75 0.751 0.752 0.753 0.754 0.755 0.756 0.757 0.758 0.759 
##  0.04  1.00  0.69  0.34  1.00  0.46  0.44  0.00  0.85  0.16  0.86  0.84  0.75  0.97  1.00  0.73  0.65  0.45  0.00  0.69  0.68  0.56 
##  0.76 0.761 0.762 0.763 0.764 0.765 0.766 0.767 0.768 0.769  0.77 0.771 0.772 0.773 0.774 0.775 0.776 0.777 0.778 0.779  0.78 0.781 
##  0.76  0.92  0.74  0.20  0.24  1.00  0.78  0.78  0.79  0.88  0.17  0.03  1.00  0.84  0.24  0.71  0.73  0.00  0.77  0.69  0.01  0.26 
## 0.782 0.783 0.784 0.785 0.786 0.787 0.788 0.789  0.79 0.791 0.792 0.793 0.794 0.795 0.796 0.797 0.798 0.799   0.8 0.801 0.802 0.803 
##  0.22  1.00  0.00  0.14  0.73  0.00  0.87  0.96  0.75  0.02  0.10  0.01  0.43  0.70  0.54  0.84  0.11  0.20  0.55  1.00  0.56  1.00 
## 0.804 0.805 0.806 0.807 0.808 0.809  0.81 0.811 0.812 0.813 0.814 0.815 0.816 0.817 0.818 0.819  0.82 0.821 0.822 0.823 0.824 0.825 
##  0.06  0.79  0.93  0.43  0.01  0.54  0.92  0.14  0.89  0.97  0.81  0.28  0.26  0.86  0.60  0.30  1.00  0.81  0.96  0.53  0.99  0.88 
## 0.826 0.827 0.828 0.829  0.83 0.831 0.832 0.833 0.834 0.835 0.836 0.837 0.838 0.839  0.84 0.841 0.842 0.843 0.844 0.845 0.846 0.847 
##  0.31  0.89  0.14  0.94  0.00  0.17  0.21  0.70  0.00  0.79  0.80  0.16  0.69  0.22  0.88  0.09  0.27  1.00  0.93  0.80  0.38  0.35 
## 0.848 0.849  0.85 0.851 0.852 0.853 0.854 0.855 0.856 0.857 0.858 0.859  0.86 0.861 0.862 0.863 0.864 0.865 0.866 0.867 0.868 0.869 
##  0.00  0.87  0.64  0.75  0.94  0.71  0.96  0.14  0.12  0.89  0.16  0.52  0.77  0.08  0.06  1.00  0.54  0.48  0.19  1.00  0.26  0.96 
##  0.87 0.871 0.872 0.873 0.874 0.875 0.876 0.877 0.878 0.879  0.88 0.881 0.882 0.883 0.884 0.885 0.886 0.887 0.888 0.889  0.89 0.891 
##  0.96  0.93  0.66  1.00  0.82  0.78  0.43  0.84  0.99  0.00  0.60  0.29  0.45  0.55  0.86  0.60  0.56  0.15  1.00  0.20  0.94  0.87 
## 0.892 0.893 0.894 0.895 0.896 0.897 0.898 0.899   0.9 
##  0.52  0.04  0.00  0.00  0.42  0.02  0.99  0.44  0.37</code></pre>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="introduction-to-probability.html#cb180-1" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb180-2"><a href="introduction-to-probability.html#cb180-2" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x =</span> griSta, <span class="at">y =</span> samPop, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/complex-system-data-1.png" width="672" /></p>
<hr />
</div>
<div id="using-models-to-explain-phenomena" class="section level3 hasAnchor" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Using Models to Explain Phenomena<a href="introduction-to-probability.html#using-models-to-explain-phenomena" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Models simplify reality while retaining essential structure.<br />
A <strong>good probability model</strong>:</p>
<ul>
<li>Explains observed variability</li>
<li>Produces realistic simulated data</li>
<li>Can be refined as new information becomes available</li>
</ul>
<hr />
</div>
<div id="final-idea-inference-without-a-model" class="section level3 hasAnchor" number="5.1.5">
<h3><span class="header-section-number">5.1.5</span> Final Idea: Inference Without a Model<a href="introduction-to-probability.html#final-idea-inference-without-a-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When you don’t have a model it is really hard to do inference.<br />
All of probability and statistics rely, either explicitly or implicitly, on the existence of a <strong>model</strong> that describes how data are generated.</p>
<p>A <strong>model</strong> provides a structured description of uncertainty. It specifies:
- what outcomes are possible,
- how these outcomes are related,
- and how frequently they are expected to occur.</p>
<p>Without such a structure, observed data are simply a collection of numbers or categories with no principled way to generalize beyond what has already been seen.</p>
<p>In probability theory, models play the role of an <em>idealized data‑generating mechanism</em>. They allow us to ask meaningful questions such as:
- <em>How surprising is this outcome?</em>
- <em>What should we expect to see if the process repeats?</em>
- <em>How does changing assumptions affect long‑run behavior?</em></p>
<p>When a model is clearly defined, inference becomes possible because we can compare observed data to what the model predicts. Differences between data and model expectations can then be interpreted as evidence about the underlying process.</p>
<p>By contrast, <strong>without a model</strong>:
- There is no clear notion of what outcomes are “likely” or “unlikely.”
- Uncertainty cannot be quantified in a principled way.
- There is no reliable mechanism for extrapolation beyond the observed data.
- Conclusions become informal, subjective, or anecdotal.</p>
<div class="definition">
<p><span id="def:inference" class="definition"><strong>Definition 5.2  (Statistical Inference) </strong></span>Statistical inference is the process of drawing conclusions about an underlying process, population, or mechanism by comparing observed data to a probabilistic model.</p>
</div>
<hr />
</div>
</div>
<div id="interpretations-of-probability" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Interpretations of Probability<a href="introduction-to-probability.html#interpretations-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="classical-interpretation" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Classical Interpretation<a href="introduction-to-probability.html#classical-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:classical-prob" class="definition"><strong>Definition 5.3  (Classical Probability) </strong></span>Classical probability defines the probability of an event as the ratio of the number of favorable outcomes to the total number of equally likely outcomes.</p>
</div>
<p>The classical interpretation is the earliest formal approach to probability. It views probability as a <strong>logical consequence of symmetry</strong> in a well‑defined experiment. When all outcomes are equally plausible <em>before</em> the experiment is performed, probability can be computed by simple counting.</p>
<p>This approach treats probability as an <em>a priori</em> quantity: probabilities are determined entirely by the structure of the experiment, not by observed data.</p>
<hr />
<div id="history" class="section level4 hasAnchor" number="5.2.1.1">
<h4><span class="header-section-number">5.2.1.1</span> History<a href="introduction-to-probability.html#history" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The classical interpretation emerged in the <strong>17th and 18th centuries</strong>, primarily through the study of games of chance.</p>
<p>Key contributors include:</p>
<ul>
<li><strong>Blaise Pascal</strong> and <strong>Pierre de Fermat</strong>, who studied fair division of gambling stakes.</li>
<li><strong>Pierre‑Simon Laplace</strong>, who formalized probability as a mathematical discipline.</li>
</ul>
<p>During this period, probability problems typically involved:</p>
<ul>
<li>dice,</li>
<li>cards,</li>
<li>coins,</li>
<li>lottery‑type mechanisms.</li>
</ul>
<p>These systems were appealing because they naturally possessed <strong>symmetry</strong>, making the assumption of equally likely outcomes reasonable.</p>
<hr />
</div>
<div id="key-assumption-equally-likely-outcomes" class="section level4 hasAnchor" number="5.2.1.2">
<h4><span class="header-section-number">5.2.1.2</span> Key Assumption: Equally Likely Outcomes<a href="introduction-to-probability.html#key-assumption-equally-likely-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The classical definition <strong>requires</strong> that all outcomes in the sample space are equally likely.</p>
<p>This assumption is critical and restrictive.</p>
<p>Consequences:</p>
<ul>
<li>Classical probability works well for idealized systems with symmetry.</li>
<li>It becomes difficult or impossible to apply when outcomes are not equally likely or when the sample space is not finite and well defined.</li>
</ul>
<p>Thus, the classical interpretation is best viewed as a <strong>modeling choice</strong>, not a universal definition.</p>
<hr />
</div>
<div id="example-rolling-a-fair-die" class="section level4 hasAnchor" number="5.2.1.3">
<h4><span class="header-section-number">5.2.1.3</span> Example: Rolling a Fair Die<a href="introduction-to-probability.html#example-rolling-a-fair-die" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider rolling a well‑balanced six‑sided die.</p>
<ul>
<li>The sample space contains 6 outcomes:<br />
{1, 2, 3, 4, 5, 6}</li>
<li>Each outcome is assumed equally likely.</li>
</ul>
<p>The probability of rolling a 4 is therefore:</p>
<p><span class="math display">\[
1 / 6
\]</span></p>
<p>This probability is computed <em>before</em> any data are collected and does not depend on repeated trials.</p>
<hr />
</div>
<div id="outcomes-and-events" class="section level4 hasAnchor" number="5.2.1.4">
<h4><span class="header-section-number">5.2.1.4</span> Outcomes and Events<a href="introduction-to-probability.html#outcomes-and-events" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definition">
<p><span id="def:outcome" class="definition"><strong>Definition 5.4  (Outcome) </strong></span>An outcome is a single possible result of a random experiment.</p>
</div>
<p>Examples:</p>
<ul>
<li>Rolling a 4 on a die</li>
<li>Drawing the Ace of Spades from a deck</li>
<li>Getting Heads in a coin toss</li>
</ul>
<p>Outcomes represent the <strong>most granular level</strong> of description of a random experiment.</p>
<div class="definition">
<p><span id="def:event" class="definition"><strong>Definition 5.5  (Event) </strong></span>An event is a collection of one or more outcomes.</p>
</div>
<p>Examples:</p>
<ul>
<li>Rolling an even number: {2, 4, 6}</li>
<li>Drawing a face card from a deck</li>
<li>Getting at least one Head in two coin tosses</li>
</ul>
<p>An event can be interpreted as a <strong>question</strong> about the experiment whose answer is either <em>yes</em> (the event occurs) or <em>no</em> (the event does not occur).</p>
<hr />
</div>
<div id="strengths-and-limitations-of-the-classical-interpretation" class="section level4 hasAnchor" number="5.2.1.5">
<h4><span class="header-section-number">5.2.1.5</span> Strengths and Limitations of the Classical Interpretation<a href="introduction-to-probability.html#strengths-and-limitations-of-the-classical-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Strengths</strong></p>
<ul>
<li>Simple and intuitive</li>
<li>Requires no data</li>
<li>Provides exact probabilities in symmetric settings</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Depends critically on the equally likely assumption</li>
<li>Not suitable for complex, real‑world phenomena</li>
<li>Does not explain how probabilities change or emerge with repeated observations</li>
</ul>
<p>These limitations motivate alternative interpretations of probability, especially those based on <strong>long‑run behavior</strong> and <strong>data‑driven modeling</strong>.</p>
<hr />
<p>In summary, the classical interpretation introduces probability as a <em>counting rule based on symmetry</em>. While historically foundational and mathematically elegant, it is best understood as a <strong>special case of probability modeling</strong>, applicable only when symmetry and equally likely outcomes are plausible assumptions.</p>
<hr />
</div>
</div>
<div id="relative-frequency-interpretation" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Relative Frequency Interpretation<a href="introduction-to-probability.html#relative-frequency-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:relfreq" class="definition"><strong>Definition 5.6  (Relative Frequency Probability) </strong></span>Probability is defined as the long‑run proportion of times an event occurs when a random experiment is repeated under identical conditions.</p>
</div>
<p>The relative frequency interpretation views probability not as a purely theoretical quantity, but as a <strong>property that emerges through repetition</strong>. Under this interpretation, probability is tied directly to observable data and is meaningful only in contexts where an experiment can, at least in principle, be repeated many times.</p>
<p>Rather than assuming equally likely outcomes, this approach defines probability through <strong>empirical behavior</strong>. Probabilities are not assigned in advance; they are <em>revealed</em> through repeated trials.</p>
<hr />
<div id="history-1" class="section level4 hasAnchor" number="5.2.2.1">
<h4><span class="header-section-number">5.2.2.1</span> History<a href="introduction-to-probability.html#history-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The relative frequency interpretation developed primarily in the <strong>19th and early 20th centuries</strong>, alongside advances in experimental science and statistics.</p>
<p>Key features of its historical development:</p>
<ul>
<li>Closely associated with scientists such as <strong>John Venn</strong> and <strong>Richard von Mises</strong></li>
<li>Motivated by the need to describe probabilities in <strong>physical and social phenomena</strong></li>
<li>Emphasized objectivity through repeatable experiments</li>
</ul>
<p>This interpretation arose as a response to the limitations of classical probability, especially in situations lacking symmetry or equally likely outcomes.</p>
<hr />
</div>
<div id="core-idea-longrun-behavior" class="section level4 hasAnchor" number="5.2.2.2">
<h4><span class="header-section-number">5.2.2.2</span> Core Idea: Long‑Run Behavior<a href="introduction-to-probability.html#core-idea-longrun-behavior" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Under the relative frequency interpretation:</p>
<ul>
<li>Probability is not about a single trial</li>
<li>Probability is about what happens <strong>in the long run</strong></li>
</ul>
<p>A single coin toss provides no meaningful probability information. Thousands of tosses, however, reveal stable patterns.</p>
<p>This interpretation implicitly assumes:</p>
<ul>
<li>The experiment can be repeated indefinitely</li>
<li>The underlying mechanism remains stable over time</li>
</ul>
<hr />
</div>
<div id="probability-as-an-approximation" class="section level4 hasAnchor" number="5.2.2.3">
<h4><span class="header-section-number">5.2.2.3</span> Probability as an Approximation<a href="introduction-to-probability.html#probability-as-an-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In practice, probabilities are <strong>approximated</strong>, not known exactly. The relative frequency of an event after <span class="math inline">\(n\)</span> trials is:</p>
<p><span class="math display">\[
\text{Relative Frequency} = \frac{\text{Number of times the event occurs}}{n}
\]</span></p>
<p>Simulation provides a natural way to observe this idea.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="introduction-to-probability.html#cb181-1" tabindex="-1"></a>p         <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb181-2"><a href="introduction-to-probability.html#cb181-2" tabindex="-1"></a>n         <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb181-3"><a href="introduction-to-probability.html#cb181-3" tabindex="-1"></a>samCoi    <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&quot;Heads&quot;</span>, <span class="st">&quot;Tails&quot;</span>), <span class="at">size =</span> n, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(p, <span class="dv">1</span> <span class="sc">-</span> p))</span>
<span id="cb181-4"><a href="introduction-to-probability.html#cb181-4" tabindex="-1"></a>relFreHea <span class="ot">&lt;-</span> <span class="fu">sum</span>(samCoi <span class="sc">==</span> <span class="st">&quot;Heads&quot;</span>) <span class="sc">/</span> n</span>
<span id="cb181-5"><a href="introduction-to-probability.html#cb181-5" tabindex="-1"></a><span class="fu">print</span>(relFreHea)</span></code></pre></div>
<pre><code>## [1] 0.412</code></pre>
<p>Each execution of this code generates a different result, but as the number of trials increases, the relative frequency tends to stabilize near a constant value.</p>
<p>This stabilization is an empirical manifestation of the <strong>Law of Large Numbers</strong> that we will talk about later.</p>
<hr />
</div>
<div id="interpretation-and-modeling-perspective" class="section level4 hasAnchor" number="5.2.2.4">
<h4><span class="header-section-number">5.2.2.4</span> Interpretation and Modeling Perspective<a href="introduction-to-probability.html#interpretation-and-modeling-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>From a modeling standpoint, the relative frequency interpretation treats probability as a <strong>feature of the data‑generating process</strong>. The model is validated not by symmetry or logic alone, but by consistency between:</p>
<ul>
<li>simulated behavior,</li>
<li>observed data,</li>
<li>and long‑run frequencies.</li>
</ul>
<p>Probability models under this interpretation are often assessed by asking:</p>
<ul>
<li>Do simulated frequencies resemble observed frequencies?</li>
<li>Does the model reproduce stable long‑run patterns?</li>
</ul>
<hr />
</div>
<div id="strengths-and-limitations" class="section level4 hasAnchor" number="5.2.2.5">
<h4><span class="header-section-number">5.2.2.5</span> Strengths and Limitations<a href="introduction-to-probability.html#strengths-and-limitations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Strengths</strong></p>
<ul>
<li>Grounded in observable data</li>
<li>Does not require equally likely outcomes</li>
<li>Naturally compatible with simulation and computation</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Requires repeatable experiments</li>
<li>Not applicable to one‑time events (e.g., a specific election outcome)</li>
<li>Does not directly address uncertainty in small samples</li>
</ul>
<hr />
<p>In summary, the relative frequency interpretation reframes probability as a <strong>long‑run empirical regularity</strong>. It forms the conceptual foundation for simulation‑based probability and provides a bridge between abstract models and observed data.</p>
<hr />
<p>Here is an <strong>expanded, textbook‑style version</strong> of the <strong>Subjective Interpretation</strong>, written fully in <strong>R Markdown</strong>, consistent with your formatting rules and the level of depth used for the Classical and Relative Frequency sections. It emphasizes <strong>modeling, information, and belief</strong>, with a clear conceptual narrative.</p>
</div>
</div>
<div id="subjective-interpretation" class="section level3 hasAnchor" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Subjective Interpretation<a href="introduction-to-probability.html#subjective-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:subjective" class="definition"><strong>Definition 5.7  (Subjective Probability) </strong></span>Subjective probability represents an individual’s degree of belief about the occurrence of an event, given the information available to them.</p>
</div>
<p>The subjective interpretation treats probability as a <strong>measure of uncertainty based on information, judgment, and belief</strong>, rather than symmetry or long‑run frequency. Under this view, probability quantifies how strongly an individual believes an event will occur, conditional on what they currently know.</p>
<p>Unlike classical or relative frequency interpretations, subjective probability does <strong>not require repeatable experiments</strong> or equally likely outcomes. Instead, it is fundamentally <strong>conditional on information</strong> and is allowed to change as new information becomes available.</p>
<hr />
<div id="history-2" class="section level4 hasAnchor" number="5.2.3.1">
<h4><span class="header-section-number">5.2.3.1</span> History<a href="introduction-to-probability.html#history-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The subjective interpretation developed primarily in the <strong>20th century</strong> and is closely associated with the rise of <strong>Bayesian statistics</strong>.</p>
<p>Key contributors include:</p>
<ul>
<li><strong>Frank Ramsey</strong>, who linked probability to rational decision‑making</li>
<li><strong>Bruno de Finetti</strong>, who argued that “probability does not exist” outside personal belief</li>
<li>Later formalization within Bayesian inference and decision theory</li>
</ul>
<p>This interpretation arose from the need to reason probabilistically about:</p>
<ul>
<li>unique or one‑time events,</li>
<li>decision‑making under uncertainty,</li>
<li>situations where long‑run frequencies are unavailable or meaningless.</li>
</ul>
<hr />
</div>
<div id="core-idea-probability-as-informationdependent-belief" class="section level4 hasAnchor" number="5.2.3.2">
<h4><span class="header-section-number">5.2.3.2</span> Core Idea: Probability as Information‑Dependent Belief<a href="introduction-to-probability.html#core-idea-probability-as-informationdependent-belief" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Under the subjective interpretation:</p>
<ul>
<li>Probability is always <strong>conditional</strong> on available information.</li>
<li>Different individuals may assign different probabilities to the same event.</li>
<li>Updating beliefs in light of new data is a central feature.</li>
</ul>
<p>From a modeling perspective, a subjective probability model represents the analyst’s <strong>state of knowledge</strong> rather than an objective property of the physical world.</p>
<p>As information changes, probabilities are updated accordingly, typically using <strong>Bayes’ theorem</strong>.</p>
<hr />
</div>
<div id="example-betting-interpretation" class="section level4 hasAnchor" number="5.2.3.3">
<h4><span class="header-section-number">5.2.3.3</span> Example: Betting Interpretation<a href="introduction-to-probability.html#example-betting-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One way to operationalize subjective probability is through <strong>betting behavior</strong>.</p>
<p>If you are willing to bet $70 to win $100 on an event occurring, your implied probability is:</p>
<p><span class="math display">\[
70 / 100
\]</span></p>
<p>This value represents the minimum probability at which you consider the bet fair, given your beliefs and risk tolerance.</p>
<p>In this framework:</p>
<ul>
<li>Probabilities reflect willingness to accept risk</li>
<li>Inconsistent probabilities can lead to guaranteed losses (Dutch books)</li>
<li>Rational probability assignments are those that avoid sure loss</li>
</ul>
<hr />
</div>
<div id="subjective-probability-as-a-model" class="section level4 hasAnchor" number="5.2.3.4">
<h4><span class="header-section-number">5.2.3.4</span> Subjective Probability as a Model<a href="introduction-to-probability.html#subjective-probability-as-a-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In subjective probability, a <strong>probability model</strong> represents beliefs about how the world operates, not just observed frequencies. These models are often used when:</p>
<ul>
<li>events are unique (e.g., an election outcome),</li>
<li>data are scarce or incomplete,</li>
<li>decisions must be made before data are observed.</li>
</ul>
<p>Such models are evaluated not by long‑run frequencies, but by:</p>
<ul>
<li>internal coherence,</li>
<li>consistency with observed evidence,</li>
<li>usefulness for decision‑making.</li>
</ul>
<hr />
</div>
<div id="strengths-and-limitations-1" class="section level4 hasAnchor" number="5.2.3.5">
<h4><span class="header-section-number">5.2.3.5</span> Strengths and Limitations<a href="introduction-to-probability.html#strengths-and-limitations-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Strengths</strong></p>
<ul>
<li>Applicable to one‑time or non‑repeatable events</li>
<li>Naturally incorporates prior knowledge and expert judgment</li>
<li>Provides a coherent framework for updating beliefs</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Depends on subjective judgment</li>
<li>Different individuals may disagree</li>
<li>Requires careful justification of assumptions</li>
</ul>
<hr />
<p>In summary, the subjective interpretation views probability as a <strong>quantitative expression of uncertainty given information</strong>. It plays a central role in Bayesian modeling, where probability models evolve as data are observed, and inference is understood as a process of <strong>belief updating</strong> rather than long‑run stabilization.</p>
<hr />
</div>
</div>
</div>
<div id="how-to-build-probability-models" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> How to Build Probability Models<a href="introduction-to-probability.html#how-to-build-probability-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<div id="finding-the-probability-of-an-event" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Finding the Probability of an Event<a href="introduction-to-probability.html#finding-the-probability-of-an-event" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:prob-event" class="definition"><strong>Definition 5.8  (Probability of an Event) </strong></span>The probability of an event is the sum of the probabilities of the outcomes that compose the event.</p>
</div>
<p>In probability theory, events are not primitive objects. Instead, they are built from <strong>outcomes</strong>, which represent the most basic possible results of a random experiment. A probability model assigns probabilities to these outcomes, and the probability of any event is obtained by aggregating the probabilities of the outcomes that form the event.</p>
<p>From a modeling perspective, this definition emphasizes that probability is <strong>additive over disjoint outcomes</strong>. Once the model specifies probabilities at the outcome level, probabilities for more complex events follow automatically.</p>
<p>In classical settings, this is achieved through counting. In simulation‑based approaches, this aggregation is approximated empirically.</p>
<hr />
<div id="simulation-approach-to-event-probability" class="section level4 hasAnchor" number="5.3.1.1">
<h4><span class="header-section-number">5.3.1.1</span> Simulation Approach to Event Probability<a href="introduction-to-probability.html#simulation-approach-to-event-probability" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Rather than computing probabilities analytically, we can <strong>estimate probabilities through repeated simulation</strong>.</p>
<p>For example, consider estimating the probability of rolling a 6 on a fair die.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="introduction-to-probability.html#cb183-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">size =</span> <span class="dv">10000</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="sc">==</span> <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 0.1629</code></pre>
<p>Each simulation run produces a slightly different value, but as the number of trials increases, the estimate stabilizes. This illustrates how probability emerges as a <strong>long‑run average behavior of a model</strong>, rather than a single deterministic calculation.</p>
<p>This computational viewpoint aligns naturally with the relative frequency interpretation of probability.</p>
<hr />
</div>
</div>
<div id="basic-event-relations-and-probability-laws" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Basic Event Relations and Probability Laws<a href="introduction-to-probability.html#basic-event-relations-and-probability-laws" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<hr />
<p>The power of probability models lies not only in assigning probabilities to single events, but in understanding how events <strong>combine and relate</strong> to one another. These relationships are governed by fundamental probability laws that hold regardless of the specific model used.</p>
<hr />
<div id="mutually-exclusive-events" class="section level4 hasAnchor" number="5.3.2.1">
<h4><span class="header-section-number">5.3.2.1</span> Mutually Exclusive Events<a href="introduction-to-probability.html#mutually-exclusive-events" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<hr />
<div class="definition">
<p><span id="def:mutually-exclusive" class="definition"><strong>Definition 5.9  (Mutually Exclusive Events) </strong></span>Two events are mutually exclusive if they cannot occur at the same time.</p>
</div>
<p>Mutually exclusive events share <strong>no common outcomes</strong>. If one event occurs, the other cannot.</p>
<p>Examples:</p>
<ul>
<li>Rolling a die and obtaining a 1 versus obtaining a 2</li>
<li>Drawing a card that is a heart versus drawing a card that is a spade</li>
</ul>
<p>Because these events do not overlap, their probabilities combine in a particularly simple way.</p>
<div class="definition">
<p><span id="def:prob-mut-ex" class="definition"><strong>Definition 5.10  (Probability of Mutually Exclusive Events) </strong></span>If A and B are mutually exclusive, then<br />
P(A ∪ B) = P(A) + P(B).</p>
</div>
<p>This property follows directly from the definition of probability as additive over disjoint outcomes.</p>
</div>
<div id="simulation-example" class="section level4 hasAnchor" number="5.3.2.2">
<h4><span class="header-section-number">5.3.2.2</span> Simulation Example<a href="introduction-to-probability.html#simulation-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Estimate the probability that a fair die roll results in either a 1 or a 2.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="introduction-to-probability.html#cb185-1" tabindex="-1"></a>dieRol <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">10000</span>, <span class="cn">TRUE</span>) </span>
<span id="cb185-2"><a href="introduction-to-probability.html#cb185-2" tabindex="-1"></a>pro12  <span class="ot">&lt;-</span> <span class="fu">mean</span>(dieRol <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb185-3"><a href="introduction-to-probability.html#cb185-3" tabindex="-1"></a>pro1   <span class="ot">&lt;-</span> <span class="fu">mean</span>(dieRol <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb185-4"><a href="introduction-to-probability.html#cb185-4" tabindex="-1"></a>pro2   <span class="ot">&lt;-</span> <span class="fu">mean</span>(dieRol <span class="sc">==</span> <span class="dv">2</span>)</span>
<span id="cb185-5"><a href="introduction-to-probability.html#cb185-5" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Probability of 1: &quot;</span>, pro1))</span></code></pre></div>
<pre><code>## [1] &quot;Probability of 1: 0.1712&quot;</code></pre>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="introduction-to-probability.html#cb187-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Probability of 2: &quot;</span>, pro2))</span></code></pre></div>
<pre><code>## [1] &quot;Probability of 2: 0.1663&quot;</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="introduction-to-probability.html#cb189-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Probability of 1 and 2: &quot;</span>, pro12))</span></code></pre></div>
<pre><code>## [1] &quot;Probability of 1 and 2: 0.3375&quot;</code></pre>
<p>This probability is approximately twice the probability of a single outcome because the events are mutually exclusive.</p>
<hr />
</div>
<div id="complement-events" class="section level4 hasAnchor" number="5.3.2.3">
<h4><span class="header-section-number">5.3.2.3</span> Complement Events<a href="introduction-to-probability.html#complement-events" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<hr />
<div class="definition">
<p><span id="def:complement" class="definition"><strong>Definition 5.11  (Complement of an Event) </strong></span>The complement of an event A consists of all outcomes that are not in A.</p>
</div>
<p>Every event has a complement, representing the event “not A.” Together, an event and its complement exhaust the entire sample space.</p>
<div class="definition">
<p><span id="def:complement-prob" class="definition"><strong>Definition 5.12  (Probability of the Complement) </strong></span>P(Aᶜ) = 1 − P(A)</p>
</div>
<p>This identity is fundamental and does not depend on assumptions such as equal likelihood or independence. It follows from the fact that the total probability of the sample space is 1.</p>
</div>
<div id="simulation-example-1" class="section level4 hasAnchor" number="5.3.2.4">
<h4><span class="header-section-number">5.3.2.4</span> Simulation Example<a href="introduction-to-probability.html#simulation-example-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Estimate the probability of <em>not</em> rolling a 1 on a fair die.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="introduction-to-probability.html#cb191-1" tabindex="-1"></a>pA <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">10000</span>, <span class="cn">TRUE</span>) <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb191-2"><a href="introduction-to-probability.html#cb191-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Probability of rolling a 1 : &quot;</span>, pA))</span></code></pre></div>
<pre><code>## [1] &quot;Probability of rolling a 1 : 0.1655&quot;</code></pre>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="introduction-to-probability.html#cb193-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Probability of not rolling a 1 : &quot;</span>, <span class="dv">1</span> <span class="sc">-</span> pA))</span></code></pre></div>
<pre><code>## [1] &quot;Probability of not rolling a 1 : 0.8345&quot;</code></pre>
<p>Using the complement is often computationally simpler, especially when the event of interest is complicated but its complement is not.</p>
<hr />
</div>
</div>
<div id="properties-of-probability" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Properties of Probability<a href="introduction-to-probability.html#properties-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<hr />
<p>Probability models are governed by a small set of foundational properties that ensure internal consistency.</p>
<div class="definition">
<p><span id="def:prob-properties" class="definition"><strong>Definition 5.13  (Probability Model Properties) </strong></span>A probability model must satisfy:</p>
<ol style="list-style-type: decimal">
<li>0 ≤ P(A) ≤ 1<br />
</li>
<li>P(S) = 1<br />
</li>
<li>If A and B are mutually exclusive, P(A ∪ B) = P(A) + P(B)</li>
</ol>
</div>
<p>These axioms define what it means for a function to be a valid probability measure. All other probability rules can be derived from them.</p>
<hr />
<div id="unions-and-intersections-of-events" class="section level4 hasAnchor" number="5.3.3.1">
<h4><span class="header-section-number">5.3.3.1</span> Unions and Intersections of Events<a href="introduction-to-probability.html#unions-and-intersections-of-events" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definition">
<p><span id="def:union" class="definition"><strong>Definition 5.14  (Union of Events) </strong></span>The union of events A and B consists of all outcomes that are in A or B (or both).</p>
</div>
<div class="definition">
<p><span id="def:intersection" class="definition"><strong>Definition 5.15  (Intersection of Events) </strong></span>The intersection of events A and B consists of all outcomes that are in both A and B.</p>
</div>
<p>Unlike mutually exclusive events, most events <strong>overlap</strong>, meaning their intersection is not empty. In such cases, naïvely adding probabilities would double‑count shared outcomes.</p>
<div class="definition">
<p><span id="def:union-prob" class="definition"><strong>Definition 5.16  (Probability of the Union) </strong></span>P(A ∪ B) = P(A) + P(B) − P(A ∩ B)</p>
</div>
<p>This formula corrects for double counting and is essential for building probability models in realistic settings.</p>
<hr />
</div>
<div id="modeling-perspective" class="section level4 hasAnchor" number="5.3.3.2">
<h4><span class="header-section-number">5.3.3.2</span> Modeling Perspective<a href="introduction-to-probability.html#modeling-perspective" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>From a modeling standpoint, these properties:</p>
<ul>
<li>ensure internal consistency of probability assignments,</li>
<li>allow complex event probabilities to be constructed from simpler ones,</li>
<li>connect analytical probability laws with simulation‑based estimation.</li>
</ul>
<p>In computational probability, unions, intersections, and complements are implemented using <strong>logical operations on simulated data</strong>, reinforcing the interpretation of probability models as data‑generating mechanisms.</p>
<hr />
</div>
</div>
<div id="conditional-probability" class="section level3 hasAnchor" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> Conditional Probability<a href="introduction-to-probability.html#conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<hr />
<div class="definition">
<p><span id="def:conditional" class="definition"><strong>Definition 5.17  (Conditional Probability) </strong></span>The conditional probability of an event A given that event B has occurred is<br />
<span class="math display">\[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad \text{provided } P(B) &gt; 0.
\]</span></p>
</div>
<p>Conditional probability formalizes the idea of <strong>updating probabilities when additional information is available</strong>. Rather than asking whether event A occurs in general, we now ask whether A occurs <em>given that</em> B has already occurred.</p>
<p>From a modeling perspective, conditioning restricts attention to a <strong>reduced sample space</strong>: only those outcomes consistent with event B are considered possible. Probabilities are then rescaled so that this restricted space has total probability 1.</p>
<p>In this way, conditional probability connects probability models to the flow of information.</p>
<hr />
<div id="interpretation-via-simulation" class="section level4 hasAnchor" number="5.3.4.1">
<h4><span class="header-section-number">5.3.4.1</span> Interpretation via Simulation<a href="introduction-to-probability.html#interpretation-via-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider a fair die. Let:</p>
<ul>
<li>A = {the outcome is 4}</li>
<li>B = {the outcome is greater than 3}</li>
</ul>
<p>Rather than reasoning abstractly, we estimate this conditional probability by simulation.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="introduction-to-probability.html#cb195-1" tabindex="-1"></a>dieRol <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">10000</span>, <span class="cn">TRUE</span>)</span>
<span id="cb195-2"><a href="introduction-to-probability.html#cb195-2" tabindex="-1"></a>proCon <span class="ot">&lt;-</span> <span class="fu">mean</span>(dieRol <span class="sc">==</span> <span class="dv">4</span> <span class="sc">&amp;</span> dieRol <span class="sc">&gt;</span> <span class="dv">3</span>) <span class="sc">/</span> <span class="fu">mean</span>(dieRol <span class="sc">&gt;</span> <span class="dv">3</span>)</span>
<span id="cb195-3"><a href="introduction-to-probability.html#cb195-3" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: &quot;</span>, proCon))</span></code></pre></div>
<pre><code>## [1] &quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: 0.322678535302448&quot;</code></pre>
<p>Here:</p>
<ul>
<li><code>dieRol &gt; 3</code> identifies the outcomes consistent with the condition B,</li>
<li><code>dieRol == 4 &amp; dieRol &gt; 3</code> identifies outcomes where both A and B occur.</li>
</ul>
<p>The ratio estimates the probability of rolling a 4 <em>given</em> that the roll exceeds 3.</p>
<p>This computational approach makes the conditioning mechanism explicit: we focus only on simulated outcomes where B occurs and examine how often A appears within that subset.</p>
<hr />
</div>
<div id="conditional-probability-as-a-modeling-tool" class="section level4 hasAnchor" number="5.3.4.2">
<h4><span class="header-section-number">5.3.4.2</span> Conditional Probability as a Modeling Tool<a href="introduction-to-probability.html#conditional-probability-as-a-modeling-tool" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Conditional probabilities are central to probabilistic modeling because they allow us to express <strong>structured dependence</strong> between events. In complex systems, probabilities rarely exist in isolation; they reflect assumptions about what is known and unknown.</p>
<p>Practically, conditional probabilities help answer questions of the form:</p>
<ul>
<li><em>What is the chance of A when we know B has happened?</em></li>
<li><em>How does information change our expectations?</em></li>
</ul>
<hr />
<div class="definition">
<p><span id="def:marginal" class="definition"><strong>Definition 5.18  (Marginal Probability) </strong></span>A marginal (or unconditional) probability is the probability of an event considered without conditioning on any other event.</p>
</div>
<p>The marginal probability describes the baseline likelihood of an event across the entire sample space, before incorporating additional information.</p>
<p>For the same die example, the marginal probability of rolling a 4 is:</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="introduction-to-probability.html#cb197-1" tabindex="-1"></a>proMar <span class="ot">&lt;-</span> <span class="fu">mean</span>(die <span class="sc">==</span> <span class="dv">4</span>)</span>
<span id="cb197-2"><a href="introduction-to-probability.html#cb197-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: &quot;</span>, proCon))</span></code></pre></div>
<pre><code>## [1] &quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: 0.322678535302448&quot;</code></pre>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="introduction-to-probability.html#cb199-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Marginal probability of rolling a 4: &quot;</span>, proMar))</span></code></pre></div>
<pre><code>## [1] &quot;Marginal probability of rolling a 4: 0.1595&quot;</code></pre>
<p>Comparing marginal and conditional probabilities highlights how information can <strong>increase, decrease, or leave unchanged</strong> the likelihood of an event.</p>
<hr />
</div>
</div>
<div id="law-of-total-probability" class="section level3 hasAnchor" number="5.3.5">
<h3><span class="header-section-number">5.3.5</span> Law of Total Probability<a href="introduction-to-probability.html#law-of-total-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<hr />
<div class="definition">
<p><span id="def:total-prob" class="definition"><strong>Definition 5.19  (Law of Total Probability) </strong></span>If events <span class="math inline">\(B_1, B_2, \dots, B_n\)</span> form a partition of the sample space, then
<span class="math display">\[
P(A) = \sum_{i=1}^{n} P(A \mid B_i)\,P(B_i).
\]</span></p>
</div>
<p>The Law of Total Probability provides a way to compute the probability of an event by <strong>decomposing it into simpler scenarios</strong>. Each event <span class="math display">\[B_i\]</span> represents a mutually exclusive and exhaustive case under which A might occur.</p>
<p>From a modeling perspective, this law expresses how <strong>overall probability emerges from conditional structure</strong>. It is especially useful when direct computation of <span class="math display">\[P(A)\]</span> is difficult, but conditional probabilities are easier to specify or estimate.</p>
<hr />
</div>
<div id="bayes-formula" class="section level3 hasAnchor" number="5.3.6">
<h3><span class="header-section-number">5.3.6</span> Bayes’ Formula<a href="introduction-to-probability.html#bayes-formula" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<hr />
<div class="definition">
<p><span id="def:bayes" class="definition"><strong>Definition 5.20  (Bayes' Formula) </strong></span>For events A and B with P(A) &gt; 0,
<span class="math display">\[
P(B \mid A) = \frac{P(A \mid B)\,P(B)}{P(A)}.
\]</span></p>
</div>
<p>Bayes’ formula reverses the direction of conditioning. Instead of computing the probability of observing A given B, it computes the probability of B given that A has been observed.</p>
<p>This inversion is fundamental to <strong>Bayesian modeling</strong>, where probabilities are updated as data arrive. Prior beliefs <span class="math display">\[P(B)\]</span> are combined with evidence <span class="math display">\[P(A \mid B)\]</span> to produce updated beliefs <span class="math display">\[P(B \mid A)\]</span>.</p>
<hr />
<div id="simulation-example-medical-testing" class="section level4 hasAnchor" number="5.3.6.1">
<h4><span class="header-section-number">5.3.6.1</span> Simulation Example: Medical Testing<a href="introduction-to-probability.html#simulation-example-medical-testing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We illustrate Bayes’ formula using a diagnostic testing scenario.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="introduction-to-probability.html#cb201-1" tabindex="-1"></a><span class="co"># Bayes example via simulation</span></span>
<span id="cb201-2"><a href="introduction-to-probability.html#cb201-2" tabindex="-1"></a>proInf <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb201-3"><a href="introduction-to-probability.html#cb201-3" tabindex="-1"></a><span class="do">## Sample disease status</span></span>
<span id="cb201-4"><a href="introduction-to-probability.html#cb201-4" tabindex="-1"></a>disease <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>), <span class="dv">100000</span>, <span class="cn">TRUE</span>, <span class="at">prob =</span> <span class="fu">c</span>(proInf, <span class="dv">1</span> <span class="sc">-</span> proInf))</span>
<span id="cb201-5"><a href="introduction-to-probability.html#cb201-5" tabindex="-1"></a><span class="do">## Sample test results given disease status</span></span>
<span id="cb201-6"><a href="introduction-to-probability.html#cb201-6" tabindex="-1"></a>sen <span class="ot">&lt;-</span> <span class="fl">0.95</span></span>
<span id="cb201-7"><a href="introduction-to-probability.html#cb201-7" tabindex="-1"></a>spe <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb201-8"><a href="introduction-to-probability.html#cb201-8" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(disease,</span>
<span id="cb201-9"><a href="introduction-to-probability.html#cb201-9" tabindex="-1"></a>               <span class="fu">sample</span>(<span class="fu">c</span>(<span class="cn">TRUE</span>,<span class="cn">FALSE</span>), <span class="dv">100000</span>, <span class="cn">TRUE</span>,<span class="at">prob=</span><span class="fu">c</span>(sen,    <span class="dv">1</span> <span class="sc">-</span> sen)),</span>
<span id="cb201-10"><a href="introduction-to-probability.html#cb201-10" tabindex="-1"></a>               <span class="fu">sample</span>(<span class="fu">c</span>(<span class="cn">TRUE</span>,<span class="cn">FALSE</span>), <span class="dv">100000</span>, <span class="cn">TRUE</span>,<span class="at">prob=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span> spe, spe)))</span>
<span id="cb201-11"><a href="introduction-to-probability.html#cb201-11" tabindex="-1"></a></span>
<span id="cb201-12"><a href="introduction-to-probability.html#cb201-12" tabindex="-1"></a><span class="fu">mean</span>(disease[test])</span></code></pre></div>
<pre><code>## [1] 0.01005025</code></pre>
<p>In this simulation:</p>
<ul>
<li>Only 1% of individuals have the disease,</li>
<li>The test is accurate but imperfect,</li>
<li>We compute the probability of disease <strong>given a positive test result</strong>.</li>
</ul>
<p>Despite high test accuracy, the conditional probability may be much lower than expected due to the rarity of the disease. This illustrates how conditioning can dramatically change intuitive conclusions.</p>
<hr />
<div class="definition">
<p><span id="def:states" class="definition"><strong>Definition 5.21  (States of Nature and Observable Events) </strong></span>States of nature represent the true but unobserved condition of a system, while observable events are the data or measurements produced by that system.</p>
</div>
<p>Bayesian reasoning explicitly distinguishes between:</p>
<ul>
<li><strong>states of nature</strong>, which are unknown but real,</li>
<li><strong>observable events</strong>, which provide indirect information about those states.</li>
</ul>
<p>Conditional probability provides the mathematical bridge between what we observe and what we wish to infer.</p>
<hr />
</div>
</div>
<div id="modeling-perspective-summary" class="section level3 hasAnchor" number="5.3.7">
<h3><span class="header-section-number">5.3.7</span> Modeling Perspective Summary<a href="introduction-to-probability.html#modeling-perspective-summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Conditional probability is not merely a computational rule—it is a <strong>modeling principle</strong>. It encodes how information alters uncertainty, enables the construction of hierarchical and Bayesian models, and connects probability theory directly to inference.</p>
<p>In simulation‑based approaches, conditioning is implemented by <strong>subsetting data and renormalizing</strong>, reinforcing the idea that probability models are best understood as data‑generating mechanisms shaped by information.</p>
<hr />
</div>
</div>
<div id="summary-first-part-intro-to-probability" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Summary First Part: Intro to Probability<a href="introduction-to-probability.html#summary-first-part-intro-to-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Probability models are <strong>data‑generating mechanisms</strong></li>
<li>Simulation connects theory to observable behavior</li>
<li>Different interpretations serve different purposes</li>
<li>Conditional probability and Bayes’ formula link belief, data, and inference</li>
</ul>
<p>Understood. Here is the expanded text with inline math using <code>$ $</code> instead of <code>\(...\)</code> or <code>\[...\]</code>, while keeping your structure and code chunks unchanged.</p>
<hr />
</div>
<div id="independence-of-events" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Independence of events<a href="introduction-to-probability.html#independence-of-events" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="definition" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Definition<a href="introduction-to-probability.html#definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:indep-events" class="definition"><strong>Definition 5.22  (Independence of Events) </strong></span>Two events A and B are independent if the occurrence of one does not change the probability of the other:
<span class="math display">\[
P(A \mid B) = P(A) \quad\text{and}\quad P(B \mid A) = P(B).
\]</span>
Equivalently,
<span class="math display">\[
P(A \cap B) = P(A) P(B).
\]</span></p>
</div>
<p>Independence is a <strong>structural assumption</strong> about the probability model. It is not something that can be verified with certainty from finite data; rather, it is justified by the way the experiment is constructed or by scientific knowledge about the mechanism generating the data.</p>
<p>Informally, two events are independent if learning that one has occurred provides <strong>no information</strong> about whether the other occurs. In that case, knowing that event <span class="math inline">\(B\)</span> happened leaves the probability of <span class="math inline">\(A\)</span> unchanged:
<span class="math inline">\(P(A \mid B) = P(A).\)</span></p>
<p>Using the definition of conditional probability,
<span class="math inline">\(P(A \mid B) = \frac{P(A \cap B)}{P(B)},\)</span>
we see that (provided <span class="math inline">\(P(B) &gt; 0\)</span>) independence is equivalent to
<span class="math inline">\(P(A \cap B) = P(A)P(B).\)</span></p>
<p>This multiplicative rule is often the most convenient working definition: when events are independent, the probability that both occur equals the product of their marginal probabilities.</p>
<p>It is important to distinguish between:</p>
<ul>
<li><strong>Mathematical independence:</strong> a property defined within a probability model.</li>
<li><strong>Empirical evidence:</strong> data may suggest approximate independence, but cannot prove it exactly.</li>
</ul>
<p>In practice, we often assume independence because the design of the experiment makes interaction between outcomes implausible (for example, separate coin flips or independent sampling with replacement).</p>
<hr />
</div>
<div id="example-1-independent-events" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Example 1: Independent Events<a href="introduction-to-probability.html#example-1-independent-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Event A = “first die shows 6”</li>
<li>Event B = “second die shows 1”</li>
</ul>
<p>When two dice are rolled independently, the outcome of one die does not affect the outcome of the other. Thus,
<span class="math display">\[P(A)=\frac{1}{6}, \qquad P(B)=\frac{1}{6}, \qquad P(A\cap B)=\frac{1}{36}.\]</span>
We should therefore observe
<span class="math display">\[P(A\cap B)=P(A)P(B).\]</span></p>
<p>The following simulation approximates these probabilities.</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="introduction-to-probability.html#cb203-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb203-2"><a href="introduction-to-probability.html#cb203-2" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">100000</span>, <span class="cn">TRUE</span>)</span>
<span id="cb203-3"><a href="introduction-to-probability.html#cb203-3" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">100000</span>, <span class="cn">TRUE</span>)</span>
<span id="cb203-4"><a href="introduction-to-probability.html#cb203-4" tabindex="-1"></a></span>
<span id="cb203-5"><a href="introduction-to-probability.html#cb203-5" tabindex="-1"></a><span class="fu">mean</span>(d1 <span class="sc">==</span> <span class="dv">6</span> <span class="sc">&amp;</span> d2 <span class="sc">==</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.02806</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="introduction-to-probability.html#cb205-1" tabindex="-1"></a><span class="fu">mean</span>(d1 <span class="sc">==</span> <span class="dv">6</span>) <span class="sc">*</span> <span class="fu">mean</span>(d2 <span class="sc">==</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.02801439</code></pre>
<p>With a large number of repetitions, the two quantities should be very close, illustrating the multiplicative rule for independent events.</p>
<hr />
</div>
<div id="example-2-independent-events-card-draws" class="section level3 hasAnchor" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Example 2: Independent Events (Card Draws)<a href="introduction-to-probability.html#example-2-independent-events-card-draws" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Event A = “draw a heart”</li>
<li>Event B = “draw a king”</li>
</ul>
<p>If we draw <strong>with replacement</strong> (or equivalently draw from two independent decks), the events are independent. The probability of drawing a heart on one draw does not affect the probability of drawing a king on another.</p>
<p><span class="math display">\[
P(A)=\frac{13}{52}=\frac{1}{4}, \qquad
P(B)=\frac{4}{52}=\frac{1}{13}.
\]</span></p>
<p>Hence,
<span class="math display">\[
P(A\cap B)=\frac{1}{4}\cdot\frac{1}{13}=\frac{1}{52}.
\]</span></p>
<p>The simulation below draws two cards and verifies the multiplicative relationship.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="introduction-to-probability.html#cb207-1" tabindex="-1"></a>suits  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;H&quot;</span>,<span class="st">&quot;S&quot;</span>,<span class="st">&quot;D&quot;</span>,<span class="st">&quot;C&quot;</span>), <span class="at">each =</span> <span class="dv">13</span>)</span>
<span id="cb207-2"><a href="introduction-to-probability.html#cb207-2" tabindex="-1"></a>ranks  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">13</span>, <span class="dv">4</span>)</span>
<span id="cb207-3"><a href="introduction-to-probability.html#cb207-3" tabindex="-1"></a>deck   <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(suits, ranks)</span>
<span id="cb207-4"><a href="introduction-to-probability.html#cb207-4" tabindex="-1"></a></span>
<span id="cb207-5"><a href="introduction-to-probability.html#cb207-5" tabindex="-1"></a>sampA  <span class="ot">&lt;-</span> deck[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">52</span>, <span class="dv">100000</span>, <span class="cn">TRUE</span>), ]</span>
<span id="cb207-6"><a href="introduction-to-probability.html#cb207-6" tabindex="-1"></a>sampB  <span class="ot">&lt;-</span> deck[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">52</span>, <span class="dv">100000</span>, <span class="cn">TRUE</span>), ]</span>
<span id="cb207-7"><a href="introduction-to-probability.html#cb207-7" tabindex="-1"></a></span>
<span id="cb207-8"><a href="introduction-to-probability.html#cb207-8" tabindex="-1"></a><span class="fu">mean</span>(sampA<span class="sc">$</span>suits <span class="sc">==</span> <span class="st">&quot;H&quot;</span> <span class="sc">&amp;</span> sampB<span class="sc">$</span>ranks <span class="sc">==</span> <span class="dv">13</span>)</span></code></pre></div>
<pre><code>## [1] 0.01923</code></pre>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="introduction-to-probability.html#cb209-1" tabindex="-1"></a><span class="fu">mean</span>(sampA<span class="sc">$</span>suits <span class="sc">==</span> <span class="st">&quot;H&quot;</span>) <span class="sc">*</span> <span class="fu">mean</span>(sampB<span class="sc">$</span>ranks <span class="sc">==</span> <span class="dv">13</span>)</span></code></pre></div>
<pre><code>## [1] 0.01926205</code></pre>
<p>Again, the simulated joint probability should be close to the product of the marginal probabilities.</p>
<hr />
</div>
<div id="example-of-dependent-events" class="section level3 hasAnchor" number="5.5.4">
<h3><span class="header-section-number">5.5.4</span> Example of Dependent Events<a href="introduction-to-probability.html#example-of-dependent-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cards <strong>without</strong> replacement:</p>
<ul>
<li>Event A = “first card is Ace”</li>
<li>Event B = “second card is Ace”</li>
</ul>
<p>Here the events are not independent. If the first card is an Ace, there are only three Aces left in the deck, so the probability that the second card is an Ace decreases. Specifically,
<span class="math display">\[
P(B \mid A) = \frac{3}{51} \neq \frac{4}{52} = P(B).
\]</span></p>
<p>Thus,
<span class="math display">\[
P(A\cap B) = P(A)P(B \mid A) = \frac{4}{52}\cdot\frac{3}{51},
\]</span>
which is not equal to <span class="math inline">\(P(A)P(B)\)</span>.</p>
<p>The simulation below illustrates this dependence.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="introduction-to-probability.html#cb211-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb211-2"><a href="introduction-to-probability.html#cb211-2" tabindex="-1"></a>count <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb211-3"><a href="introduction-to-probability.html#cb211-3" tabindex="-1"></a>rep   <span class="ot">&lt;-</span> <span class="dv">20000</span></span>
<span id="cb211-4"><a href="introduction-to-probability.html#cb211-4" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rep){</span>
<span id="cb211-5"><a href="introduction-to-probability.html#cb211-5" tabindex="-1"></a>  sel1  <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">52</span>, <span class="at">size =</span> <span class="dv">1</span>)</span>
<span id="cb211-6"><a href="introduction-to-probability.html#cb211-6" tabindex="-1"></a>  sel2  <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="at">x =</span> (<span class="dv">1</span><span class="sc">:</span><span class="dv">52</span>)[<span class="sc">-</span>sel1], <span class="at">size =</span> <span class="dv">1</span>)</span>
<span id="cb211-7"><a href="introduction-to-probability.html#cb211-7" tabindex="-1"></a>  A     <span class="ot">&lt;-</span> deck<span class="sc">$</span>ranks[sel1] <span class="sc">==</span> <span class="dv">1</span></span>
<span id="cb211-8"><a href="introduction-to-probability.html#cb211-8" tabindex="-1"></a>  B     <span class="ot">&lt;-</span> deck<span class="sc">$</span>ranks[sel2] <span class="sc">==</span> <span class="dv">1</span></span>
<span id="cb211-9"><a href="introduction-to-probability.html#cb211-9" tabindex="-1"></a>  count <span class="ot">&lt;-</span> count <span class="sc">+</span> (A <span class="sc">&amp;</span> B)</span>
<span id="cb211-10"><a href="introduction-to-probability.html#cb211-10" tabindex="-1"></a>}</span>
<span id="cb211-11"><a href="introduction-to-probability.html#cb211-11" tabindex="-1"></a>count<span class="sc">/</span>rep</span></code></pre></div>
<pre><code>## [1] 0.00355</code></pre>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="introduction-to-probability.html#cb213-1" tabindex="-1"></a>(<span class="dv">4</span><span class="sc">/</span><span class="dv">52</span>) <span class="sc">*</span> (<span class="dv">4</span><span class="sc">/</span><span class="dv">52</span>)   <span class="co"># independence NOT satisfied</span></span></code></pre></div>
<pre><code>## [1] 0.00591716</code></pre>
<p>The simulated joint probability is smaller than the product <span class="math inline">\((4/52)^2\)</span>, reflecting the negative dependence introduced by sampling without replacement.</p>
<hr />
</div>
<div id="independence-vs.-mutually-exclusive" class="section level3 hasAnchor" number="5.5.5">
<h3><span class="header-section-number">5.5.5</span> Independence vs. Mutually Exclusive<a href="introduction-to-probability.html#independence-vs.-mutually-exclusive" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two events are <strong>mutually exclusive</strong> if they cannot occur at the same time:
<span class="math display">\[
A \cap B = \varnothing.
\]</span>
In that case,
<span class="math display">\[
P(A\cap B)=0.
\]</span></p>
<p>If both <span class="math inline">\(P(A)\)</span> and <span class="math inline">\(P(B)\)</span> are positive, then
<span class="math display">\[
P(A)P(B) &gt; 0,
\]</span>
so
<span class="math display">\[
P(A\cap B)=0 \neq P(A)P(B).
\]</span></p>
<p>Therefore, mutually exclusive events with positive probability cannot be independent.</p>
<p>The distinction is conceptual:</p>
<ul>
<li>Mutual exclusivity concerns <strong>impossibility</strong>: the events cannot happen together.</li>
<li>Independence concerns <strong>information</strong>: knowing one event occurs does not change the probability of the other.</li>
</ul>
<p>These ideas are fundamentally different. In fact, independence typically requires that events be able to occur together, whereas mutual exclusivity forbids it.</p>
<hr />
</div>
</div>
<div id="random-variables" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Random Variables<a href="introduction-to-probability.html#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random variables allow us to translate outcomes of an experiment into <strong>numerical quantities</strong> that can be analyzed algebraically. Formally, a random variable is not itself random; rather, it is a <strong>function</strong> defined on the sample space.</p>
<p>Let <span class="math inline">\(\Omega\)</span> denote the sample space of an experiment. A random variable <span class="math inline">\(X\)</span> is a function
<span class="math display">\[
X:\Omega \rightarrow \mathbb{R},
\]</span>
meaning that for each outcome <span class="math inline">\(\omega \in \Omega\)</span>, the random variable assigns a real number <span class="math inline">\(X(\omega)\)</span>.</p>
<p>This perspective is fundamental: probability is defined on events in the sample space, but statistical analysis is typically conducted using numerical values. Random variables provide the bridge between these two viewpoints.</p>
<p>Random variables are useful because they:</p>
<ul>
<li>Simplify notation,</li>
<li>Allow probability models to be expressed algebraically,</li>
<li>Make it possible to compute expectations, variances, and other summaries,</li>
<li>Are convenient for simulation and computation.</li>
</ul>
<p>Example (sum of two dice):</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="introduction-to-probability.html#cb215-1" tabindex="-1"></a>d1 <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">20</span>, <span class="cn">TRUE</span>)</span>
<span id="cb215-2"><a href="introduction-to-probability.html#cb215-2" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">20</span>, <span class="cn">TRUE</span>)</span>
<span id="cb215-3"><a href="introduction-to-probability.html#cb215-3" tabindex="-1"></a>X  <span class="ot">&lt;-</span> d1 <span class="sc">+</span> d2</span>
<span id="cb215-4"><a href="introduction-to-probability.html#cb215-4" tabindex="-1"></a>X</span></code></pre></div>
<pre><code>##  [1]  6  7  9  7  7  4  9  3  5  5  7  5  6  6  6  7  8 11  5 10</code></pre>
<p>Here the experiment consists of rolling two dice. The sample space contains ordered pairs such as <span class="math inline">\((1,4)\)</span> or <span class="math inline">\((6,6)\)</span>. The random variable <span class="math inline">\(X\)</span> assigns to each outcome the sum of the two dice. For instance, if the outcome is <span class="math inline">\((2,5)\)</span>, then <span class="math inline">\(X=7\)</span>.</p>
<p>Thus, instead of working directly with pairs of dice outcomes, we work with the numerical values of their sum.</p>
<hr />
</div>
<div id="discrete-random-variables" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Discrete Random Variables<a href="introduction-to-probability.html#discrete-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introducing-rvs-as-convenient-labels-for-outcomes" class="section level3 hasAnchor" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Introducing RVs as Convenient Labels for Outcomes<a href="introduction-to-probability.html#introducing-rvs-as-convenient-labels-for-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A random variable is a <strong>rule assigning numbers to outcomes</strong>. In many experiments, the outcomes themselves are complicated objects, but we only care about certain numerical summaries. A random variable extracts that information.</p>
<p>Examples:</p>
<ol style="list-style-type: decimal">
<li>Number of heads in 3 flips</li>
<li>Number of defective items in a batch</li>
<li>Sum of two dice</li>
<li>Number of emails received per hour</li>
</ol>
<p>In each case, the underlying experiment may be complex, but the random variable records only the quantity of interest.</p>
<p>For example, in three coin flips the sample space has <span class="math inline">\(2^3=8\)</span> outcomes, but the random variable “number of heads” takes only the values <span class="math inline">\(0,1,2,3\)</span>. Many different outcomes correspond to the same value of the random variable.</p>
<p>This many-to-one mapping is typical and useful: it reduces the complexity of the sample space while preserving the information relevant to the problem.</p>
<hr />
</div>
<div id="qualitative-random-variable" class="section level3 hasAnchor" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> Qualitative Random Variable<a href="introduction-to-probability.html#qualitative-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:qualrv" class="definition"><strong>Definition 5.23  (Random Variable (General) ) </strong></span>A random variable is a function that assigns a numerical value to each outcome of a random experiment.</p>
</div>
<p>In practice, not all variables of interest are naturally numerical. Some experiments produce <strong>categories</strong> rather than numbers. These are often called qualitative or categorical variables. Strictly speaking, probability theory still requires numerical values, but we can encode categories using labels.</p>
<p>Examples of qualitative outcomes:</p>
<ol style="list-style-type: decimal">
<li>Weather: {sunny, cloudy, rainy}</li>
<li>Political affiliation: {A, B, C}</li>
<li>Survey response: {agree, neutral, disagree}</li>
</ol>
<p>To treat these within the framework of random variables, we typically assign numerical codes (for example, 1 = sunny, 2 = cloudy, 3 = rainy). The numerical values themselves have no arithmetic meaning; they simply serve as labels.</p>
<p>Thus, qualitative variables can be represented numerically, but their interpretation remains categorical rather than quantitative.</p>
<hr />
</div>
<div id="quantitative-random-variables" class="section level3 hasAnchor" number="5.7.3">
<h3><span class="header-section-number">5.7.3</span> Quantitative Random Variables<a href="introduction-to-probability.html#quantitative-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:discrv" class="definition"><strong>Definition 5.24  (Discrete Random Variable) </strong></span>A discrete random variable is one that takes on a countable set of possible values.</p>
</div>
<p>A discrete random variable has a finite or countably infinite set of possible values. These values often arise from counting processes.</p>
<p>Examples:</p>
<ol style="list-style-type: decimal">
<li>Number of children in a family</li>
<li>Number of goals in a soccer match</li>
<li>Number of customers arriving in a minute</li>
<li>Number of defective items in a shipment</li>
</ol>
<p>In each case, the random variable can take values such as <span class="math inline">\(0,1,2,\dots\)</span>. Because the set of possible values is countable, we can assign a probability to each individual value.</p>
<p>This leads to the concept of a probability distribution for a discrete random variable.</p>
<hr />
</div>
<div id="probability-distribution-for-a-discrete-random-variable" class="section level3 hasAnchor" number="5.7.4">
<h3><span class="header-section-number">5.7.4</span> Probability Distribution for a Discrete Random Variable<a href="introduction-to-probability.html#probability-distribution-for-a-discrete-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:pmf-def" class="definition"><strong>Definition 5.25  (Probability Distribution for a Discrete RV) </strong></span>A probability distribution assigns a probability to each possible value of a discrete random variable, satisfying:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(0 \le P(X=x) \le 1\)</span></li>
<li><span class="math inline">\(\sum_x P(X=x) = 1\)</span></li>
</ol>
</div>
<p>The function <span class="math inline">\(p(x)=P(X=x)\)</span> is called the <strong>probability mass function (pmf)</strong> of the discrete random variable. It describes how probability is distributed across the possible values of <span class="math inline">\(X\)</span>.</p>
<p>Two basic requirements must hold:</p>
<ul>
<li>Each probability must lie between 0 and 1.</li>
<li>The probabilities over all possible values must sum to 1.</li>
</ul>
<p>If either condition fails, the proposed function cannot be a valid probability distribution.</p>
<div id="example-1-number-of-heads-in-3-flips" class="section level4 hasAnchor" number="5.7.4.1">
<h4><span class="header-section-number">5.7.4.1</span> Example 1: Number of Heads in 3 Flips<a href="introduction-to-probability.html#example-1-number-of-heads-in-3-flips" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(X\)</span> be the number of heads in three independent coin flips. The possible values are <span class="math inline">\(0,1,2,3\)</span>. The simulation below approximates the distribution.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="introduction-to-probability.html#cb217-1" tabindex="-1"></a>sims <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">30000</span>, <span class="fu">sum</span>(<span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dv">3</span>,<span class="cn">TRUE</span>)))</span>
<span id="cb217-2"><a href="introduction-to-probability.html#cb217-2" tabindex="-1"></a><span class="fu">table</span>(sims)<span class="sc">/</span><span class="fu">length</span>(sims)</span></code></pre></div>
<pre><code>## sims
##         0         1         2         3 
## 0.1260667 0.3727000 0.3748333 0.1264000</code></pre>
<p>The empirical probabilities should be close to:
<span class="math display">\[
P(X=0)=\tfrac{1}{8}; \quad
P(X=1)=\tfrac{3}{8}; \quad
P(X=2)=\tfrac{3}{8}; \quad
P(X=3)=\tfrac{1}{8}.
\]</span></p>
</div>
<div id="example-2-sum-of-two-dice" class="section level4 hasAnchor" number="5.7.4.2">
<h4><span class="header-section-number">5.7.4.2</span> Example 2: Sum of Two Dice<a href="introduction-to-probability.html#example-2-sum-of-two-dice" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(X\)</span> be the sum of two dice. The possible values are <span class="math inline">\(2,3,\dots,12\)</span>. The distribution is not uniform: middle values such as 6, 7, and 8 occur more frequently because more combinations produce them.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="introduction-to-probability.html#cb219-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rowSums</span>(<span class="fu">cbind</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,<span class="dv">20000</span>,<span class="cn">TRUE</span>), <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,<span class="dv">20000</span>,<span class="cn">TRUE</span>)))</span>
<span id="cb219-2"><a href="introduction-to-probability.html#cb219-2" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(X))</span></code></pre></div>
<pre><code>## X
##       2       3       4       5       6       7       8       9      10      11      12 
## 0.02985 0.05345 0.08290 0.11570 0.13940 0.16430 0.13830 0.11140 0.08005 0.05835 0.02630</code></pre>
<p>The simulation shows that <span class="math inline">\(P(X=7)\)</span> is the largest probability, reflecting the six combinations that produce a sum of 7.</p>
</div>
<div id="example-3-customers-arriving-poisson-like-simulation" class="section level4 hasAnchor" number="5.7.4.3">
<h4><span class="header-section-number">5.7.4.3</span> Example 3: Customers Arriving (Poisson-like simulation)<a href="introduction-to-probability.html#example-3-customers-arriving-poisson-like-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Counts of arrivals over time are often modeled using the Poisson distribution. The command below generates simulated counts with mean 3.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="introduction-to-probability.html#cb221-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">rpois</span>(<span class="dv">10000</span>, <span class="at">lambda =</span> <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## [1] 2.9847</code></pre>
<p>Here the random variable represents the number of arrivals in a fixed time interval. Its possible values are <span class="math inline">\(0,1,2,\dots\)</span>.</p>
<hr />
<p><strong>Important:</strong> A proposed discrete distribution must satisfy the basic probability laws. In particular, the probabilities assigned to all possible values must sum to 1. Any proposed model that violates this condition cannot represent a valid random variable.</p>
<p>Random variables and their distributions form the foundation of statistical analysis. Once outcomes are expressed numerically, we can compute expectations, variances, and other summaries, and we can build probability models for real-world phenomena.</p>
<hr />
</div>
</div>
</div>
<div id="important-discrete-distributions" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Important Discrete Distributions<a href="introduction-to-probability.html#important-discrete-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Probability distributions provide <strong>standardized probabilistic models</strong> for commonly occurring types of random variables. Instead of building a model from scratch each time, we match a real-world situation to a known distribution whose mathematical properties are already understood.</p>
<p>A useful conceptual distinction:</p>
<ul>
<li>A <strong>random variable</strong> is the numerical quantity of interest.</li>
<li>A <strong>distribution</strong> is the mathematical model describing how that quantity behaves probabilistically.</li>
</ul>
<p>Once a random variable is associated with a known distribution, we can compute probabilities, expectations, variances, and make predictions using established formulas.</p>
<hr />
<div id="the-binomial-distribution" class="section level3 hasAnchor" number="5.8.1">
<h3><span class="header-section-number">5.8.1</span> The Binomial Distribution<a href="introduction-to-probability.html#the-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The binomial distribution models the number of successes in a fixed number of independent trials, where each trial has only two possible outcomes and a constant probability of success.</p>
<div id="customers-purchases" class="section level4 hasAnchor" number="5.8.1.1">
<h4><span class="header-section-number">5.8.1.1</span> Customers Purchases<a href="introduction-to-probability.html#customers-purchases" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose each customer independently makes a purchase with probability <span class="math inline">\(0.3\)</span>.
Let <span class="math inline">\(X =\)</span> number of purchasers among <span class="math inline">\(20\)</span> customers.</p>
<p>Then <span class="math inline">\(X\)</span> counts how many of the 20 independent customers make a purchase. This is a typical binomial setting.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="introduction-to-probability.html#cb223-1" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">rbinom</span>(<span class="fl">1e5</span>,<span class="dv">20</span>,<span class="fl">0.3</span>))</span></code></pre></div>
<pre><code>## [1] 6.00541</code></pre>
<p>The simulated mean should be close to <span class="math inline">\(20 \times 0.3 = 6\)</span>, which is the theoretical expected value of a binomial random variable with parameters <span class="math inline">\(n=20\)</span> and <span class="math inline">\(p=0.3\)</span>.</p>
<hr />
</div>
<div id="definition-binomial-experiment" class="section level4 hasAnchor" number="5.8.1.2">
<h4><span class="header-section-number">5.8.1.2</span> Definition: Binomial Experiment<a href="introduction-to-probability.html#definition-binomial-experiment" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A binomial model is appropriate when the following conditions hold:</p>
<ol style="list-style-type: decimal">
<li>There is a fixed number of trials <span class="math inline">\(n\)</span>.</li>
<li>Each trial has only two possible outcomes: success or failure.</li>
<li>The probability of success <span class="math inline">\(p\)</span> is the same for every trial.</li>
<li>The trials are independent.</li>
</ol>
<p>If all four conditions are satisfied, then the total number of successes follows a binomial distribution.</p>
<hr />
</div>
<div id="independent-random-variables" class="section level4 hasAnchor" number="5.8.1.3">
<h4><span class="header-section-number">5.8.1.3</span> Independent Random Variables<a href="introduction-to-probability.html#independent-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definition">
<p><span id="def:indep-rv" class="definition"><strong>Definition 5.26  (Independence of Random Variables) </strong></span>Random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if
<span class="math display">\[
P(X=x, Y=y) = P(X=x)P(Y=y)
\]</span>
for all values <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
</div>
<p>Independence of random variables means that knowing the value of one random variable provides no information about the other. In this case, the joint probability distribution factors into the product of the marginal distributions.</p>
<p>It is important to distinguish:</p>
<ul>
<li><p><strong>Independence of events:</strong>
<span class="math display">\[
P(A \mid B) = P(A).
\]</span></p></li>
<li><p><strong>Independence of random variables:</strong>
The joint distribution factorizes into marginal distributions.</p></li>
</ul>
<p>Event independence is a statement about particular subsets of the sample space. Random-variable independence is a stronger statement about the entire joint distribution.</p>
<hr />
</div>
<div id="binomial-examples" class="section level4 hasAnchor" number="5.8.1.4">
<h4><span class="header-section-number">5.8.1.4</span> Binomial Examples<a href="introduction-to-probability.html#binomial-examples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="coin-flips-example" class="section level5 hasAnchor" number="5.8.1.4.1">
<h5><span class="header-section-number">5.8.1.4.1</span> Coin Flips Example<a href="introduction-to-probability.html#coin-flips-example" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Twenty independent coin flips with probability of heads <span class="math inline">\(p=0.5\)</span>. Let <span class="math inline">\(X\)</span> be the number of heads.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="introduction-to-probability.html#cb225-1" tabindex="-1"></a><span class="fu">rbinom</span>(<span class="dv">10</span>,<span class="dv">20</span>,<span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>##  [1] 11  8  8  6 13 12 13 11  8 11</code></pre>
<p>All binomial conditions hold: fixed <span class="math inline">\(n\)</span>, two outcomes, constant probability, independence.</p>
</div>
<div id="not-binomial-fails-identical-trials" class="section level5 hasAnchor" number="5.8.1.4.2">
<h5><span class="header-section-number">5.8.1.4.2</span> 2. Not Binomial — Fails identical trials<a href="introduction-to-probability.html#not-binomial-fails-identical-trials" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Suppose the probability of success changes from trial to trial (for example, a machine that becomes less reliable over time). Then the trials are not identical, and the binomial model does not apply.</p>
</div>
<div id="not-binomial-fails-only-two-outcomes" class="section level5 hasAnchor" number="5.8.1.4.3">
<h5><span class="header-section-number">5.8.1.4.3</span> 3. Not Binomial — Fails only two outcomes<a href="introduction-to-probability.html#not-binomial-fails-only-two-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Rolling a die produces six outcomes, not two. If we are counting something more complex than success/failure, the binomial model is inappropriate unless we recode outcomes into two categories.</p>
</div>
<div id="not-binomial-fails-same-p" class="section level5 hasAnchor" number="5.8.1.4.4">
<h5><span class="header-section-number">5.8.1.4.4</span> 4. Not Binomial — Fails same <span class="math inline">\(p\)</span><a href="introduction-to-probability.html#not-binomial-fails-same-p" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>If the probability of success changes across trials (for example, a coin that becomes increasingly biased), the binomial assumptions are violated.</p>
</div>
<div id="not-binomial-fails-independence" class="section level5 hasAnchor" number="5.8.1.4.5">
<h5><span class="header-section-number">5.8.1.4.5</span> 5. Not Binomial — Fails independence<a href="introduction-to-probability.html#not-binomial-fails-independence" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Drawing cards without replacement introduces dependence between trials. Once a card is drawn, the probabilities for subsequent draws change.</p>
<p>The binomial model is appropriate only when <strong>all</strong> four conditions are satisfied.</p>
<hr />
</div>
</div>
<div id="pmf-of-the-binomial" class="section level4 hasAnchor" number="5.8.1.5">
<h4><span class="header-section-number">5.8.1.5</span> PMF of the Binomial<a href="introduction-to-probability.html#pmf-of-the-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definition">
<p><span id="def:binompmf" class="definition"><strong>Definition 5.27  (Binomial PMF) </strong></span><span class="math display">\[
P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}
\]</span></p>
</div>
<p>Here:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of trials,</li>
<li><span class="math inline">\(p\)</span> is the probability of success on each trial,</li>
<li><span class="math inline">\(k\)</span> is the number of observed successes.</li>
</ul>
<p>The binomial coefficient
<span class="math display">\[
\binom{n}{k}
\]</span>
counts the number of ways to arrange <span class="math inline">\(k\)</span> successes among <span class="math inline">\(n\)</span> trials. The term <span class="math inline">\(p^k(1-p)^{n-k}\)</span> gives the probability of any specific sequence with <span class="math inline">\(k\)</span> successes and <span class="math inline">\(n-k\)</span> failures.</p>
<p>Multiplying these gives the probability of observing exactly <span class="math inline">\(k\)</span> successes.</p>
</div>
<div id="binomila-visualization" class="section level4 hasAnchor" number="5.8.1.6">
<h4><span class="header-section-number">5.8.1.6</span> Binomila Visualization<a href="introduction-to-probability.html#binomila-visualization" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="introduction-to-probability.html#cb227-1" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span></span>
<span id="cb227-2"><a href="introduction-to-probability.html#cb227-2" tabindex="-1"></a><span class="fu">plot</span>(k, <span class="fu">dbinom</span>(k,<span class="dv">20</span>,<span class="fl">0.3</span>), <span class="at">type=</span><span class="st">&quot;h&quot;</span>, <span class="at">lwd=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/binomial-visualization-1.png" width="672" /></p>
<p>The shape of the binomial distribution depends on <span class="math inline">\(p\)</span>:</p>
<ul>
<li><span class="math inline">\(p = 0.5\)</span> → symmetric distribution</li>
<li><span class="math inline">\(p &lt; 0.5\)</span> → right-skewed</li>
<li><span class="math inline">\(p &gt; 0.5\)</span> → left-skewed</li>
</ul>
<p>As <span class="math inline">\(n\)</span> becomes large, the binomial distribution becomes more bell-shaped, especially when <span class="math inline">\(p\)</span> is near <span class="math inline">\(0.5\)</span>.</p>
<hr />
</div>
<div id="deriving-the-poisson-from-the-binomial" class="section level4 hasAnchor" number="5.8.1.7">
<h4><span class="header-section-number">5.8.1.7</span> Deriving the Poisson from the Binomial<a href="introduction-to-probability.html#deriving-the-poisson-from-the-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Poisson distribution can be obtained as a limiting case of the binomial distribution.</p>
<p>Suppose:</p>
<ul>
<li><span class="math inline">\(n\)</span> is large,</li>
<li><span class="math inline">\(p\)</span> is small,</li>
<li><span class="math inline">\(\lambda = np\)</span> remains fixed.</li>
</ul>
<p>Then for fixed <span class="math inline">\(k\)</span>,
<span class="math display">\[
\binom{n}{k} p^k (1-p)^{n-k}
\rightarrow
\frac{e^{-\lambda}\lambda^k}{k!}.
\]</span></p>
<p>This result explains why the Poisson distribution is often used to model counts of rare events occurring in a large number of opportunities.</p>
<hr />
</div>
<div id="properties-of-the-binomial" class="section level4 hasAnchor" number="5.8.1.8">
<h4><span class="header-section-number">5.8.1.8</span> Properties of the Binomial<a href="introduction-to-probability.html#properties-of-the-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a binomial random variable <span class="math inline">\(X \sim \text{Binomial}(n,p)\)</span>:</p>
<p><span class="math display">\[
E[X] = np,
\]</span></p>
<p><span class="math display">\[
\text{Var}(X) = np(1-p),
\]</span></p>
<p><span class="math display">\[
\text{SD}(X) = \sqrt{np(1-p)}.
\]</span></p>
<p>The mean represents the expected number of successes, while the variance measures variability around that mean.</p>
<hr />
</div>
</div>
<div id="the-poisson-distribution" class="section level3 hasAnchor" number="5.8.2">
<h3><span class="header-section-number">5.8.2</span> The Poisson Distribution<a href="introduction-to-probability.html#the-poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Poisson distribution models counts of events occurring over time, space, or some other continuous dimension, when events occur randomly and independently at a constant average rate.</p>
<div id="examples" class="section level4 hasAnchor" number="5.8.2.1">
<h4><span class="header-section-number">5.8.2.1</span> Examples<a href="introduction-to-probability.html#examples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>Number of website hits per minute</li>
<li>Number of accidents per month</li>
<li>Number of emails per hour</li>
<li>Number of defects per unit length of material</li>
</ol>
<p>In each case, we count how many events occur within a fixed interval.</p>
<hr />
</div>
<div id="definition-poisson-conditions" class="section level4 hasAnchor" number="5.8.2.2">
<h4><span class="header-section-number">5.8.2.2</span> Definition: Poisson Conditions<a href="introduction-to-probability.html#definition-poisson-conditions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A Poisson model is appropriate when:</p>
<ul>
<li>Events occur independently.</li>
<li>The average rate <span class="math inline">\(\lambda\)</span> is constant over any interval of the same size.</li>
<li>Events occur one at a time.</li>
<li>The probability of more than one event in a very small interval is approximately zero.</li>
</ul>
<p>These assumptions describe a process in which events occur randomly but with a stable average rate.</p>
<hr />
</div>
<div id="pmf" class="section level4 hasAnchor" number="5.8.2.3">
<h4><span class="header-section-number">5.8.2.3</span> PMF<a href="introduction-to-probability.html#pmf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a Poisson random variable <span class="math inline">\(X\)</span> with rate parameter <span class="math inline">\(\lambda &gt; 0\)</span>,</p>
<p><span class="math display">\[
P(X=k) = \frac{e^{-\lambda}\lambda^k}{k!}, \quad k=0,1,2,\dots
\]</span></p>
<p>The parameter <span class="math inline">\(\lambda\)</span> represents the expected number of events in the interval.</p>
<p>Example simulation:</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="introduction-to-probability.html#cb228-1" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">rpois</span>(<span class="dv">10000</span>,<span class="dv">3</span>))<span class="sc">/</span><span class="dv">10000</span></span></code></pre></div>
<pre><code>## 
##      0      1      2      3      4      5      6      7      8      9     10     12 
## 0.0513 0.1495 0.2269 0.2226 0.1644 0.1066 0.0465 0.0213 0.0068 0.0032 0.0008 0.0001</code></pre>
<p>The simulated relative frequencies should approximate the theoretical Poisson probabilities with <span class="math inline">\(\lambda = 3\)</span>.</p>
<hr />
</div>
<div id="properties-of-poisson" class="section level4 hasAnchor" number="5.8.2.4">
<h4><span class="header-section-number">5.8.2.4</span> Properties of Poisson<a href="introduction-to-probability.html#properties-of-poisson" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>:</p>
<p><span class="math display">\[
E[X] = \lambda,
\]</span></p>
<p><span class="math display">\[
\text{Var}(X) = \lambda.
\]</span></p>
<p>Thus, the mean and variance of a Poisson distribution are equal. This property is often used as a diagnostic when assessing whether a Poisson model is reasonable for observed data.</p>
<hr />
</div>
</div>
</div>
<div id="continuous-random-variables" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Continuous Random Variables<a href="introduction-to-probability.html#continuous-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="from-discrete-to-continuous" class="section level3 hasAnchor" number="5.9.1">
<h3><span class="header-section-number">5.9.1</span> From Discrete to Continuous<a href="introduction-to-probability.html#from-discrete-to-continuous" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Up to this point we have assigned probability directly to individual values of a random variable. This works naturally for <strong>discrete</strong> random variables, such as the number of heads in coin flips or the number of customers arriving in a store. In those settings, it makes sense to ask for probabilities like <span class="math inline">\(P(X=3)\)</span> or <span class="math inline">\(P(X=10)\)</span>.</p>
<p>Many quantities in science and data analysis, however, are not naturally discrete. Measurements such as time, temperature, weight, voltage, and distance vary on a continuum. When we measure them, we record a finite number of decimal places, but conceptually they can take any value within an interval. These are modeled using <strong>continuous random variables</strong>.</p>
<p>A continuous random variable takes values on an interval (or a union of intervals) of the real line. Unlike discrete random variables, which take a countable set of values, a continuous random variable can take <strong>uncountably many</strong> possible values.</p>
<p>A key conceptual difference is that for a continuous random variable,
<span class="math display">\[
P(X = x) = 0
\]</span>
for any specific value <span class="math inline">\(x\)</span>. Individual points have zero probability because probability is spread continuously across an interval rather than concentrated at individual values.</p>
<p>This can initially feel counterintuitive. If <span class="math inline">\(X\)</span> represents a measurement such as height, surely someone could have height exactly <span class="math inline">\(170\)</span> cm. But in a continuous model, the probability of observing that exact number is zero because there are infinitely many possible values near 170, and probability is distributed across all of them.</p>
<p>The appropriate question is not “What is the probability that <span class="math inline">\(X\)</span> equals exactly 170?” but rather:</p>
<p><span class="math display">\[
P(169.5 \le X \le 170.5).
\]</span></p>
<p>We assign probability to <strong>intervals</strong>, not individual points.</p>
<p>Simulation example:</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="introduction-to-probability.html#cb230-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb230-2"><a href="introduction-to-probability.html#cb230-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="at">n =</span> <span class="dv">100000</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">100</span>)</span>
<span id="cb230-3"><a href="introduction-to-probability.html#cb230-3" tabindex="-1"></a><span class="co"># Zero Probability for a Point</span></span>
<span id="cb230-4"><a href="introduction-to-probability.html#cb230-4" tabindex="-1"></a>pro50 <span class="ot">&lt;-</span> <span class="fu">mean</span>(x <span class="sc">==</span> <span class="dv">50</span>)  <span class="co"># always 0</span></span>
<span id="cb230-5"><a href="introduction-to-probability.html#cb230-5" tabindex="-1"></a><span class="co"># Non-zero probability for an Interval</span></span>
<span id="cb230-6"><a href="introduction-to-probability.html#cb230-6" tabindex="-1"></a>proInt <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="dv">49</span> <span class="sc">&lt;</span> x <span class="sc">&amp;</span> x <span class="sc">&lt;</span> <span class="dv">51</span>)</span>
<span id="cb230-7"><a href="introduction-to-probability.html#cb230-7" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Probability getting exactly 50: &quot;</span>, <span class="fu">round</span>(pro50 <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span>))</span></code></pre></div>
<pre><code>## [1] &quot;Probability getting exactly 50: 0%&quot;</code></pre>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="introduction-to-probability.html#cb232-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Probability getting around  50: &quot;</span>, <span class="fu">round</span>(proInt <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span>))</span></code></pre></div>
<pre><code>## [1] &quot;Probability getting around  50: 2.03%&quot;</code></pre>
<p>The simulation shows two important facts:</p>
<ol style="list-style-type: decimal">
<li>The probability of observing <strong>exactly</strong> 50 is essentially zero.</li>
<li>The probability of observing a value <strong>near</strong> 50 is positive.</li>
</ol>
<p>Even though values close to 50 occur frequently, the probability of observing exactly 50 is zero in a continuous model.</p>
<p>Because single points have zero probability, probability must be assigned to <strong>intervals</strong> rather than individual values. For example,</p>
<p><span class="math display">\[
P(a \le X \le b)
\]</span></p>
<p>represents the probability that <span class="math inline">\(X\)</span> falls within an interval. Graphically, this probability corresponds to the <strong>area under a curve</strong> between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>This shift—from point probabilities to interval probabilities—is the defining feature of continuous distributions and marks the transition from discrete probability to continuous probability.</p>
<hr />
</div>
<div id="pdf-and-cdf" class="section level3 hasAnchor" number="5.9.2">
<h3><span class="header-section-number">5.9.2</span> PDF and CDF<a href="introduction-to-probability.html#pdf-and-cdf" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A continuous distribution is described using two related functions:</p>
<ul>
<li>the <strong>probability density function (PDF)</strong> <span class="math inline">\(f(x)\)</span></li>
<li>the <strong>cumulative distribution function (CDF)</strong> <span class="math inline">\(F(x)\)</span></li>
</ul>
<p>These functions play complementary roles.</p>
<div id="probability-density-function-pdf" class="section level4 hasAnchor" number="5.9.2.1">
<h4><span class="header-section-number">5.9.2.1</span> Probability Density Function (PDF)<a href="introduction-to-probability.html#probability-density-function-pdf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The PDF, denoted <span class="math inline">\(f(x)\)</span>, describes how probability is distributed across values of <span class="math inline">\(x\)</span>. It is a nonnegative function satisfying</p>
<p><span class="math display">\[
f(x) \ge 0
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\int_{-\infty}^{\infty} f(x)dx = 1.
\]</span></p>
<p>The PDF itself does not give probabilities directly. Instead, probabilities are obtained by integrating the density over an interval:</p>
<p><span class="math display">\[
P(a \le X \le b) = \int_a^b f(x)dx.
\]</span></p>
<p>Thus, probability corresponds to <strong>area under the density curve</strong>. Regions where the density is high correspond to values of <span class="math inline">\(X\)</span> that are more likely to occur.</p>
</div>
<div id="cumulative-distribution-function-cdf" class="section level4 hasAnchor" number="5.9.2.2">
<h4><span class="header-section-number">5.9.2.2</span> Cumulative Distribution Function (CDF)<a href="introduction-to-probability.html#cumulative-distribution-function-cdf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The CDF is defined as</p>
<p><span class="math display">\[
F(x) = P(X \le x).
\]</span></p>
<p>It gives the total probability accumulated up to the point <span class="math inline">\(x\)</span>. For a continuous random variable, the CDF can be written in terms of the PDF:</p>
<p><span class="math display">\[
F(x) = \int_{-\infty}^{x} f(t)dt.
\]</span></p>
<p>The CDF has several important properties:</p>
<ul>
<li>It is nondecreasing.</li>
<li>It approaches 0 as <span class="math inline">\(x \to -\infty\)</span>.</li>
<li>It approaches 1 as <span class="math inline">\(x \to \infty\)</span>.</li>
</ul>
<p>Probabilities for intervals can be computed using the CDF:</p>
<p><span class="math display">\[
P(a \le X \le b) = F(b) - F(a).
\]</span></p>
<p>Thus, the CDF accumulates probability up to a given point, while the PDF describes how probability density is distributed across values.</p>
</div>
<div id="example-normal-distribution" class="section level4 hasAnchor" number="5.9.2.3">
<h4><span class="header-section-number">5.9.2.3</span> Example: Normal Distribution<a href="introduction-to-probability.html#example-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The following code visualizes both the PDF and CDF for a normal distribution.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="introduction-to-probability.html#cb234-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x), <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-pdf-cdf-normal-distribution-example-1.png" width="672" /></p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="introduction-to-probability.html#cb235-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">pnorm</span>(x), <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-pdf-cdf-normal-distribution-example-2.png" width="672" /></p>
<p>The first curve is the PDF of a normal distribution. It shows the familiar bell-shaped density.</p>
<p>The second curve is the CDF. It starts near 0, increases steadily, and approaches 1 as <span class="math inline">\(x\)</span> increases. Each value of the CDF represents the probability that the random variable is less than or equal to that value.</p>
<p>Together, the PDF and CDF provide two complementary ways of describing a continuous random variable:</p>
<ul>
<li>The PDF describes how probability is distributed.</li>
<li>The CDF describes how probability accumulates.</li>
</ul>
</div>
</div>
<div id="interpreting-areas-as-probabilities" class="section level3 hasAnchor" number="5.9.3">
<h3><span class="header-section-number">5.9.3</span> Interpreting Areas as Probabilities<a href="introduction-to-probability.html#interpreting-areas-as-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A useful way to think about a continuous distribution is to imagine probability as <strong>area under a curve</strong>. The total area under the PDF is 1, representing total probability. Any subinterval corresponds to a portion of that area.</p>
<p>For example, consider a normal distribution again. The shaded region between two values represents the probability that the random variable falls in that interval.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="introduction-to-probability.html#cb236-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x), <span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb236-2"><a href="introduction-to-probability.html#cb236-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;Probability as Area Under the Curve&quot;</span>,</span>
<span id="cb236-3"><a href="introduction-to-probability.html#cb236-3" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Density&quot;</span>)</span>
<span id="cb236-4"><a href="introduction-to-probability.html#cb236-4" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb236-5"><a href="introduction-to-probability.html#cb236-5" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb236-6"><a href="introduction-to-probability.html#cb236-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(a, b, <span class="at">length=</span><span class="dv">200</span>)</span>
<span id="cb236-7"><a href="introduction-to-probability.html#cb236-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x)</span>
<span id="cb236-8"><a href="introduction-to-probability.html#cb236-8" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(a, x, b), <span class="fu">c</span>(<span class="dv">0</span>, y, <span class="dv">0</span>), <span class="at">col=</span><span class="st">&quot;lightblue&quot;</span>)</span>
<span id="cb236-9"><a href="introduction-to-probability.html#cb236-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">c</span>(a,b), <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-probability-of-an-interval-normal-1.png" width="672" /></p>
<p>This shaded area corresponds to
<span class="math display">\[
P(-1 \le X \le 1).
\]</span></p>
<p>We compute it numerically using the CDF:</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="introduction-to-probability.html#cb237-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">1</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.6826895</code></pre>
<p>Even though the PDF at a point gives the height of the curve, probability is always determined by <strong>area</strong>, not height. This distinction is essential:</p>
<ul>
<li><span class="math inline">\(f(x)\)</span> is not a probability.</li>
<li>Probabilities come from integrals (areas).</li>
</ul>
<hr />
</div>
<div id="why-the-pdf-can-be-greater-than-1" class="section level3 hasAnchor" number="5.9.4">
<h3><span class="header-section-number">5.9.4</span> Why the PDF Can Be Greater Than 1<a href="introduction-to-probability.html#why-the-pdf-can-be-greater-than-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Students often expect probabilities to be at most 1, and they are. However, the <strong>density</strong> <span class="math inline">\(f(x)\)</span> itself can be greater than 1 in some distributions.</p>
<p>For example, consider a uniform distribution on a very small interval:</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="introduction-to-probability.html#cb239-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dunif</span>(x, <span class="dv">0</span>, <span class="fl">0.2</span>), <span class="at">from=</span><span class="sc">-</span><span class="fl">0.1</span>, <span class="at">to=</span><span class="fl">0.3</span>, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb239-2"><a href="introduction-to-probability.html#cb239-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;Uniform Density on a Small Interval&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-density-curve-uniform-1.png" width="672" /></p>
<p>The height of this density is <span class="math inline">\(1/0.2 = 5\)</span>, which is greater than 1. This is not a problem because probability is area:</p>
<p><span class="math display">\[
\text{area} = \text{height} \times \text{width} = 5 \times 0.2 = 1.
\]</span></p>
<p>Thus:</p>
<ul>
<li>The PDF describes density.</li>
<li>Probability corresponds to area under the curve.</li>
</ul>
<hr />
</div>
<div id="from-simulation-to-density" class="section level3 hasAnchor" number="5.9.5">
<h3><span class="header-section-number">5.9.5</span> From Simulation to Density<a href="introduction-to-probability.html#from-simulation-to-density" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A computational approach helps build intuition. We can simulate many observations and compare a histogram with a theoretical density.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="introduction-to-probability.html#cb240-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb240-2"><a href="introduction-to-probability.html#cb240-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>)</span>
<span id="cb240-3"><a href="introduction-to-probability.html#cb240-3" tabindex="-1"></a></span>
<span id="cb240-4"><a href="introduction-to-probability.html#cb240-4" tabindex="-1"></a><span class="fu">hist</span>(x, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">40</span>,</span>
<span id="cb240-5"><a href="introduction-to-probability.html#cb240-5" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Histogram and Density&quot;</span>,</span>
<span id="cb240-6"><a href="introduction-to-probability.html#cb240-6" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;lightgray&quot;</span>, <span class="at">border=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb240-7"><a href="introduction-to-probability.html#cb240-7" tabindex="-1"></a></span>
<span id="cb240-8"><a href="introduction-to-probability.html#cb240-8" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x), <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-histogram-density-comparisson-1.png" width="672" /></p>
<p>The histogram approximates the distribution of simulated data. As the sample size increases:</p>
<ul>
<li>The histogram becomes smoother.</li>
<li>It approaches the theoretical density curve.</li>
</ul>
<p>This demonstrates an important principle:</p>
<blockquote>
<p>A probability density function can be viewed as the limiting shape of a histogram as the number of observations grows and bin widths shrink.</p>
</blockquote>
<hr />
</div>
<div id="computing-interval-probabilities-in-r" class="section level3 hasAnchor" number="5.9.6">
<h3><span class="header-section-number">5.9.6</span> Computing Interval Probabilities in R<a href="introduction-to-probability.html#computing-interval-probabilities-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In practice, we compute probabilities using built-in functions for distributions.</p>
<p>General pattern:</p>
<ul>
<li><code>d*</code> gives the density (PDF)</li>
<li><code>p*</code> gives cumulative probabilities (CDF)</li>
<li><code>q*</code> gives quantiles</li>
<li><code>r*</code> generates random values</li>
</ul>
<p>For the normal distribution:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="introduction-to-probability.html#cb241-1" tabindex="-1"></a><span class="co"># Density at a point</span></span>
<span id="cb241-2"><a href="introduction-to-probability.html#cb241-2" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0.3989423</code></pre>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="introduction-to-probability.html#cb243-1" tabindex="-1"></a><span class="co"># Probability up to a point</span></span>
<span id="cb243-2"><a href="introduction-to-probability.html#cb243-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.9331928</code></pre>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="introduction-to-probability.html#cb245-1" tabindex="-1"></a><span class="co"># Probability between two values</span></span>
<span id="cb245-2"><a href="introduction-to-probability.html#cb245-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">2</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.8185946</code></pre>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="introduction-to-probability.html#cb247-1" tabindex="-1"></a><span class="co"># Random sample</span></span>
<span id="cb247-2"><a href="introduction-to-probability.html#cb247-2" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] -1.5163733  0.6291412 -1.6781940  1.1797811  1.1176545</code></pre>
<p>These functions exist for many distributions and allow us to move seamlessly between:</p>
<ul>
<li>Simulation</li>
<li>Visualization</li>
<li>Analytical probability</li>
</ul>
<hr />
</div>
<div id="key-conceptual-shift" class="section level3 hasAnchor" number="5.9.7">
<h3><span class="header-section-number">5.9.7</span> Key Conceptual Shift<a href="introduction-to-probability.html#key-conceptual-shift" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For discrete random variables:</p>
<ul>
<li>Probability is assigned to individual values.</li>
<li>We sum probabilities.</li>
</ul>
<p>For continuous random variables:</p>
<ul>
<li>Probability is assigned to intervals.</li>
<li>We integrate densities.</li>
</ul>
<p>In summary:</p>
<p><span class="math display">\[
P(X = x) = 0
\]</span></p>
<p><span class="math display">\[
P(a \le X \le b) = \int_a^b f(x)dx = F(b) - F(a).
\]</span></p>
<p>This framework—probability as accumulated area—will guide everything that follows when we study specific continuous distributions.</p>
<hr />
</div>
</div>
<div id="important-continuous-distributions" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> Important Continuous Distributions<a href="introduction-to-probability.html#important-continuous-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-uniform-distribution" class="section level3 hasAnchor" number="5.10.1">
<h3><span class="header-section-number">5.10.1</span> The Uniform Distribution<a href="introduction-to-probability.html#the-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The uniform distribution is the most intuitive continuous distribution. Every value in an interval is equally likely.</p>
<div id="discrete-uniform" class="section level4 hasAnchor" number="5.10.1.1">
<h4><span class="header-section-number">5.10.1.1</span> Discrete Uniform<a href="introduction-to-probability.html#discrete-uniform" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Rolling a fair die is a discrete uniform distribution: each value has equal probability.</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="introduction-to-probability.html#cb249-1" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,<span class="dv">10000</span>,<span class="cn">TRUE</span>))<span class="sc">/</span><span class="dv">10000</span></span></code></pre></div>
<pre><code>## 
##      1      2      3      4      5      6      7      8      9     10 
## 0.1007 0.0958 0.1017 0.1004 0.1002 0.0986 0.1087 0.0987 0.0982 0.0970</code></pre>
</div>
<div id="continuous-uniform" class="section level4 hasAnchor" number="5.10.1.2">
<h4><span class="header-section-number">5.10.1.2</span> Continuous Uniform<a href="introduction-to-probability.html#continuous-uniform" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A continuous uniform distribution on <span class="math inline">\([a,b]\)</span> has density</p>
<p><span class="math display">\[
f(x) = \frac{1}{b-a}, \quad a \le x \le b.
\]</span></p>
<p>Simulation:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="introduction-to-probability.html#cb251-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">10000</span>,<span class="dv">0</span>,<span class="dv">10</span>)</span>
<span id="cb251-2"><a href="introduction-to-probability.html#cb251-2" tabindex="-1"></a><span class="fu">hist</span>(x, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">30</span>)</span>
<span id="cb251-3"><a href="introduction-to-probability.html#cb251-3" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dunif</span>(x,<span class="dv">0</span>,<span class="dv">10</span>), <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-continuos-uniform-distribution-1.png" width="672" /></p>
</div>
<div id="from-discrete-to-continuous-uniform" class="section level4 hasAnchor" number="5.10.1.3">
<h4><span class="header-section-number">5.10.1.3</span> From Discrete to Continuous Uniform<a href="introduction-to-probability.html#from-discrete-to-continuous-uniform" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If we refine a discrete uniform grid enough, it begins to resemble a continuous uniform distribution.</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="introduction-to-probability.html#cb252-1" tabindex="-1"></a>samSiz <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb252-2"><a href="introduction-to-probability.html#cb252-2" tabindex="-1"></a>x      <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="at">length=</span>samSiz), samSiz, <span class="cn">TRUE</span>)</span>
<span id="cb252-3"><a href="introduction-to-probability.html#cb252-3" tabindex="-1"></a><span class="fu">hist</span>(x, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">30</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-discrete-uniform-distribution-1.png" width="672" /></p>
<p>On a fine grid, discrete and continuous uniform models can look nearly identical.</p>
<hr />
</div>
</div>
<div id="the-normal-distribution" class="section level3 hasAnchor" number="5.10.2">
<h3><span class="header-section-number">5.10.2</span> The Normal Distribution<a href="introduction-to-probability.html#the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The normal distribution is the most important distribution in statistics. It serves both as a theoretical model and as a practical approximation for many real-world phenomena.</p>
<p>It appears in:</p>
<ul>
<li>measurement error</li>
<li>biological traits</li>
<li>averages and sample means</li>
<li>noise in physical and engineering systems</li>
</ul>
<p>Because of its mathematical properties and frequent appearance in data, the normal distribution is a central reference point for statistical modeling.</p>
<hr />
<div id="where-does-it-come-from" class="section level4 hasAnchor" number="5.10.2.1">
<h4><span class="header-section-number">5.10.2.1</span> Where Does It Come From?<a href="introduction-to-probability.html#where-does-it-come-from" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The normal distribution often arises as the sum of many small, independent effects. Each individual effect may not be normal, but their sum tends to be approximately normal. This phenomenon is formalized by the <strong>Central Limit Theorem</strong> (which we will study later).</p>
<p>A computational illustration helps build intuition.</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="introduction-to-probability.html#cb253-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb253-2"><a href="introduction-to-probability.html#cb253-2" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">sum</span>(<span class="fu">runif</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb253-3"><a href="introduction-to-probability.html#cb253-3" tabindex="-1"></a></span>
<span id="cb253-4"><a href="introduction-to-probability.html#cb253-4" tabindex="-1"></a><span class="fu">hist</span>(s, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">40</span>,</span>
<span id="cb253-5"><a href="introduction-to-probability.html#cb253-5" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Sum of Uniform Variables&quot;</span>,</span>
<span id="cb253-6"><a href="introduction-to-probability.html#cb253-6" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;lightgray&quot;</span>, <span class="at">border=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb253-7"><a href="introduction-to-probability.html#cb253-7" tabindex="-1"></a></span>
<span id="cb253-8"><a href="introduction-to-probability.html#cb253-8" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="fu">mean</span>(s), <span class="fu">sd</span>(s)),</span>
<span id="cb253-9"><a href="introduction-to-probability.html#cb253-9" tabindex="-1"></a>      <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-sum-of-uniforms-distribution-1.png" width="672" /></p>
<p>Each observation in <code>s</code> is the sum of 10 uniform random variables. Even though the underlying variables are not normal, the distribution of their sum is approximately normal.</p>
<p>This idea explains why normal distributions appear so often:</p>
<blockquote>
<p>Many processes are influenced by numerous small, independent factors. Their combined effect tends to be approximately normal.</p>
</blockquote>
<hr />
</div>
<div id="the-normal-curve" class="section level4 hasAnchor" number="5.10.2.2">
<h4><span class="header-section-number">5.10.2.2</span> The Normal Curve<a href="introduction-to-probability.html#the-normal-curve" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, its probability density function is</p>
<p><span class="math display">\[
f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/(2\sigma^2)}.
\]</span></p>
<p>The parameters have clear interpretations:</p>
<ul>
<li><span class="math inline">\(\mu\)</span> controls the <strong>center</strong> (location)</li>
<li><span class="math inline">\(\sigma\)</span> controls the <strong>spread</strong> (scale)</li>
</ul>
<p>Visualization:</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="introduction-to-probability.html#cb254-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x,<span class="dv">0</span>,<span class="dv">1</span>), <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb254-2"><a href="introduction-to-probability.html#cb254-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;Standard Normal Density&quot;</span>,</span>
<span id="cb254-3"><a href="introduction-to-probability.html#cb254-3" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Density&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-normal-distribution-shape-1.png" width="672" /></p>
<p>This is the <strong>standard normal distribution</strong>, which has <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>.</p>
<p>Changing parameters shifts or stretches the curve:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="introduction-to-probability.html#cb255-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x,<span class="dv">0</span>,<span class="dv">1</span>), <span class="sc">-</span><span class="dv">6</span>,<span class="dv">6</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;black&quot;</span>,</span>
<span id="cb255-2"><a href="introduction-to-probability.html#cb255-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;Effect of Mean and SD&quot;</span>)</span>
<span id="cb255-3"><a href="introduction-to-probability.html#cb255-3" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x,<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb255-4"><a href="introduction-to-probability.html#cb255-4" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x,<span class="dv">0</span>,<span class="dv">2</span>), <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb255-5"><a href="introduction-to-probability.html#cb255-5" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,</span>
<span id="cb255-6"><a href="introduction-to-probability.html#cb255-6" tabindex="-1"></a>       <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;N(0,1)&quot;</span>, <span class="st">&quot;N(2,1)&quot;</span>, <span class="st">&quot;N(0,2)&quot;</span>),</span>
<span id="cb255-7"><a href="introduction-to-probability.html#cb255-7" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),</span>
<span id="cb255-8"><a href="introduction-to-probability.html#cb255-8" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-normal-distribution-shift-and-stretch-1.png" width="672" /></p>
<ul>
<li>Changing <span class="math inline">\(\mu\)</span> shifts the curve horizontally.</li>
<li>Changing <span class="math inline">\(\sigma\)</span> stretches or compresses the curve.</li>
</ul>
<hr />
</div>
<div id="description-of-the-normal-distribution" class="section level4 hasAnchor" number="5.10.2.3">
<h4><span class="header-section-number">5.10.2.3</span> Description of the Normal Distribution<a href="introduction-to-probability.html#description-of-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Key properties:</p>
<ul>
<li>symmetric around <span class="math inline">\(\mu\)</span></li>
<li>bell-shaped</li>
<li>unimodal (one peak)</li>
<li>mean = median = mode</li>
<li>tails extend indefinitely in both directions</li>
</ul>
<p>Although the curve extends to infinity, most probability mass is concentrated near the center.</p>
<hr />
</div>
<div id="standardization" class="section level4 hasAnchor" number="5.10.2.4">
<h4><span class="header-section-number">5.10.2.4</span> Standardization<a href="introduction-to-probability.html#standardization" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A crucial idea is <strong>standardizing</strong> a normal random variable.</p>
<p>If <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, define</p>
<p><span class="math display">\[
Z = \frac{X-\mu}{\sigma}.
\]</span></p>
<p>Then <span class="math inline">\(Z \sim N(0,1)\)</span>.</p>
<p>This allows any normal probability to be computed using the standard normal distribution:</p>
<p><span class="math display">\[
P(a \le X \le b)
=
P\left(\frac{a-\mu}{\sigma} \le Z \le \frac{b-\mu}{\sigma}\right).
\]</span></p>
<p>In R:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="introduction-to-probability.html#cb256-1" tabindex="-1"></a><span class="co"># Example: X ~ N(10, 4^2)</span></span>
<span id="cb256-2"><a href="introduction-to-probability.html#cb256-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">12</span>, <span class="at">mean=</span><span class="dv">10</span>, <span class="at">sd=</span><span class="dv">4</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">8</span>, <span class="at">mean=</span><span class="dv">10</span>, <span class="at">sd=</span><span class="dv">4</span>)</span></code></pre></div>
<pre><code>## [1] 0.3829249</code></pre>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="introduction-to-probability.html#cb258-1" tabindex="-1"></a><span class="co"># Using standardization</span></span>
<span id="cb258-2"><a href="introduction-to-probability.html#cb258-2" tabindex="-1"></a><span class="fu">pnorm</span>((<span class="dv">12-10</span>)<span class="sc">/</span><span class="dv">4</span>) <span class="sc">-</span> <span class="fu">pnorm</span>((<span class="dv">8-10</span>)<span class="sc">/</span><span class="dv">4</span>)</span></code></pre></div>
<pre><code>## [1] 0.3829249</code></pre>
<p>Both calculations give the same result.</p>
<hr />
</div>
<div id="rule" class="section level4 hasAnchor" number="5.10.2.5">
<h4><span class="header-section-number">5.10.2.5</span> 68–95–99.7 Rule<a href="introduction-to-probability.html#rule" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a normal distribution:</p>
<p><span class="math display">\[
P(\mu \pm \sigma) \approx 0.68
\]</span></p>
<p><span class="math display">\[
P(\mu \pm 2\sigma) \approx 0.95
\]</span></p>
<p><span class="math display">\[
P(\mu \pm 3\sigma) \approx 0.997
\]</span></p>
<p>We can verify computationally for the standard normal:</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="introduction-to-probability.html#cb260-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">1</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.6826895</code></pre>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="introduction-to-probability.html#cb262-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">2</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.9544997</code></pre>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="introduction-to-probability.html#cb264-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">3</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.9973002</code></pre>
<p>Visualization:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="introduction-to-probability.html#cb266-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x), <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb266-2"><a href="introduction-to-probability.html#cb266-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;68-95-99.7 Rule&quot;</span>)</span>
<span id="cb266-3"><a href="introduction-to-probability.html#cb266-3" tabindex="-1"></a></span>
<span id="cb266-4"><a href="introduction-to-probability.html#cb266-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="at">length=</span><span class="dv">200</span>)</span>
<span id="cb266-5"><a href="introduction-to-probability.html#cb266-5" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,x,<span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">dnorm</span>(x),<span class="dv">0</span>), <span class="at">col=</span><span class="st">&quot;lightblue&quot;</span>)</span>
<span id="cb266-6"><a href="introduction-to-probability.html#cb266-6" tabindex="-1"></a></span>
<span id="cb266-7"><a href="introduction-to-probability.html#cb266-7" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="at">length=</span><span class="dv">200</span>)</span>
<span id="cb266-8"><a href="introduction-to-probability.html#cb266-8" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,x2,<span class="dv">2</span>), <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">dnorm</span>(x2),<span class="dv">0</span>), <span class="at">col=</span><span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.2</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-68-95-99-rule-plot-1.png" width="672" /></p>
<p>These percentages provide quick approximations for probabilities without computation.</p>
<hr />
</div>
<div id="computing-probabilities-in-r" class="section level4 hasAnchor" number="5.10.2.6">
<h4><span class="header-section-number">5.10.2.6</span> Computing Probabilities in R<a href="introduction-to-probability.html#computing-probabilities-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a normal distribution:</p>
<ul>
<li><code>dnorm(x, mean, sd)</code> → density</li>
<li><code>pnorm(x, mean, sd)</code> → cumulative probability</li>
<li><code>qnorm(p, mean, sd)</code> → quantile</li>
<li><code>rnorm(n, mean, sd)</code> → simulation</li>
</ul>
<p>Example:</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="introduction-to-probability.html#cb267-1" tabindex="-1"></a><span class="co"># Probability between 0.5 and 1.5</span></span>
<span id="cb267-2"><a href="introduction-to-probability.html#cb267-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.5</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.2417303</code></pre>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="introduction-to-probability.html#cb269-1" tabindex="-1"></a><span class="co"># Upper tail probability</span></span>
<span id="cb269-2"><a href="introduction-to-probability.html#cb269-2" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.02275013</code></pre>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="introduction-to-probability.html#cb271-1" tabindex="-1"></a><span class="co"># Lower tail probability</span></span>
<span id="cb271-2"><a href="introduction-to-probability.html#cb271-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.1586553</code></pre>
<hr />
</div>
<div id="three-example-calculations" class="section level4 hasAnchor" number="5.10.2.7">
<h4><span class="header-section-number">5.10.2.7</span> Three Example Calculations<a href="introduction-to-probability.html#three-example-calculations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="introduction-to-probability.html#cb273-1" tabindex="-1"></a><span class="co"># Example 1</span></span>
<span id="cb273-2"><a href="introduction-to-probability.html#cb273-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.8413447</code></pre>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="introduction-to-probability.html#cb275-1" tabindex="-1"></a><span class="co"># Example 2</span></span>
<span id="cb275-2"><a href="introduction-to-probability.html#cb275-2" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.02275013</code></pre>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="introduction-to-probability.html#cb277-1" tabindex="-1"></a><span class="co"># Example 3</span></span>
<span id="cb277-2"><a href="introduction-to-probability.html#cb277-2" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">0.5</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.5328072</code></pre>
<p>These represent:</p>
<ul>
<li>probability below a value</li>
<li>probability above a value</li>
<li>probability between two values</li>
</ul>
<hr />
</div>
<div id="from-histogram-to-density" class="section level4 hasAnchor" number="5.10.2.8">
<h4><span class="header-section-number">5.10.2.8</span> From Histogram to Density<a href="introduction-to-probability.html#from-histogram-to-density" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A histogram of simulated normal data approximates the theoretical density.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="introduction-to-probability.html#cb279-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb279-2"><a href="introduction-to-probability.html#cb279-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>)</span>
<span id="cb279-3"><a href="introduction-to-probability.html#cb279-3" tabindex="-1"></a></span>
<span id="cb279-4"><a href="introduction-to-probability.html#cb279-4" tabindex="-1"></a><span class="fu">hist</span>(x, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">30</span>,</span>
<span id="cb279-5"><a href="introduction-to-probability.html#cb279-5" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Histogram Approximating a Density&quot;</span>,</span>
<span id="cb279-6"><a href="introduction-to-probability.html#cb279-6" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;lightgray&quot;</span>, <span class="at">border=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb279-7"><a href="introduction-to-probability.html#cb279-7" tabindex="-1"></a></span>
<span id="cb279-8"><a href="introduction-to-probability.html#cb279-8" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="fu">mean</span>(x), <span class="fu">sd</span>(x)),</span>
<span id="cb279-9"><a href="introduction-to-probability.html#cb279-9" tabindex="-1"></a>      <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-histogram-density-approximation-1.png" width="672" /></p>
<p>As sample size increases:</p>
<ul>
<li>the histogram becomes smoother</li>
<li>it approaches the theoretical density</li>
</ul>
<p>This illustrates how densities describe the limiting behavior of large samples.</p>
<hr />
</div>
<div id="are-observations-from-the-same-variable" class="section level4 hasAnchor" number="5.10.2.9">
<h4><span class="header-section-number">5.10.2.9</span> Are Observations From the Same Variable?<a href="introduction-to-probability.html#are-observations-from-the-same-variable" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A histogram may represent:</p>
<ul>
<li>repeated measurements of one variable</li>
<li>measurements from different individuals</li>
</ul>
<p>For example:</p>
<ul>
<li>heights of many people</li>
<li>repeated measurements of one instrument</li>
<li>test scores from many students</li>
</ul>
<p>In all cases, the normal distribution can serve as a useful model. The interpretation depends on context:</p>
<ul>
<li>One variable measured repeatedly</li>
<li>Many individuals with similar variability</li>
</ul>
<p>The key is whether the distribution of values is approximately normal.</p>
<hr />
</div>
<div id="why-the-normal-distribution-matters" class="section level4 hasAnchor" number="5.10.2.10">
<h4><span class="header-section-number">5.10.2.10</span> Why the Normal Distribution Matters<a href="introduction-to-probability.html#why-the-normal-distribution-matters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The normal distribution is fundamental because:</p>
<ol style="list-style-type: decimal">
<li>It models many natural phenomena.</li>
<li>It arises from sums of small effects.</li>
<li>It provides approximations for many other distributions.</li>
<li>It underlies much of statistical inference.</li>
</ol>
<p>Most statistical methods—confidence intervals, hypothesis tests, regression—rely on normal-based reasoning. Understanding the normal distribution is therefore essential before moving forward to other continuous distributions.</p>
<hr />
</div>
</div>
<div id="chi-squared-distribution" class="section level3 hasAnchor" number="5.10.3">
<h3><span class="header-section-number">5.10.3</span> Chi-Squared Distribution<a href="introduction-to-probability.html#chi-squared-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One advantage of working with random variables is that we can form <strong>transformations</strong> of existing variables and study the resulting distributions. Many important distributions arise as functions of simpler ones.</p>
<p>A central example comes from the normal distribution.</p>
<p>If <span class="math inline">\(Z \sim N(0,1)\)</span>, then</p>
<p><span class="math display">\[
Z^2
\]</span></p>
<p>follows a chi-squared distribution with 1 degree of freedom. We write</p>
<p><span class="math display">\[
Z^2 \sim \chi^2_1.
\]</span></p>
<p>More generally, if <span class="math inline">\(Z_1,\dots,Z_k\)</span> are independent standard normal random variables, then the sum of their squares</p>
<p><span class="math display">\[
\sum_{i=1}^k Z_i^2
\]</span></p>
<p>follows a chi-squared distribution with <span class="math inline">\(k\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\sum_{i=1}^k Z_i^2 \sim \chi^2_k.
\]</span></p>
<p>The parameter <span class="math inline">\(k\)</span> is called the <strong>degrees of freedom</strong>. It determines the shape and spread of the distribution.</p>
<p>This construction is fundamental in statistics because many important quantities—such as sample variances and sums of squared residuals—can be expressed as sums of squared normal variables.</p>
<hr />
<div id="simulation-of-a-chi-squared-distribution" class="section level4 hasAnchor" number="5.10.3.1">
<h4><span class="header-section-number">5.10.3.1</span> Simulation of a Chi-Squared Distribution<a href="introduction-to-probability.html#simulation-of-a-chi-squared-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can build a chi-squared distribution directly from simulated normal variables.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="introduction-to-probability.html#cb280-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb280-2"><a href="introduction-to-probability.html#cb280-2" tabindex="-1"></a></span>
<span id="cb280-3"><a href="introduction-to-probability.html#cb280-3" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>)</span>
<span id="cb280-4"><a href="introduction-to-probability.html#cb280-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> z<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb280-5"><a href="introduction-to-probability.html#cb280-5" tabindex="-1"></a></span>
<span id="cb280-6"><a href="introduction-to-probability.html#cb280-6" tabindex="-1"></a><span class="fu">hist</span>(x, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">40</span>,</span>
<span id="cb280-7"><a href="introduction-to-probability.html#cb280-7" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Chi-Squared Distribution (df = 1)&quot;</span>,</span>
<span id="cb280-8"><a href="introduction-to-probability.html#cb280-8" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;lightgray&quot;</span>, <span class="at">border=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb280-9"><a href="introduction-to-probability.html#cb280-9" tabindex="-1"></a></span>
<span id="cb280-10"><a href="introduction-to-probability.html#cb280-10" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dchisq</span>(x, <span class="at">df=</span><span class="dv">1</span>),</span>
<span id="cb280-11"><a href="introduction-to-probability.html#cb280-11" tabindex="-1"></a>      <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-chi-squared-simulation-1.png" width="672" /></p>
<p>Each value in <code>x</code> is the square of a standard normal observation. The resulting histogram closely matches the theoretical chi-squared density.</p>
<p>This illustrates the transformation:</p>
<p><span class="math display">\[
Z \rightarrow Z^2.
\]</span></p>
<hr />
</div>
<div id="simulation-with-multiple-degrees-of-freedom" class="section level4 hasAnchor" number="5.10.3.2">
<h4><span class="header-section-number">5.10.3.2</span> Simulation with Multiple Degrees of Freedom<a href="introduction-to-probability.html#simulation-with-multiple-degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To generate a chi-squared distribution with <span class="math inline">\(k\)</span> degrees of freedom, we sum the squares of <span class="math inline">\(k\)</span> independent standard normal variables.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="introduction-to-probability.html#cb281-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb281-2"><a href="introduction-to-probability.html#cb281-2" tabindex="-1"></a></span>
<span id="cb281-3"><a href="introduction-to-probability.html#cb281-3" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb281-4"><a href="introduction-to-probability.html#cb281-4" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">10000</span> <span class="sc">*</span> k), <span class="at">ncol=</span>k)</span>
<span id="cb281-5"><a href="introduction-to-probability.html#cb281-5" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rowSums</span>(z<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb281-6"><a href="introduction-to-probability.html#cb281-6" tabindex="-1"></a></span>
<span id="cb281-7"><a href="introduction-to-probability.html#cb281-7" tabindex="-1"></a><span class="fu">hist</span>(x, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">40</span>,</span>
<span id="cb281-8"><a href="introduction-to-probability.html#cb281-8" tabindex="-1"></a>     <span class="at">main=</span><span class="fu">paste</span>(<span class="st">&quot;Chi-Squared Distribution (df =&quot;</span>, k, <span class="st">&quot;)&quot;</span>),</span>
<span id="cb281-9"><a href="introduction-to-probability.html#cb281-9" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;lightgray&quot;</span>, <span class="at">border=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb281-10"><a href="introduction-to-probability.html#cb281-10" tabindex="-1"></a></span>
<span id="cb281-11"><a href="introduction-to-probability.html#cb281-11" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dchisq</span>(x, <span class="at">df=</span>k),</span>
<span id="cb281-12"><a href="introduction-to-probability.html#cb281-12" tabindex="-1"></a>      <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-chi-squared-k-degrees-simulation-1.png" width="672" /></p>
<p>As the number of degrees of freedom increases, the distribution becomes less skewed and more symmetric.</p>
<hr />
</div>
<div id="shape-of-the-chi-squared-distribution" class="section level4 hasAnchor" number="5.10.3.3">
<h4><span class="header-section-number">5.10.3.3</span> Shape of the Chi-Squared Distribution<a href="introduction-to-probability.html#shape-of-the-chi-squared-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The shape of the chi-squared distribution depends strongly on the degrees of freedom.</p>
<p>Key properties:</p>
<ul>
<li>defined only for <span class="math inline">\(x \ge 0\)</span></li>
<li>right-skewed for small <span class="math inline">\(k\)</span></li>
<li>becomes more symmetric as <span class="math inline">\(k\)</span> increases</li>
<li>mean = <span class="math inline">\(k\)</span></li>
<li>variance = <span class="math inline">\(2k\)</span></li>
</ul>
<p>Visualization:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="introduction-to-probability.html#cb282-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dchisq</span>(x,<span class="dv">1</span>), <span class="dv">0</span>, <span class="dv">12</span>, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb282-2"><a href="introduction-to-probability.html#cb282-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;Effect of Degrees of Freedom&quot;</span>,</span>
<span id="cb282-3"><a href="introduction-to-probability.html#cb282-3" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Density&quot;</span>)</span>
<span id="cb282-4"><a href="introduction-to-probability.html#cb282-4" tabindex="-1"></a></span>
<span id="cb282-5"><a href="introduction-to-probability.html#cb282-5" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dchisq</span>(x,<span class="dv">5</span>), <span class="dv">0</span>, <span class="dv">12</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb282-6"><a href="introduction-to-probability.html#cb282-6" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dchisq</span>(x,<span class="dv">10</span>), <span class="dv">0</span>, <span class="dv">12</span>, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb282-7"><a href="introduction-to-probability.html#cb282-7" tabindex="-1"></a></span>
<span id="cb282-8"><a href="introduction-to-probability.html#cb282-8" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,</span>
<span id="cb282-9"><a href="introduction-to-probability.html#cb282-9" tabindex="-1"></a>       <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;df = 1&quot;</span>, <span class="st">&quot;df = 5&quot;</span>, <span class="st">&quot;df = 10&quot;</span>),</span>
<span id="cb282-10"><a href="introduction-to-probability.html#cb282-10" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>),</span>
<span id="cb282-11"><a href="introduction-to-probability.html#cb282-11" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-chi-squared-distribution-1.png" width="672" /></p>
<p>For small degrees of freedom:</p>
<ul>
<li>the distribution is highly skewed</li>
<li>most mass is near zero</li>
</ul>
<p>For larger degrees of freedom:</p>
<ul>
<li>the distribution becomes more bell-shaped</li>
<li>it begins to resemble a normal distribution</li>
</ul>
<p>In fact, for large <span class="math inline">\(k\)</span>,</p>
<p><span class="math display">\[
\chi^2_k \approx N(k, 2k).
\]</span></p>
<hr />
</div>
<div id="why-the-chi-squared-distribution-matters" class="section level4 hasAnchor" number="5.10.3.4">
<h4><span class="header-section-number">5.10.3.4</span> Why the Chi-Squared Distribution Matters<a href="introduction-to-probability.html#why-the-chi-squared-distribution-matters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The chi-squared distribution plays a central role in statistics because it appears naturally when we work with squared deviations.</p>
<p>Examples:</p>
<ul>
<li>sample variance of normal data</li>
<li>sums of squared residuals in regression</li>
<li>goodness-of-fit tests</li>
<li>contingency table analysis</li>
</ul>
<p>Many statistical procedures rely on the fact that certain sums of squared normal variables follow a chi-squared distribution.</p>
<hr />
</div>
<div id="generating-chi-squared-values-in-r" class="section level4 hasAnchor" number="5.10.3.5">
<h4><span class="header-section-number">5.10.3.5</span> Generating Chi-Squared Values in R<a href="introduction-to-probability.html#generating-chi-squared-values-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>R provides built-in functions for working with the chi-squared distribution:</p>
<ul>
<li><code>dchisq(x, df)</code> → density</li>
<li><code>pchisq(x, df)</code> → cumulative probability</li>
<li><code>qchisq(p, df)</code> → quantile</li>
<li><code>rchisq(n, df)</code> → simulation</li>
</ul>
<p>Examples:</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="introduction-to-probability.html#cb283-1" tabindex="-1"></a><span class="co"># Simulate chi-squared values</span></span>
<span id="cb283-2"><a href="introduction-to-probability.html#cb283-2" tabindex="-1"></a><span class="fu">rchisq</span>(<span class="dv">10</span>, <span class="at">df=</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>##  [1] 1.861377 6.259160 2.946891 1.461699 3.640620 3.655798 2.338470 1.527219 1.881949 7.641909</code></pre>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="introduction-to-probability.html#cb285-1" tabindex="-1"></a><span class="co"># Probability X ≤ 4</span></span>
<span id="cb285-2"><a href="introduction-to-probability.html#cb285-2" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="dv">4</span>, <span class="at">df=</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.7385359</code></pre>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="introduction-to-probability.html#cb287-1" tabindex="-1"></a><span class="co"># Upper tail probability</span></span>
<span id="cb287-2"><a href="introduction-to-probability.html#cb287-2" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(<span class="dv">6</span>, <span class="at">df=</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.1116102</code></pre>
<hr />
</div>
<div id="conceptual-summary" class="section level4 hasAnchor" number="5.10.3.6">
<h4><span class="header-section-number">5.10.3.6</span> Conceptual Summary<a href="introduction-to-probability.html#conceptual-summary" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The chi-squared distribution illustrates an important principle:</p>
<blockquote>
<p>Transformations of random variables produce new random variables with new distributions.</p>
</blockquote>
<p>Starting with standard normal variables and squaring them leads to the chi-squared distribution. Summing multiple squared normals introduces the degrees of freedom parameter.</p>
<p>This connection between normal variables and squared sums is foundational for later topics such as:</p>
<ul>
<li>sampling distributions</li>
<li>variance estimation</li>
<li>hypothesis testing</li>
<li>regression analysis</li>
</ul>
<p>Understanding the chi-squared distribution prepares us for many of the statistical tools developed later in the course.</p>
<hr />
</div>
</div>
<div id="t-distribution" class="section level3 hasAnchor" number="5.10.4">
<h3><span class="header-section-number">5.10.4</span> t Distribution<a href="introduction-to-probability.html#t-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <span class="math inline">\(t\)</span> distribution is one of the most important continuous distributions in statistics. It arises naturally when estimating means using small samples and unknown variance. Much of classical statistical inference—confidence intervals, hypothesis tests, regression coefficients—relies on this distribution.</p>
<hr />
<div id="history-3" class="section level4 hasAnchor" number="5.10.4.1">
<h4><span class="header-section-number">5.10.4.1</span> History<a href="introduction-to-probability.html#history-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(t\)</span> distribution was developed by William Gosset, a statistician working at the Guinness brewery in the early 1900s. Because company policy prohibited publishing under his real name, he wrote under the pseudonym <strong>“Student.”</strong> For this reason, the distribution is sometimes called the <em>Student’s <span class="math inline">\(t\)</span> distribution</em>.</p>
<p>Gosset needed methods for making reliable inferences from <strong>small samples</strong>, where the normal approximation alone was insufficient. His work showed that when variance must be estimated from the data, additional uncertainty must be incorporated into the distribution used for inference. The result was the <span class="math inline">\(t\)</span> distribution.</p>
<hr />
</div>
<div id="difference-from-the-normal-distribution" class="section level4 hasAnchor" number="5.10.4.2">
<h4><span class="header-section-number">5.10.4.2</span> Difference from the Normal Distribution<a href="introduction-to-probability.html#difference-from-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(t\)</span> distribution resembles the normal distribution but has important differences.</p>
<p>Key characteristics:</p>
<ul>
<li>symmetric and centered at 0</li>
<li>bell-shaped</li>
<li>heavier tails than the normal distribution</li>
<li>depends on degrees of freedom <span class="math inline">\(k\)</span></li>
</ul>
<p>“Heavier tails” means that extreme values are more likely than under a normal model. This reflects the additional uncertainty introduced when estimating variance from data.</p>
<p>As the degrees of freedom increase,</p>
<p><span class="math display">\[
t_k \longrightarrow N(0,1).
\]</span></p>
<p>Thus, for large samples, the <span class="math inline">\(t\)</span> distribution becomes nearly indistinguishable from the standard normal distribution.</p>
<hr />
</div>
<div id="visualization-effect-of-degrees-of-freedom" class="section level4 hasAnchor" number="5.10.4.3">
<h4><span class="header-section-number">5.10.4.3</span> Visualization: Effect of Degrees of Freedom<a href="introduction-to-probability.html#visualization-effect-of-degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="introduction-to-probability.html#cb289-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x), <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb289-2"><a href="introduction-to-probability.html#cb289-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;Normal vs t Distributions&quot;</span>,</span>
<span id="cb289-3"><a href="introduction-to-probability.html#cb289-3" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Density&quot;</span>)</span>
<span id="cb289-4"><a href="introduction-to-probability.html#cb289-4" tabindex="-1"></a></span>
<span id="cb289-5"><a href="introduction-to-probability.html#cb289-5" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(x,<span class="dv">1</span>),  <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>,   <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb289-6"><a href="introduction-to-probability.html#cb289-6" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(x,<span class="dv">5</span>),  <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>,  <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb289-7"><a href="introduction-to-probability.html#cb289-7" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(x,<span class="dv">20</span>), <span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="at">col=</span><span class="st">&quot;green&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb289-8"><a href="introduction-to-probability.html#cb289-8" tabindex="-1"></a></span>
<span id="cb289-9"><a href="introduction-to-probability.html#cb289-9" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,</span>
<span id="cb289-10"><a href="introduction-to-probability.html#cb289-10" tabindex="-1"></a>       <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;Normal&quot;</span>,<span class="st">&quot;t df=1&quot;</span>,<span class="st">&quot;t df=5&quot;</span>,<span class="st">&quot;t df=20&quot;</span>),</span>
<span id="cb289-11"><a href="introduction-to-probability.html#cb289-11" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>),</span>
<span id="cb289-12"><a href="introduction-to-probability.html#cb289-12" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-t-distribution-1.png" width="672" /></p>
<p>For small degrees of freedom:</p>
<ul>
<li>tails are very heavy</li>
<li>extreme values are more common</li>
</ul>
<p>For large degrees of freedom:</p>
<ul>
<li>the curve approaches the normal density</li>
</ul>
<hr />
</div>
<div id="derivation-from-normal-and-chi-squared-variables" class="section level4 hasAnchor" number="5.10.4.4">
<h4><span class="header-section-number">5.10.4.4</span> Derivation from Normal and Chi-Squared Variables<a href="introduction-to-probability.html#derivation-from-normal-and-chi-squared-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(t\)</span> distribution arises from combining a normal random variable and a chi-squared random variable.</p>
<p>If</p>
<p><span class="math display">\[
Z \sim N(0,1), \quad U \sim \chi^2_k,
\]</span></p>
<p>and <span class="math inline">\(Z\)</span> and <span class="math inline">\(U\)</span> are independent, then</p>
<p><span class="math display">\[
T = \frac{Z}{\sqrt{U/k}}
\]</span></p>
<p>follows a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(k\)</span> degrees of freedom. We write</p>
<p><span class="math display">\[
T \sim t_k.
\]</span></p>
<p>This construction shows that the <span class="math inline">\(t\)</span> distribution accounts for the variability introduced by estimating variance. The denominator <span class="math inline">\(\sqrt{U/k}\)</span> behaves like a random estimate of the standard deviation, which introduces extra spread compared with the normal distribution.</p>
<hr />
</div>
<div id="simulation-from-definition" class="section level4 hasAnchor" number="5.10.4.5">
<h4><span class="header-section-number">5.10.4.5</span> Simulation from Definition<a href="introduction-to-probability.html#simulation-from-definition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can simulate the <span class="math inline">\(t\)</span> distribution directly from its defining relationship.</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="introduction-to-probability.html#cb290-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb290-2"><a href="introduction-to-probability.html#cb290-2" tabindex="-1"></a></span>
<span id="cb290-3"><a href="introduction-to-probability.html#cb290-3" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>)</span>
<span id="cb290-4"><a href="introduction-to-probability.html#cb290-4" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(<span class="dv">10000</span>, <span class="at">df=</span><span class="dv">5</span>)</span>
<span id="cb290-5"><a href="introduction-to-probability.html#cb290-5" tabindex="-1"></a>t <span class="ot">&lt;-</span> z <span class="sc">/</span> <span class="fu">sqrt</span>(u<span class="sc">/</span><span class="dv">5</span>)</span>
<span id="cb290-6"><a href="introduction-to-probability.html#cb290-6" tabindex="-1"></a></span>
<span id="cb290-7"><a href="introduction-to-probability.html#cb290-7" tabindex="-1"></a><span class="fu">hist</span>(t, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">40</span>,</span>
<span id="cb290-8"><a href="introduction-to-probability.html#cb290-8" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;t Distribution (df = 5)&quot;</span>,</span>
<span id="cb290-9"><a href="introduction-to-probability.html#cb290-9" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;lightgray&quot;</span>, <span class="at">border=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb290-10"><a href="introduction-to-probability.html#cb290-10" tabindex="-1"></a></span>
<span id="cb290-11"><a href="introduction-to-probability.html#cb290-11" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(x, <span class="at">df=</span><span class="dv">5</span>),</span>
<span id="cb290-12"><a href="introduction-to-probability.html#cb290-12" tabindex="-1"></a>      <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/pro-t-distribution-generation-1.png" width="672" /></p>
<p>The simulated histogram aligns closely with the theoretical <span class="math inline">\(t\)</span> density.</p>
<hr />
</div>
<div id="generating-t-values-in-r" class="section level4 hasAnchor" number="5.10.4.6">
<h4><span class="header-section-number">5.10.4.6</span> Generating t Values in R<a href="introduction-to-probability.html#generating-t-values-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>R provides built-in functions for the <span class="math inline">\(t\)</span> distribution:</p>
<ul>
<li><code>dt(x, df)</code> → density</li>
<li><code>pt(x, df)</code> → cumulative probability</li>
<li><code>qt(p, df)</code> → quantile</li>
<li><code>rt(n, df)</code> → simulation</li>
</ul>
<p>Examples:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="introduction-to-probability.html#cb291-1" tabindex="-1"></a><span class="co"># simulate</span></span>
<span id="cb291-2"><a href="introduction-to-probability.html#cb291-2" tabindex="-1"></a><span class="fu">rt</span>(<span class="dv">10</span>, <span class="at">df=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>##  [1] -3.68826252 -1.07843091  2.39899684 -0.14320625  0.37991643 -0.04020562 -1.25015796  1.52204650 -0.45327619 -2.40707048</code></pre>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="introduction-to-probability.html#cb293-1" tabindex="-1"></a><span class="co"># probability T ≤ 1.5</span></span>
<span id="cb293-2"><a href="introduction-to-probability.html#cb293-2" tabindex="-1"></a><span class="fu">pt</span>(<span class="fl">1.5</span>, <span class="at">df=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.9030482</code></pre>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="introduction-to-probability.html#cb295-1" tabindex="-1"></a><span class="co"># upper tail probability</span></span>
<span id="cb295-2"><a href="introduction-to-probability.html#cb295-2" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pt</span>(<span class="dv">2</span>, <span class="at">df=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.05096974</code></pre>
<hr />
</div>
<div id="why-the-t-distribution-matters" class="section level4 hasAnchor" number="5.10.4.7">
<h4><span class="header-section-number">5.10.4.7</span> Why the t Distribution Matters<a href="introduction-to-probability.html#why-the-t-distribution-matters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(t\)</span> distribution is central to statistical inference because it appears whenever we standardize a sample mean using an estimated standard deviation.</p>
<p>If <span class="math inline">\(X_1,\dots,X_n\)</span> are independent normal observations with unknown variance, then</p>
<p><span class="math display">\[
\frac{\bar{X} - \mu}{S/\sqrt{n}}
\]</span></p>
<p>follows a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<p>This result underlies:</p>
<ul>
<li>confidence intervals for means</li>
<li>hypothesis tests for means</li>
<li>regression coefficient inference</li>
<li>many classical statistical procedures</li>
</ul>
<hr />
</div>
<div id="conceptual-summary-1" class="section level4 hasAnchor" number="5.10.4.8">
<h4><span class="header-section-number">5.10.4.8</span> Conceptual Summary<a href="introduction-to-probability.html#conceptual-summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(t\)</span> distribution arises from combining:</p>
<ul>
<li>a normal random variable (numerator)</li>
<li>a chi-squared random variable (denominator)</li>
</ul>
<p>It reflects the extra uncertainty introduced when variance must be estimated from data.</p>
<p>Compared with the normal distribution:</p>
<ul>
<li>same center</li>
<li>similar shape</li>
<li>heavier tails</li>
<li>depends on degrees of freedom</li>
</ul>
<p>As sample size increases, the <span class="math inline">\(t\)</span> distribution converges to the normal distribution, linking small-sample inference to large-sample approximations used throughout statistics.</p>
<hr />
</div>
</div>
<div id="f-distribution" class="section level3 hasAnchor" number="5.10.5">
<h3><span class="header-section-number">5.10.5</span> F Distribution<a href="introduction-to-probability.html#f-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <span class="math inline">\(F\)</span> distribution is another fundamental continuous distribution that arises from transformations of normal random variables. It plays a central role in comparing variances and in many procedures in regression and analysis of variance.</p>
<hr />
<div id="history-4" class="section level4 hasAnchor" number="5.10.5.1">
<h4><span class="header-section-number">5.10.5.1</span> History<a href="introduction-to-probability.html#history-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(F\)</span> distribution was introduced by <strong>Ronald Fisher</strong> in the development of analysis of variance (ANOVA). Fisher needed a distribution for comparing two independent estimates of variance. The ratio of these variance estimates led naturally to the <span class="math inline">\(F\)</span> distribution.</p>
<p>Today, the <span class="math inline">\(F\)</span> distribution is used in:</p>
<ul>
<li>comparing population variances</li>
<li>regression model comparisons</li>
<li>ANOVA</li>
<li>many hypothesis testing procedures</li>
</ul>
<hr />
</div>
<div id="difference-from-the-normal-distribution-1" class="section level4 hasAnchor" number="5.10.5.2">
<h4><span class="header-section-number">5.10.5.2</span> Difference from the Normal Distribution<a href="introduction-to-probability.html#difference-from-the-normal-distribution-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(F\)</span> distribution differs substantially from the normal distribution.</p>
<p>Key characteristics:</p>
<ul>
<li>takes only positive values</li>
<li>right-skewed</li>
<li>depends on two degrees of freedom parameters</li>
<li>becomes more symmetric as degrees of freedom increase</li>
</ul>
<p>If <span class="math inline">\(F \sim F_{d_1,d_2}\)</span>, then:</p>
<ul>
<li><span class="math inline">\(d_1\)</span> = numerator degrees of freedom</li>
<li><span class="math inline">\(d_2\)</span> = denominator degrees of freedom</li>
</ul>
<p>The shape depends on both parameters.</p>
<p>For small degrees of freedom:</p>
<ul>
<li>highly skewed</li>
<li>long right tail</li>
</ul>
<p>For large degrees of freedom:</p>
<ul>
<li>more concentrated</li>
<li>less skewed</li>
</ul>
<hr />
</div>
<div id="derivation-from-chi-squared-variables" class="section level4 hasAnchor" number="5.10.5.3">
<h4><span class="header-section-number">5.10.5.3</span> Derivation from Chi-Squared Variables<a href="introduction-to-probability.html#derivation-from-chi-squared-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(F\)</span> distribution arises from ratios of scaled chi-squared random variables.</p>
<p>If</p>
<p><span class="math display">\[
U_1 \sim \chi^2_{d_1}, \quad
U_2 \sim \chi^2_{d_2}
\]</span></p>
<p>are independent, then</p>
<p><span class="math display">\[
F = \frac{U_1/d_1}{U_2/d_2}
\]</span></p>
<p>follows an <span class="math inline">\(F\)</span> distribution with <span class="math inline">\((d_1,d_2)\)</span> degrees of freedom:</p>
<p><span class="math display">\[
F \sim F_{d_1,d_2}.
\]</span></p>
<p>This construction shows that the <span class="math inline">\(F\)</span> distribution measures the relative size of two variance-like quantities. Each chi-squared variable represents a sum of squared normal variables, so their ratio compares two independent estimates of variability.</p>
<hr />
</div>
<div id="simulation-from-definition-1" class="section level4 hasAnchor" number="5.10.5.4">
<h4><span class="header-section-number">5.10.5.4</span> Simulation from Definition<a href="introduction-to-probability.html#simulation-from-definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can simulate an <span class="math inline">\(F\)</span> distribution directly from its definition.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="introduction-to-probability.html#cb297-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2026</span>)</span>
<span id="cb297-2"><a href="introduction-to-probability.html#cb297-2" tabindex="-1"></a></span>
<span id="cb297-3"><a href="introduction-to-probability.html#cb297-3" tabindex="-1"></a>u1 <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(<span class="dv">10000</span>, <span class="at">df=</span><span class="dv">5</span>)</span>
<span id="cb297-4"><a href="introduction-to-probability.html#cb297-4" tabindex="-1"></a>u2 <span class="ot">&lt;-</span> <span class="fu">rchisq</span>(<span class="dv">10000</span>, <span class="at">df=</span><span class="dv">10</span>)</span>
<span id="cb297-5"><a href="introduction-to-probability.html#cb297-5" tabindex="-1"></a></span>
<span id="cb297-6"><a href="introduction-to-probability.html#cb297-6" tabindex="-1"></a>f  <span class="ot">&lt;-</span> (u1<span class="sc">/</span><span class="dv">5</span>)<span class="sc">/</span>(u2<span class="sc">/</span><span class="dv">10</span>)</span>
<span id="cb297-7"><a href="introduction-to-probability.html#cb297-7" tabindex="-1"></a></span>
<span id="cb297-8"><a href="introduction-to-probability.html#cb297-8" tabindex="-1"></a><span class="fu">hist</span>(f, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">breaks=</span><span class="dv">40</span>,</span>
<span id="cb297-9"><a href="introduction-to-probability.html#cb297-9" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;F Distribution (df1 = 5, df2 = 10)&quot;</span>,</span>
<span id="cb297-10"><a href="introduction-to-probability.html#cb297-10" tabindex="-1"></a>     <span class="at">col=</span><span class="st">&quot;lightgray&quot;</span>, <span class="at">border=</span><span class="st">&quot;white&quot;</span>)</span>
<span id="cb297-11"><a href="introduction-to-probability.html#cb297-11" tabindex="-1"></a></span>
<span id="cb297-12"><a href="introduction-to-probability.html#cb297-12" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">df</span>(x,<span class="dv">5</span>,<span class="dv">10</span>),</span>
<span id="cb297-13"><a href="introduction-to-probability.html#cb297-13" tabindex="-1"></a>      <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/f-distribution-generation-1.png" width="672" /></p>
<p>The histogram of simulated values closely matches the theoretical <span class="math inline">\(F\)</span> density.</p>
<hr />
</div>
<div id="shape-and-behavior" class="section level4 hasAnchor" number="5.10.5.5">
<h4><span class="header-section-number">5.10.5.5</span> Shape and Behavior<a href="introduction-to-probability.html#shape-and-behavior" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <span class="math inline">\(F\)</span> distribution has several notable properties:</p>
<ul>
<li>defined only for <span class="math inline">\(x&gt;0\)</span></li>
<li>right-skewed</li>
<li>mean exists when <span class="math inline">\(d_2&gt;2\)</span></li>
<li>variance exists when <span class="math inline">\(d_2&gt;4\)</span></li>
</ul>
<p>As the denominator degrees of freedom <span class="math inline">\(d_2\)</span> increase, the distribution becomes less variable. As both degrees of freedom grow large, the distribution becomes more concentrated around 1.</p>
<p>Visualization for different parameters:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="introduction-to-probability.html#cb298-1" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">df</span>(x,<span class="dv">1</span>,<span class="dv">5</span>), <span class="dv">0</span>,<span class="dv">5</span>, <span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb298-2"><a href="introduction-to-probability.html#cb298-2" tabindex="-1"></a>      <span class="at">main=</span><span class="st">&quot;Effect of Degrees of Freedom&quot;</span>,</span>
<span id="cb298-3"><a href="introduction-to-probability.html#cb298-3" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Density&quot;</span>)</span>
<span id="cb298-4"><a href="introduction-to-probability.html#cb298-4" tabindex="-1"></a></span>
<span id="cb298-5"><a href="introduction-to-probability.html#cb298-5" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">df</span>(x,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">0</span>,<span class="dv">5</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb298-6"><a href="introduction-to-probability.html#cb298-6" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">df</span>(x,<span class="dv">10</span>,<span class="dv">10</span>), <span class="dv">0</span>,<span class="dv">5</span>, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb298-7"><a href="introduction-to-probability.html#cb298-7" tabindex="-1"></a></span>
<span id="cb298-8"><a href="introduction-to-probability.html#cb298-8" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>,</span>
<span id="cb298-9"><a href="introduction-to-probability.html#cb298-9" tabindex="-1"></a>       <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;F(1,5)&quot;</span>,<span class="st">&quot;F(5,5)&quot;</span>,<span class="st">&quot;F(10,10)&quot;</span>),</span>
<span id="cb298-10"><a href="introduction-to-probability.html#cb298-10" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>),</span>
<span id="cb298-11"><a href="introduction-to-probability.html#cb298-11" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/f-distribution-1.png" width="672" /></p>
<hr />
</div>
<div id="generating-f-values-in-r" class="section level4 hasAnchor" number="5.10.5.6">
<h4><span class="header-section-number">5.10.5.6</span> Generating F Values in R<a href="introduction-to-probability.html#generating-f-values-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>R includes built-in functions for the <span class="math inline">\(F\)</span> distribution:</p>
<ul>
<li><code>df(x, d1, d2)</code> → density</li>
<li><code>pf(x, d1, d2)</code> → cumulative probability</li>
<li><code>qf(p, d1, d2)</code> → quantile</li>
<li><code>rf(n, d1, d2)</code> → simulation</li>
</ul>
<p>Examples:</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="introduction-to-probability.html#cb299-1" tabindex="-1"></a><span class="co"># simulate values</span></span>
<span id="cb299-2"><a href="introduction-to-probability.html#cb299-2" tabindex="-1"></a><span class="fu">rf</span>(<span class="dv">10</span>, <span class="dv">5</span>, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  [1] 1.2125722 0.5195305 0.3304498 2.0613735 0.9729444 0.1864224 0.4309595 0.6210289 1.9900651 0.1281117</code></pre>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="introduction-to-probability.html#cb301-1" tabindex="-1"></a><span class="co"># probability F ≤ 2</span></span>
<span id="cb301-2"><a href="introduction-to-probability.html#cb301-2" tabindex="-1"></a><span class="fu">pf</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [1] 0.835805</code></pre>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="introduction-to-probability.html#cb303-1" tabindex="-1"></a><span class="co"># upper tail probability</span></span>
<span id="cb303-2"><a href="introduction-to-probability.html#cb303-2" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pf</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [1] 0.06555756</code></pre>
<hr />
</div>
</div>
<div id="summary-1" class="section level3 hasAnchor" number="5.10.6">
<h3><span class="header-section-number">5.10.6</span> Summary<a href="introduction-to-probability.html#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Continuous random variables differ from discrete random variables in a fundamental way: they assign probability to <strong>intervals</strong> rather than individual points.</p>
<p>They are described using two functions:</p>
<ul>
<li>the probability density function (PDF)</li>
<li>the cumulative distribution function (CDF)</li>
</ul>
<p>Several key continuous distributions appear repeatedly in statistics:</p>
<ul>
<li>Uniform</li>
<li>Normal</li>
<li>Chi-squared</li>
<li><span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(F\)</span></li>
</ul>
<p>Each arises from transformations of simpler random variables, particularly the normal distribution. These connections are crucial because they allow complex statistical quantities to be studied using known probability models.</p>
<p>Simulation provides a powerful computational approach for understanding continuous distributions:</p>
<ul>
<li>visualizing densities</li>
<li>approximating probabilities</li>
<li>verifying theoretical results</li>
<li>building intuition</li>
</ul>
<p>These distributions form the mathematical foundation for statistical inference developed in later chapters, including confidence intervals, hypothesis testing, regression, and analysis of variance.</p>
<hr />
</div>
</div>
<div id="the-mean-of-a-random-variable" class="section level2 hasAnchor" number="5.11">
<h2><span class="header-section-number">5.11</span> The Mean of a Random Variable<a href="introduction-to-probability.html#the-mean-of-a-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>mean</strong> (or expected value) of a random variable describes its long-run average value.
It is not the average of a particular dataset, but rather a property of the <strong>probability model</strong> itself.</p>
<p>If we imagine repeating the same random experiment over and over under identical conditions, the sample average of the outcomes will tend to stabilize near the mean. For this reason, the mean is often interpreted as the <strong>center of the distribution</strong> or the value around which observations fluctuate.</p>
<p>From a modeling perspective, the mean is determined entirely by the probability distribution of the random variable.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span> with probabilities <span class="math inline">\(P(X=x)\)</span>,</p>
<p><span class="math display">\[
E[X] = \sum_x x P(X=x).
\]</span></p>
<p>For a continuous random variable with density <span class="math inline">\(f(x)\)</span>,</p>
<p><span class="math display">\[
E[X] = \int_{-\infty}^{\infty} x f(x)dx.
\]</span></p>
<p>Both formulas express the same idea: the mean is a <strong>probability-weighted average</strong> of all possible values the random variable can take.</p>
<hr />
<div id="interpreting-the-mean" class="section level3 hasAnchor" number="5.11.1">
<h3><span class="header-section-number">5.11.1</span> Interpreting the Mean<a href="introduction-to-probability.html#interpreting-the-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several useful interpretations of the mean:</p>
<ul>
<li><strong>Long-run average:</strong> If we simulate the experiment many times, the sample average approaches the mean.</li>
<li><strong>Balance point:</strong> In physics, the mean is the balance point of a distribution.</li>
<li><strong>Center of mass:</strong> Values farther from the mean contribute more strongly if they occur with high probability.</li>
<li><strong>Prediction under squared loss:</strong> The mean is the best constant predictor when using squared error.</li>
</ul>
<p>These interpretations make the mean one of the most important summaries of a random variable.</p>
<hr />
</div>
<div id="mean-as-a-distribution-parameter-vs-not" class="section level3 hasAnchor" number="5.11.2">
<h3><span class="header-section-number">5.11.2</span> Mean as a Distribution Parameter vs Not<a href="introduction-to-probability.html#mean-as-a-distribution-parameter-vs-not" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In many probability models, the mean is one of the defining parameters of the distribution.
In others, it is a derived quantity computed from the parameters.</p>
<p>Examples where the mean <strong>is</strong> a parameter:</p>
<ul>
<li>Normal distribution: mean <span class="math inline">\(=\mu\)</span></li>
<li>Poisson distribution: mean <span class="math inline">\(=\lambda\)</span></li>
<li>Exponential distribution: mean <span class="math inline">\(=1/\lambda\)</span></li>
</ul>
<p>Examples where the mean is <strong>not directly</strong> a parameter:</p>
<ul>
<li>Uniform distribution on <span class="math inline">\([a,b]\)</span>: mean is <span class="math inline">\((a+b)/2\)</span></li>
<li>Derived variables such as <span class="math inline">\(X^2\)</span> when <span class="math inline">\(X\sim N(0,1)\)</span></li>
<li>The binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, whose mean is <span class="math inline">\(np\)</span></li>
</ul>
<p>Even when the mean is not explicitly listed as a parameter, it is still determined by the distribution.</p>
<hr />
</div>
<div id="simulation-example-mean-of-a-binomial-distribution" class="section level3 hasAnchor" number="5.11.3">
<h3><span class="header-section-number">5.11.3</span> Simulation Example: Mean of a Binomial Distribution<a href="introduction-to-probability.html#simulation-example-mean-of-a-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a binomial random variable <span class="math inline">\(X\sim \text{Binomial}(n,p)\)</span>.
The theoretical mean is</p>
<p><span class="math display">\[
E[X]=np.
\]</span></p>
<p>We can verify this using simulation and direct calculation.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="introduction-to-probability.html#cb305-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb305-2"><a href="introduction-to-probability.html#cb305-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb305-3"><a href="introduction-to-probability.html#cb305-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb305-4"><a href="introduction-to-probability.html#cb305-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="at">n =</span> <span class="dv">10000</span>, <span class="at">size =</span> n, <span class="at">prob =</span> p)</span>
<span id="cb305-5"><a href="introduction-to-probability.html#cb305-5" tabindex="-1"></a></span>
<span id="cb305-6"><a href="introduction-to-probability.html#cb305-6" tabindex="-1"></a><span class="co"># Mean Computed Directly</span></span>
<span id="cb305-7"><a href="introduction-to-probability.html#cb305-7" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;The exact mean is: &quot;</span>, n <span class="sc">*</span> p))</span></code></pre></div>
<pre><code>## [1] &quot;The exact mean is: 1.5&quot;</code></pre>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="introduction-to-probability.html#cb307-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;The approximate mean is: &quot;</span>, <span class="fu">mean</span>(x)))</span></code></pre></div>
<pre><code>## [1] &quot;The approximate mean is: 1.5019&quot;</code></pre>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="introduction-to-probability.html#cb309-1" tabindex="-1"></a><span class="co"># Computing the Mean using the formula</span></span>
<span id="cb309-2"><a href="introduction-to-probability.html#cb309-2" tabindex="-1"></a>pmf <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(<span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span>n, <span class="at">size =</span> n, <span class="at">prob =</span> p)</span>
<span id="cb309-3"><a href="introduction-to-probability.html#cb309-3" tabindex="-1"></a>exaMeaBin <span class="ot">&lt;-</span> <span class="fu">sum</span>(pmf <span class="sc">*</span> (<span class="dv">0</span><span class="sc">:</span>n))</span>
<span id="cb309-4"><a href="introduction-to-probability.html#cb309-4" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;The exact mean is: &quot;</span>, exaMeaBin))</span></code></pre></div>
<pre><code>## [1] &quot;The exact mean is: 1.5&quot;</code></pre>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="introduction-to-probability.html#cb311-1" tabindex="-1"></a><span class="co"># Approximate Mean using empirical probabilities</span></span>
<span id="cb311-2"><a href="introduction-to-probability.html#cb311-2" tabindex="-1"></a>proTab <span class="ot">&lt;-</span> <span class="fu">prop.table</span>(<span class="fu">table</span>(x))</span>
<span id="cb311-3"><a href="introduction-to-probability.html#cb311-3" tabindex="-1"></a>appMea <span class="ot">&lt;-</span> <span class="fu">sum</span>(proTab <span class="sc">*</span> (<span class="dv">0</span><span class="sc">:</span>n))</span>
<span id="cb311-4"><a href="introduction-to-probability.html#cb311-4" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;The approximate mean is: &quot;</span>, appMea))</span></code></pre></div>
<pre><code>## [1] &quot;The approximate mean is: 1.5019&quot;</code></pre>
<p>This example illustrates several key points:</p>
<ol style="list-style-type: decimal">
<li>The mean can be computed <strong>analytically</strong> from the formula.</li>
<li>It can be approximated using <strong>simulation</strong>.</li>
<li>It can be approximated using <strong>empirical frequencies</strong>.</li>
<li>All approaches converge as the number of simulations increases.</li>
</ol>
<p>Simulation provides an intuitive interpretation: the mean is the value the random variable averages to over many repetitions.</p>
<hr />
</div>
<div id="the-mean-as-a-weighted-average" class="section level3 hasAnchor" number="5.11.4">
<h3><span class="header-section-number">5.11.4</span> The Mean as a Weighted Average<a href="introduction-to-probability.html#the-mean-as-a-weighted-average" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The expected value is a weighted average in which each possible outcome is weighted by its probability. Values that occur with high probability contribute more strongly to the mean.</p>
<p>For example, if a random variable takes values <span class="math inline">\(0,1,2,3\)</span> with different probabilities, the mean reflects both the magnitude of each value and how often it occurs.</p>
<p>This explains why the mean does not always coincide with the most likely value or the median. In skewed distributions, the mean can be pulled toward extreme values.</p>
<hr />
</div>
<div id="properties-of-the-mean-operator" class="section level3 hasAnchor" number="5.11.5">
<h3><span class="header-section-number">5.11.5</span> Properties of the Mean Operator<a href="introduction-to-probability.html#properties-of-the-mean-operator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The mean operator has several important mathematical properties.
The most important is <strong>linearity</strong>.</p>
<div id="linearity" class="section level4 hasAnchor" number="5.11.5.1">
<h4><span class="header-section-number">5.11.5.1</span> Linearity<a href="introduction-to-probability.html#linearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>,</p>
<p><span class="math display">\[
E[aX + bY] = aE[X] + bE[Y].
\]</span></p>
<p>This property holds regardless of whether <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
<p>Linearity allows us to compute means of complicated expressions by breaking them into simpler parts. It is one of the most frequently used tools in probability and statistics.</p>
<hr />
</div>
<div id="consequences-of-linearity" class="section level4 hasAnchor" number="5.11.5.2">
<h4><span class="header-section-number">5.11.5.2</span> Consequences of Linearity<a href="introduction-to-probability.html#consequences-of-linearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Some useful special cases:</p>
<p><span class="math display">\[
E[X + Y] = E[X] + E[Y]
\]</span></p>
<p><span class="math display">\[
E[cX] = cE[X]
\]</span></p>
<p><span class="math display">\[
E[X + c] = E[X] + c
\]</span></p>
<p>These rules allow us to compute expected values without needing the full distribution of the transformed variable.</p>
<hr />
</div>
<div id="example-of-linearity-via-simulation" class="section level4 hasAnchor" number="5.11.5.3">
<h4><span class="header-section-number">5.11.5.3</span> Example of Linearity via Simulation<a href="introduction-to-probability.html#example-of-linearity-via-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="introduction-to-probability.html#cb313-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb313-2"><a href="introduction-to-probability.html#cb313-2" tabindex="-1"></a>mea1 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb313-3"><a href="introduction-to-probability.html#cb313-3" tabindex="-1"></a>mea2 <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb313-4"><a href="introduction-to-probability.html#cb313-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> mea1, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb313-5"><a href="introduction-to-probability.html#cb313-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> mea2, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb313-6"><a href="introduction-to-probability.html#cb313-6" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> y</span>
<span id="cb313-7"><a href="introduction-to-probability.html#cb313-7" tabindex="-1"></a></span>
<span id="cb313-8"><a href="introduction-to-probability.html#cb313-8" tabindex="-1"></a><span class="co"># Using the simulation</span></span>
<span id="cb313-9"><a href="introduction-to-probability.html#cb313-9" tabindex="-1"></a>appMea    <span class="ot">&lt;-</span> <span class="fu">mean</span>(z)</span>
<span id="cb313-10"><a href="introduction-to-probability.html#cb313-10" tabindex="-1"></a>appMeaLin <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">*</span><span class="fu">mean</span>(x) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">mean</span>(y)</span>
<span id="cb313-11"><a href="introduction-to-probability.html#cb313-11" tabindex="-1"></a>exaMeaLin <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="sc">*</span> mea1 <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> mea2</span>
<span id="cb313-12"><a href="introduction-to-probability.html#cb313-12" tabindex="-1"></a></span>
<span id="cb313-13"><a href="introduction-to-probability.html#cb313-13" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;The approximate mean is: &quot;</span>, appMea))</span></code></pre></div>
<pre><code>## [1] &quot;The approximate mean is: 15.963628618867&quot;</code></pre>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="introduction-to-probability.html#cb315-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;The approximate mean using linearity is: &quot;</span>, appMeaLin))</span></code></pre></div>
<pre><code>## [1] &quot;The approximate mean using linearity is: 15.963628618867&quot;</code></pre>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="introduction-to-probability.html#cb317-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;The exact mean using linearity is: &quot;</span>, exaMeaLin))</span></code></pre></div>
<pre><code>## [1] &quot;The exact mean using linearity is: 16&quot;</code></pre>
<p>The three quantities are approximately equal:</p>
<ul>
<li>the simulated mean of <span class="math inline">\(z\)</span></li>
<li>the simulated linear combination of means</li>
<li>the exact theoretical value</li>
</ul>
<p>This demonstrates that linearity holds both theoretically and empirically.</p>
<hr />
</div>
</div>
<div id="why-the-mean-matters" class="section level3 hasAnchor" number="5.11.6">
<h3><span class="header-section-number">5.11.6</span> Why the Mean Matters<a href="introduction-to-probability.html#why-the-mean-matters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The mean plays a central role throughout statistics:</p>
<ul>
<li>It is the foundation of estimation and inference.</li>
<li>Many statistical procedures are built around averages.</li>
<li>Sampling distributions of means are central to statistical theory.</li>
<li>The Central Limit Theorem explains why averages tend to be normally distributed.</li>
</ul>
<p>Because of these properties, understanding the mean of a random variable is essential for understanding statistical modeling and inference.</p>
<p>In later sections, we will study how means behave when computed from samples, leading to the concept of <strong>sampling distributions</strong> and the foundations of statistical inference.</p>
<hr />
</div>
</div>
<div id="the-variance-of-a-random-variable" class="section level2 hasAnchor" number="5.12">
<h2><span class="header-section-number">5.12</span> The Variance of a Random Variable<a href="introduction-to-probability.html#the-variance-of-a-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The variance of a random variable quantifies how much its values fluctuate around its mean. While the mean captures the center of a distribution, the variance measures its dispersion. Together, these two quantities provide a fundamental summary of a distribution’s behavior.</p>
<p>For a random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(E[X]\)</span>, the variance is defined as</p>
<p><span class="math display">\[
\mathbb{V}(X) = E\big[(X - E[X])^2\big].
\]</span></p>
<p>This definition expresses variance as the expected squared deviation from the mean. Squaring ensures that deviations above and below the mean do not cancel each other out and that larger deviations contribute more heavily to the measure of spread.</p>
<p>Using algebra and the linearity of the mean operator, we can derive an equivalent and often more convenient expression:</p>
<p><span class="math display">\[
\mathbb{V}(X) = E[X^2] - (E[X])^2.
\]</span></p>
<p>This alternative representation is widely used in both theoretical work and computation because it separates the variance into two simpler expectations: the mean of the square and the square of the mean.</p>
<hr />
<div id="interpreting-variance" class="section level3 hasAnchor" number="5.12.1">
<h3><span class="header-section-number">5.12.1</span> Interpreting Variance<a href="introduction-to-probability.html#interpreting-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Variance is always nonnegative. A variance of zero implies that the random variable takes a constant value with probability one. Larger variances indicate greater dispersion around the mean.</p>
<p>In practical terms, if we repeatedly observe outcomes from the distribution of <span class="math inline">\(X\)</span>, the variance describes how widely those outcomes tend to vary. For instance:</p>
<ul>
<li>A distribution tightly clustered around its mean has small variance.</li>
<li>A distribution with frequent extreme values has large variance.</li>
</ul>
<p>Because variance is expressed in squared units, it is often useful to also consider the standard deviation, defined as <span class="math inline">\(\sqrt{\mathbb{V}(X)}\)</span>, which returns the measure of spread to the original units of the variable.</p>
<hr />
</div>
<div id="variance-as-a-distribution-parameter-vs-derived-quantity" class="section level3 hasAnchor" number="5.12.2">
<h3><span class="header-section-number">5.12.2</span> Variance as a Distribution Parameter vs Derived Quantity<a href="introduction-to-probability.html#variance-as-a-distribution-parameter-vs-derived-quantity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some probability models, the variance appears explicitly as a parameter of the distribution. In others, it is determined indirectly from the model’s parameters.</p>
<p>Examples where the variance is a parameter:</p>
<ul>
<li>Normal distribution: variance = <span class="math inline">\(\sigma^2\)</span></li>
<li>Poisson distribution: variance = <span class="math inline">\(\lambda\)</span></li>
</ul>
<p>Examples where the variance is derived from parameters:</p>
<ul>
<li>Uniform distribution on <span class="math inline">\([a,b]\)</span>: variance is <span class="math inline">\((b-a)^2/12\)</span></li>
<li>Binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>: variance is <span class="math inline">\(np(1-p)\)</span></li>
<li>Transformed variables such as <span class="math inline">\(Y = X^2\)</span> where <span class="math inline">\(X\sim N(0,1)\)</span></li>
</ul>
<p>This distinction is important conceptually. Sometimes the variance is directly specified in the model; other times it must be computed from the structure of the random variable.</p>
<hr />
</div>
<div id="simulation-based-understanding-of-variance" class="section level3 hasAnchor" number="5.12.3">
<h3><span class="header-section-number">5.12.3</span> Simulation-Based Understanding of Variance<a href="introduction-to-probability.html#simulation-based-understanding-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Simulation allows us to approximate variance numerically and verify theoretical identities. The following code generates data from a normal distribution and compares different ways of computing variance.</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="introduction-to-probability.html#cb319-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb319-2"><a href="introduction-to-probability.html#cb319-2" tabindex="-1"></a>var <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb319-3"><a href="introduction-to-probability.html#cb319-3" tabindex="-1"></a>x   <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(var))</span>
<span id="cb319-4"><a href="introduction-to-probability.html#cb319-4" tabindex="-1"></a><span class="co"># Compute the Mean</span></span>
<span id="cb319-5"><a href="introduction-to-probability.html#cb319-5" tabindex="-1"></a>meaX <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb319-6"><a href="introduction-to-probability.html#cb319-6" tabindex="-1"></a><span class="co"># Compute the Mean of the Square</span></span>
<span id="cb319-7"><a href="introduction-to-probability.html#cb319-7" tabindex="-1"></a>meaX2 <span class="ot">&lt;-</span> <span class="fu">mean</span>(x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb319-8"><a href="introduction-to-probability.html#cb319-8" tabindex="-1"></a><span class="co"># Variance directly</span></span>
<span id="cb319-9"><a href="introduction-to-probability.html#cb319-9" tabindex="-1"></a>dirVar <span class="ot">&lt;-</span> <span class="fu">var</span>(x)</span>
<span id="cb319-10"><a href="introduction-to-probability.html#cb319-10" tabindex="-1"></a><span class="co"># Variance alternative representation</span></span>
<span id="cb319-11"><a href="introduction-to-probability.html#cb319-11" tabindex="-1"></a>altVar <span class="ot">&lt;-</span> meaX2 <span class="sc">-</span> meaX<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb319-12"><a href="introduction-to-probability.html#cb319-12" tabindex="-1"></a><span class="co"># Prints results</span></span>
<span id="cb319-13"><a href="introduction-to-probability.html#cb319-13" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Direct approximate variance: &quot;</span>, dirVar))</span></code></pre></div>
<pre><code>## [1] &quot;Direct approximate variance: 4.09946234933146&quot;</code></pre>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="introduction-to-probability.html#cb321-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Alternative approximate variance: &quot;</span>, altVar))</span></code></pre></div>
<pre><code>## [1] &quot;Alternative approximate variance: 4.09905240309653&quot;</code></pre>
<p>The direct computation using <code>var(x)</code> should closely match the alternative computation <span class="math inline">\(E[X^2] - (E[X])^2\)</span>. Small discrepancies arise only from simulation noise.</p>
<p>This illustrates that variance is fundamentally an expectation-based quantity that can be approximated through repeated sampling.</p>
<hr />
</div>
<div id="properties-of-variance" class="section level3 hasAnchor" number="5.12.4">
<h3><span class="header-section-number">5.12.4</span> Properties of Variance<a href="introduction-to-probability.html#properties-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Variance has several algebraic properties that make it useful for analyzing transformed variables and combinations of random variables.</p>
<hr />
<div id="adding-a-constant" class="section level4 hasAnchor" number="5.12.4.1">
<h4><span class="header-section-number">5.12.4.1</span> Adding a Constant<a href="introduction-to-probability.html#adding-a-constant" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If we define a new random variable <span class="math inline">\(Y = X + c\)</span>, where <span class="math inline">\(c\)</span> is a constant, then</p>
<p><span class="math display">\[
\mathbb{V}(Y) = \mathbb{V}(X).
\]</span></p>
<p>Adding a constant shifts the distribution without changing its spread. The distance between observations remains the same, so the variance is unchanged.</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="introduction-to-probability.html#cb323-1" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb323-2"><a href="introduction-to-probability.html#cb323-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>)</span>
<span id="cb323-3"><a href="introduction-to-probability.html#cb323-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> c</span>
<span id="cb323-4"><a href="introduction-to-probability.html#cb323-4" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approximate varaince X: &quot;</span>, <span class="fu">var</span>(x)))</span></code></pre></div>
<pre><code>## [1] &quot;Approximate varaince X: 0.98164040967736&quot;</code></pre>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="introduction-to-probability.html#cb325-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approximate varaince Y: &quot;</span>, <span class="fu">var</span>(y)))</span></code></pre></div>
<pre><code>## [1] &quot;Approximate varaince Y: 0.98164040967736&quot;</code></pre>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="introduction-to-probability.html#cb327-1" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb327-2"><a href="introduction-to-probability.html#cb327-2" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb327-3"><a href="introduction-to-probability.html#cb327-3" tabindex="-1"></a>      <span class="at">from =</span> <span class="sc">-</span><span class="dv">5</span>,</span>
<span id="cb327-4"><a href="introduction-to-probability.html#cb327-4" tabindex="-1"></a>      <span class="at">to   =</span> <span class="dv">5</span> <span class="sc">+</span> c,</span>
<span id="cb327-5"><a href="introduction-to-probability.html#cb327-5" tabindex="-1"></a>      <span class="at">lwd  =</span> <span class="dv">2</span>,</span>
<span id="cb327-6"><a href="introduction-to-probability.html#cb327-6" tabindex="-1"></a>      <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),</span>
<span id="cb327-7"><a href="introduction-to-probability.html#cb327-7" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;pdf&quot;</span>)</span>
<span id="cb327-8"><a href="introduction-to-probability.html#cb327-8" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new=</span><span class="cn">TRUE</span>)</span>
<span id="cb327-9"><a href="introduction-to-probability.html#cb327-9" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span> <span class="sc">+</span> c, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb327-10"><a href="introduction-to-probability.html#cb327-10" tabindex="-1"></a>      <span class="at">from =</span> <span class="sc">-</span><span class="dv">5</span>,</span>
<span id="cb327-11"><a href="introduction-to-probability.html#cb327-11" tabindex="-1"></a>      <span class="at">to   =</span> <span class="dv">5</span> <span class="sc">+</span> c,</span>
<span id="cb327-12"><a href="introduction-to-probability.html#cb327-12" tabindex="-1"></a>      <span class="at">lwd  =</span> <span class="dv">2</span>,</span>
<span id="cb327-13"><a href="introduction-to-probability.html#cb327-13" tabindex="-1"></a>      <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb327-14"><a href="introduction-to-probability.html#cb327-14" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/varaince-adding-a-constant-1.png" width="672" /></p>
<p>The two curves have identical shapes but different centers, illustrating that variance measures spread rather than location.</p>
<hr />
</div>
<div id="multiplying-by-a-scalar" class="section level4 hasAnchor" number="5.12.4.2">
<h4><span class="header-section-number">5.12.4.2</span> Multiplying by a Scalar<a href="introduction-to-probability.html#multiplying-by-a-scalar" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If <span class="math inline">\(Y = aX\)</span> for some constant <span class="math inline">\(a\)</span>, then</p>
<p><span class="math display">\[
\mathbb{V}(Y) = a^2 \mathbb{V}(X).
\]</span></p>
<p>Multiplying by a constant rescales the spread of the distribution. If <span class="math inline">\(|a|&gt;1\)</span>, the distribution becomes more spread out; if <span class="math inline">\(|a|&lt;1\)</span>, it becomes more concentrated.</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="introduction-to-probability.html#cb328-1" tabindex="-1"></a>a    <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb328-2"><a href="introduction-to-probability.html#cb328-2" tabindex="-1"></a>varX <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb328-3"><a href="introduction-to-probability.html#cb328-3" tabindex="-1"></a>x    <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(varX))</span>
<span id="cb328-4"><a href="introduction-to-probability.html#cb328-4" tabindex="-1"></a>y    <span class="ot">&lt;-</span> a <span class="sc">*</span> x</span>
<span id="cb328-5"><a href="introduction-to-probability.html#cb328-5" tabindex="-1"></a>varY <span class="ot">&lt;-</span> a<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> varX</span>
<span id="cb328-6"><a href="introduction-to-probability.html#cb328-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Exact varaince X: &quot;</span>, varX))</span></code></pre></div>
<pre><code>## [1] &quot;Exact varaince X: 1&quot;</code></pre>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="introduction-to-probability.html#cb330-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approximate varaince X: &quot;</span>, <span class="fu">var</span>(x)))</span></code></pre></div>
<pre><code>## [1] &quot;Approximate varaince X: 1.01546350237623&quot;</code></pre>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="introduction-to-probability.html#cb332-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Exact varaince Y: &quot;</span>, varY))</span></code></pre></div>
<pre><code>## [1] &quot;Exact varaince Y: 4&quot;</code></pre>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="introduction-to-probability.html#cb334-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approximate varaince Y: &quot;</span>, <span class="fu">var</span>(y)))</span></code></pre></div>
<pre><code>## [1] &quot;Approximate varaince Y: 4.06185400950491&quot;</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="introduction-to-probability.html#cb336-1" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb336-2"><a href="introduction-to-probability.html#cb336-2" tabindex="-1"></a>xmin <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">*</span> <span class="fu">max</span>(a, <span class="dv">1</span>) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb336-3"><a href="introduction-to-probability.html#cb336-3" tabindex="-1"></a>xmax <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="sc">*</span> <span class="fu">max</span>(a, <span class="dv">1</span>) <span class="sc">^</span> <span class="dv">2</span> </span>
<span id="cb336-4"><a href="introduction-to-probability.html#cb336-4" tabindex="-1"></a>yval <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(<span class="at">x =</span> <span class="fu">seq</span>(xmin, xmax, <span class="at">length =</span> <span class="dv">101</span>), <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(varX))</span>
<span id="cb336-5"><a href="introduction-to-probability.html#cb336-5" tabindex="-1"></a>ymin <span class="ot">&lt;-</span> <span class="fu">min</span>(yval)</span>
<span id="cb336-6"><a href="introduction-to-probability.html#cb336-6" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(yval)</span>
<span id="cb336-7"><a href="introduction-to-probability.html#cb336-7" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(varX)),</span>
<span id="cb336-8"><a href="introduction-to-probability.html#cb336-8" tabindex="-1"></a>      <span class="at">from =</span> xmin,</span>
<span id="cb336-9"><a href="introduction-to-probability.html#cb336-9" tabindex="-1"></a>      <span class="at">to   =</span> xmax,</span>
<span id="cb336-10"><a href="introduction-to-probability.html#cb336-10" tabindex="-1"></a>      <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb336-11"><a href="introduction-to-probability.html#cb336-11" tabindex="-1"></a>      <span class="at">lwd  =</span> <span class="dv">2</span>,</span>
<span id="cb336-12"><a href="introduction-to-probability.html#cb336-12" tabindex="-1"></a>      <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),</span>
<span id="cb336-13"><a href="introduction-to-probability.html#cb336-13" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;pdf&quot;</span>)</span>
<span id="cb336-14"><a href="introduction-to-probability.html#cb336-14" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new=</span><span class="cn">TRUE</span>)</span>
<span id="cb336-15"><a href="introduction-to-probability.html#cb336-15" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> a <span class="sc">*</span> <span class="fu">sqrt</span>(varX)),</span>
<span id="cb336-16"><a href="introduction-to-probability.html#cb336-16" tabindex="-1"></a>      <span class="at">from =</span> xmin,</span>
<span id="cb336-17"><a href="introduction-to-probability.html#cb336-17" tabindex="-1"></a>      <span class="at">to   =</span> xmax,</span>
<span id="cb336-18"><a href="introduction-to-probability.html#cb336-18" tabindex="-1"></a>      <span class="at">ylim =</span> <span class="fu">c</span>(ymin, ymax),</span>
<span id="cb336-19"><a href="introduction-to-probability.html#cb336-19" tabindex="-1"></a>      <span class="at">lwd  =</span> <span class="dv">2</span>,</span>
<span id="cb336-20"><a href="introduction-to-probability.html#cb336-20" tabindex="-1"></a>      <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb336-21"><a href="introduction-to-probability.html#cb336-21" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/varaince-multiplying-by-a-constant-1.png" width="672" /></p>
<p>The blue curve is wider, reflecting the increased variance after scaling by <span class="math inline">\(a=2\)</span>.</p>
<hr />
</div>
<div id="adding-two-random-variables" class="section level4 hasAnchor" number="5.12.4.3">
<h4><span class="header-section-number">5.12.4.3</span> Adding Two Random Variables<a href="introduction-to-probability.html#adding-two-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When combining two random variables, variance depends not only on their individual variances but also on how they move together.</p>
<p>For random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[
\mathbb{V}(X+Y) = \mathbb{V}(X) + \mathbb{V}(Y) + 2,\text{Cov}(X,Y).
\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\text{Cov}(X,Y)=0\)</span>, and the formula simplifies to</p>
<p><span class="math display">\[
\mathbb{V}(X+Y) = \mathbb{V}(X) + \mathbb{V}(Y).
\]</span></p>
<p>This result is central in probability and statistics. It explains why sums of independent random variables tend to have larger variance and underlies many results involving sampling distributions and the Central Limit Theorem.</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="introduction-to-probability.html#cb337-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb337-2"><a href="introduction-to-probability.html#cb337-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb337-3"><a href="introduction-to-probability.html#cb337-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb337-4"><a href="introduction-to-probability.html#cb337-4" tabindex="-1"></a>z <span class="ot">&lt;-</span> x <span class="sc">+</span> y</span>
<span id="cb337-5"><a href="introduction-to-probability.html#cb337-5" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(X): &quot;</span>, <span class="fu">var</span>(x)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(X): 1.02486558733286&quot;</code></pre>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="introduction-to-probability.html#cb339-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(Y): &quot;</span>, <span class="fu">var</span>(y)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(Y): 3.92656163870944&quot;</code></pre>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="introduction-to-probability.html#cb341-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(X)+var(Y): &quot;</span>, <span class="fu">var</span>(x)<span class="sc">+</span><span class="fu">var</span>(y)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(X)+var(Y): 4.9514272260423&quot;</code></pre>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="introduction-to-probability.html#cb343-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(X+Y): &quot;</span>, <span class="fu">var</span>(z)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(X+Y): 4.97086607183689&quot;</code></pre>
<p>Because the variables are generated independently, the variance of the sum is approximately the sum of the variances.</p>
<p>If the variables were positively correlated, the variance of the sum would be larger. If negatively correlated, it would be smaller.</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="introduction-to-probability.html#cb345-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb345-2"><a href="introduction-to-probability.html#cb345-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb345-3"><a href="introduction-to-probability.html#cb345-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb345-4"><a href="introduction-to-probability.html#cb345-4" tabindex="-1"></a>z <span class="ot">&lt;-</span> x <span class="sc">+</span> y</span>
<span id="cb345-5"><a href="introduction-to-probability.html#cb345-5" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(X): &quot;</span>, <span class="fu">var</span>(x)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(X): 1.02486558733286&quot;</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="introduction-to-probability.html#cb347-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(Y): &quot;</span>, <span class="fu">var</span>(y)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(Y): 2.01622541990752&quot;</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="introduction-to-probability.html#cb349-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(X)+var(Y): &quot;</span>, <span class="fu">var</span>(x)<span class="sc">+</span><span class="fu">var</span>(y)<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span><span class="fu">cov</span>(x,y)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(X)+var(Y): 5.1005416048034&quot;</code></pre>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="introduction-to-probability.html#cb351-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste0</span>(<span class="st">&quot;Approx var(X+Y): &quot;</span>, <span class="fu">var</span>(z)))</span></code></pre></div>
<pre><code>## [1] &quot;Approx var(X+Y): 5.1005416048034&quot;</code></pre>
<hr />
</div>
</div>
<div id="summary-2" class="section level3 hasAnchor" number="5.12.5">
<h3><span class="header-section-number">5.12.5</span> Summary<a href="introduction-to-probability.html#summary-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Variance measures how much a random variable fluctuates around its mean. It can be computed as <span class="math inline">\(E[(X-E[X])^2]\)</span> or as <span class="math inline">\(E[X^2] - (E[X])^2\)</span>. Some distributions include variance as a parameter, while in others it must be derived.</p>
<p>Key properties include:</p>
<ul>
<li>Adding a constant does not change variance.</li>
<li>Multiplying by a constant scales variance by the square of that constant.</li>
<li>The variance of a sum depends on both individual variances and covariance.</li>
</ul>
<p>Understanding these properties is essential for analyzing transformations of random variables and for studying sampling distributions, which rely heavily on how means and variances behave under repeated sampling.</p>
<hr />
</div>
</div>
<div id="conditional-expectation" class="section level2 hasAnchor" number="5.13">
<h2><span class="header-section-number">5.13</span> Conditional Expectation<a href="introduction-to-probability.html#conditional-expectation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Conditional expectation formalizes the idea of the “average value of <span class="math inline">\(X\)</span> once we know <span class="math inline">\(Y\)</span>.”</p>
<p>It is one of the central objects in probability and statistics because it connects information, prediction, and uncertainty.</p>
<hr />
<div id="discrete-case" class="section level3 hasAnchor" number="5.13.1">
<h3><span class="header-section-number">5.13.1</span> Discrete Case<a href="introduction-to-probability.html#discrete-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables, the conditional expectation of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span> is</p>
<p><span class="math display">\[
E[X \mid Y=y] = \sum_x x  P(X=x \mid Y=y).
\]</span></p>
<p>It is simply the mean of the conditional distribution of <span class="math inline">\(X\)</span> given that <span class="math inline">\(Y=y\)</span>.</p>
<p>For a random variable <span class="math inline">\(Y\)</span>, the conditional expectation <span class="math inline">\(E[X \mid Y]\)</span> is itself a random variable, because it depends on the random value of <span class="math inline">\(Y\)</span>.</p>
<hr />
</div>
<div id="continuous-case" class="section level3 hasAnchor" number="5.13.2">
<h3><span class="header-section-number">5.13.2</span> Continuous Case<a href="introduction-to-probability.html#continuous-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous with conditional density <span class="math inline">\(f_{X \mid Y}(x \mid y)\)</span>, then</p>
<p><span class="math display">\[
E[X \mid Y=y] = \int x  f_{X \mid Y}(x \mid y) dx.
\]</span></p>
<p>Again, this is just the mean of the conditional distribution.</p>
<hr />
</div>
<div id="interpretation" class="section level3 hasAnchor" number="5.13.3">
<h3><span class="header-section-number">5.13.3</span> Interpretation<a href="introduction-to-probability.html#interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are two complementary ways to understand <span class="math inline">\(E[X \mid Y]\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>As a conditional mean</strong>
It is the expected value of <span class="math inline">\(X\)</span> after restricting attention to the subpopulation where <span class="math inline">\(Y=y\)</span>.</p></li>
<li><p><strong>As the best predictor of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> (in mean squared error)</strong>
Among all functions <span class="math inline">\(g(Y)\)</span>, the function that minimizes</p>
<p><span class="math display">\[
E\big[(X - g(Y))^2\big]
\]</span></p>
<p>is</p>
<p><span class="math display">\[
g(Y) = E[X \mid Y].
\]</span></p></li>
</ol>
<p>This makes conditional expectation fundamental in regression, prediction theory, and statistical learning.</p>
<hr />
</div>
<div id="simple-example" class="section level3 hasAnchor" number="5.13.4">
<h3><span class="header-section-number">5.13.4</span> Simple Example<a href="introduction-to-probability.html#simple-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose</p>
<ul>
<li><span class="math inline">\(P(Y=0)=0.6\)</span>,</li>
<li><span class="math inline">\(P(Y=1)=0.4\)</span>,</li>
</ul>
<p>and</p>
<ul>
<li><span class="math inline">\(X \mid Y=0 \sim N(5,1)\)</span>,</li>
<li><span class="math inline">\(X \mid Y=1 \sim N(10,1)\)</span>.</li>
</ul>
<p>Then</p>
<p><span class="math display">\[
E[X \mid Y=0] = 5, \quad E[X \mid Y=1] = 10.
\]</span></p>
<p>So</p>
<p><span class="math display">\[
E[X \mid Y] =
\begin{cases}
5  &amp; \text{if } Y=0, \\
10 &amp; \text{if } Y=1.
\end{cases}
\]</span></p>
<p>Notice that this is a random variable that takes two possible values depending on <span class="math inline">\(Y\)</span>.</p>
<hr />
</div>
<div id="simulation-illustration" class="section level3 hasAnchor" number="5.13.5">
<h3><span class="header-section-number">5.13.5</span> Simulation Illustration<a href="introduction-to-probability.html#simulation-illustration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="introduction-to-probability.html#cb353-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb353-2"><a href="introduction-to-probability.html#cb353-2" tabindex="-1"></a></span>
<span id="cb353-3"><a href="introduction-to-probability.html#cb353-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb353-4"><a href="introduction-to-probability.html#cb353-4" tabindex="-1"></a></span>
<span id="cb353-5"><a href="introduction-to-probability.html#cb353-5" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fl">0.4</span>)</span>
<span id="cb353-6"><a href="introduction-to-probability.html#cb353-6" tabindex="-1"></a></span>
<span id="cb353-7"><a href="introduction-to-probability.html#cb353-7" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(Y <span class="sc">==</span> <span class="dv">0</span>,</span>
<span id="cb353-8"><a href="introduction-to-probability.html#cb353-8" tabindex="-1"></a>            <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb353-9"><a href="introduction-to-probability.html#cb353-9" tabindex="-1"></a>            <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">1</span>))</span>
<span id="cb353-10"><a href="introduction-to-probability.html#cb353-10" tabindex="-1"></a></span>
<span id="cb353-11"><a href="introduction-to-probability.html#cb353-11" tabindex="-1"></a><span class="co"># Empirical conditional expectations</span></span>
<span id="cb353-12"><a href="introduction-to-probability.html#cb353-12" tabindex="-1"></a><span class="fu">mean</span>(X[Y <span class="sc">==</span> <span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] 5.008079</code></pre>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="introduction-to-probability.html#cb355-1" tabindex="-1"></a><span class="fu">mean</span>(X[Y <span class="sc">==</span> <span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 9.998375</code></pre>
<p>You will observe that:</p>
<ul>
<li>The empirical mean when <span class="math inline">\(Y=0\)</span> is close to 5.</li>
<li>The empirical mean when <span class="math inline">\(Y=1\)</span> is close to 10.</li>
</ul>
<p>These are empirical estimates of</p>
<p><span class="math display">\[
E[X \mid Y=0] \quad \text{and} \quad E[X \mid Y=1].
\]</span></p>
<hr />
</div>
<div id="key-properties" class="section level3 hasAnchor" number="5.13.6">
<h3><span class="header-section-number">5.13.6</span> Key Properties<a href="introduction-to-probability.html#key-properties" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p><strong>Linearity</strong></p>
<p><span class="math display">\[
E[aX + bZ \mid Y] = aE[X \mid Y] + bE[Z \mid Y].
\]</span></p></li>
<li><p><strong>Taking out what is known</strong></p>
<p>If <span class="math inline">\(g(Y)\)</span> is a function of <span class="math inline">\(Y\)</span>, then</p>
<p><span class="math display">\[
E[g(Y)X \mid Y] = g(Y)E[X \mid Y].
\]</span></p></li>
<li><p><strong>Law of Total Expectation</strong></p>
<p><span class="math display">\[
E[X] = E[E[X \mid Y]].
\]</span></p></li>
</ol>
<hr />
</div>
<div id="conceptual-importance" class="section level3 hasAnchor" number="5.13.7">
<h3><span class="header-section-number">5.13.7</span> Conceptual Importance<a href="introduction-to-probability.html#conceptual-importance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Conditional expectation is the mathematical foundation of:</p>
<ul>
<li>Regression functions</li>
<li>Bayesian updating</li>
<li>Hierarchical models</li>
<li>Martingales</li>
<li>Optimal prediction</li>
</ul>
<p>It formalizes the idea that once we incorporate available information (<span class="math inline">\(Y\)</span>), our best description of <span class="math inline">\(X\)</span> is its conditional mean.</p>
</div>
<div id="law-of-total-expectation" class="section level3 hasAnchor" number="5.13.8">
<h3><span class="header-section-number">5.13.8</span> Law of Total Expectation<a href="introduction-to-probability.html#law-of-total-expectation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Law of Total Expectation</strong> (also called the <strong>Tower Property</strong>) provides a systematic way to compute an expected value by conditioning on another random variable.</p>
<hr />
<div id="statement-discrete-case" class="section level4 hasAnchor" number="5.13.8.1">
<h4><span class="header-section-number">5.13.8.1</span> Statement (Discrete Case)<a href="introduction-to-probability.html#statement-discrete-case" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables. Then</p>
<p><span class="math display">\[
E[X] = E\big[E[X \mid Y]\big].
\]</span></p>
<p>If <span class="math inline">\(Y\)</span> is discrete, this can be written explicitly as</p>
<p><span class="math display">\[
E[X] = \sum_y E[X \mid Y=y]  P(Y=y).
\]</span></p>
<p>This formula says:</p>
<ol style="list-style-type: decimal">
<li>Compute the conditional expectation of <span class="math inline">\(X\)</span> given each value of <span class="math inline">\(Y\)</span>.</li>
<li>Average those conditional expectations using the probabilities of <span class="math inline">\(Y\)</span>.</li>
</ol>
<hr />
</div>
<div id="continuous-version" class="section level4 hasAnchor" number="5.13.8.2">
<h4><span class="header-section-number">5.13.8.2</span> Continuous Version<a href="introduction-to-probability.html#continuous-version" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If <span class="math inline">\(Y\)</span> has density <span class="math inline">\(f_Y(y)\)</span>, then</p>
<p><span class="math display">\[
E[X] = \int E[X \mid Y=y]  f_Y(y) dy.
\]</span></p>
<p>Same structure: compute conditional expectations first, then average over the distribution of <span class="math inline">\(Y\)</span>.</p>
<hr />
</div>
<div id="intuition" class="section level4 hasAnchor" number="5.13.8.3">
<h4><span class="header-section-number">5.13.8.3</span> Intuition<a href="introduction-to-probability.html#intuition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Think of <span class="math inline">\(Y\)</span> as defining subpopulations.</p>
<ul>
<li>Within each subgroup (<span class="math inline">\(Y=y\)</span>), <span class="math inline">\(X\)</span> has its own mean.</li>
<li>The overall mean of <span class="math inline">\(X\)</span> is a weighted average of those subgroup means.</li>
<li>The weights are determined by the distribution of <span class="math inline">\(Y\)</span>.</li>
</ul>
<p>In words:</p>
<blockquote>
<p>The expectation of <span class="math inline">\(X\)</span> equals the expectation of its conditional expectation.</p>
</blockquote>
<hr />
</div>
<div id="analytical-example" class="section level4 hasAnchor" number="5.13.8.4">
<h4><span class="header-section-number">5.13.8.4</span> Analytical Example<a href="introduction-to-probability.html#analytical-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose</p>
<ul>
<li><span class="math inline">\(P(Y=0) = 0.6\)</span></li>
<li><span class="math inline">\(P(Y=1) = 0.4\)</span></li>
</ul>
<p>And</p>
<ul>
<li><span class="math inline">\(X \mid Y=0 \sim N(5,1)\)</span></li>
<li><span class="math inline">\(X \mid Y=1 \sim N(10,1)\)</span></li>
</ul>
<p>Then</p>
<p><span class="math display">\[
E[X \mid Y=0] = 5, \quad E[X \mid Y=1] = 10.
\]</span></p>
<p>By the Law of Total Expectation,</p>
<p><span class="math display">\[
E[X] = 5(0.6) + 10(0.4) = 7.
\]</span></p>
<hr />
</div>
</div>
<div id="simulation-example-2" class="section level3 hasAnchor" number="5.13.9">
<h3><span class="header-section-number">5.13.9</span> Simulation Example<a href="introduction-to-probability.html#simulation-example-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now verify this result with simulation.</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="introduction-to-probability.html#cb357-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb357-2"><a href="introduction-to-probability.html#cb357-2" tabindex="-1"></a></span>
<span id="cb357-3"><a href="introduction-to-probability.html#cb357-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb357-4"><a href="introduction-to-probability.html#cb357-4" tabindex="-1"></a></span>
<span id="cb357-5"><a href="introduction-to-probability.html#cb357-5" tabindex="-1"></a><span class="co"># Generate Y</span></span>
<span id="cb357-6"><a href="introduction-to-probability.html#cb357-6" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, <span class="fl">0.4</span>)</span>
<span id="cb357-7"><a href="introduction-to-probability.html#cb357-7" tabindex="-1"></a></span>
<span id="cb357-8"><a href="introduction-to-probability.html#cb357-8" tabindex="-1"></a><span class="co"># Generate X conditional on Y</span></span>
<span id="cb357-9"><a href="introduction-to-probability.html#cb357-9" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(Y <span class="sc">==</span> <span class="dv">0</span>,</span>
<span id="cb357-10"><a href="introduction-to-probability.html#cb357-10" tabindex="-1"></a>            <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb357-11"><a href="introduction-to-probability.html#cb357-11" tabindex="-1"></a>            <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">1</span>))</span>
<span id="cb357-12"><a href="introduction-to-probability.html#cb357-12" tabindex="-1"></a></span>
<span id="cb357-13"><a href="introduction-to-probability.html#cb357-13" tabindex="-1"></a><span class="co"># Empirical mean of X</span></span>
<span id="cb357-14"><a href="introduction-to-probability.html#cb357-14" tabindex="-1"></a><span class="fu">mean</span>(X)</span></code></pre></div>
<pre><code>## [1] 6.99756</code></pre>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="introduction-to-probability.html#cb359-1" tabindex="-1"></a><span class="co"># Conditional means</span></span>
<span id="cb359-2"><a href="introduction-to-probability.html#cb359-2" tabindex="-1"></a><span class="fu">mean</span>(X[Y <span class="sc">==</span> <span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] 5.008079</code></pre>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="introduction-to-probability.html#cb361-1" tabindex="-1"></a><span class="fu">mean</span>(X[Y <span class="sc">==</span> <span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 9.998375</code></pre>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="introduction-to-probability.html#cb363-1" tabindex="-1"></a><span class="co"># Weighted average of conditional means</span></span>
<span id="cb363-2"><a href="introduction-to-probability.html#cb363-2" tabindex="-1"></a><span class="fu">mean</span>(X[Y <span class="sc">==</span> <span class="dv">0</span>]) <span class="sc">*</span> <span class="fu">mean</span>(Y <span class="sc">==</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb363-3"><a href="introduction-to-probability.html#cb363-3" tabindex="-1"></a>  <span class="fu">mean</span>(X[Y <span class="sc">==</span> <span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">mean</span>(Y <span class="sc">==</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 6.99756</code></pre>
<hr />
<div id="what-this-shows" class="section level4 hasAnchor" number="5.13.9.1">
<h4><span class="header-section-number">5.13.9.1</span> What This Shows<a href="introduction-to-probability.html#what-this-shows" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><code>mean(X)</code> will be very close to 7.</li>
<li>The weighted average of the empirical conditional means will also be very close to 7.</li>
<li>The two quantities will be nearly identical (up to simulation error).</li>
</ol>
<p>This simulation illustrates the identity:</p>
<p><span class="math display">\[
E[X] = E[E[X \mid Y]].
\]</span></p>
<p>Even though <span class="math inline">\(X\)</span> comes from two different normal distributions depending on <span class="math inline">\(Y\)</span>, the overall mean is simply the probability-weighted average of the subgroup means.</p>
<hr />
</div>
</div>
<div id="key-insight" class="section level3 hasAnchor" number="5.13.10">
<h3><span class="header-section-number">5.13.10</span> Key Insight<a href="introduction-to-probability.html#key-insight" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Law of Total Expectation allows us to:</p>
<ul>
<li>Decompose complex expectations</li>
<li>Analyze mixture distributions</li>
<li>Understand hierarchical and Bayesian models</li>
<li>Connect conditional and marginal behavior</li>
</ul>
<p>It is one of the structural identities that underlies nearly all modern probability and statistical modeling.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-description.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="from-probability-to-statistics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
