[["index.html", "STAT 5428/6428 Spring 2026 1 Stat 5428/6428 Spring 2026 1.1 Weekly Course Outline 1.2 Topical Learning Outcomes (Aligned with Bloom’s Taxonomy) 1.3 Course Overview 1.4 Course Perspective", " STAT 5428/6428 Spring 2026 Rene Gutierrez University of Texas at El Paso 1 Stat 5428/6428 Spring 2026 This is the website for STAT 5428/6428. It will contain the relevant information for the course and lecture notes and code seen in class. Find the syllabus here: Syllabus Find the github page for the course (with the data sets) here: Github Find the 1.1 Weekly Course Outline Introduction to Statistical Thinking and Data Analysis Role of statistics in research; types of data; study design; observational versus experimental studies; introduction to statistical software (R). Descriptive Statistics and Exploratory Data Analysis (EDA) Numerical summaries; measures of center and variability; graphical methods; data visualization; exploratory data analysis workflow. Probability Concepts and Random Variables Basic probability rules; discrete and continuous random variables; common distributions; expectation and variance. Sampling Distributions and Simulation Concept of sampling distributions; Central Limit Theorem; simulation‑based illustration; intuition for confidence levels and statistical power. Statistical Inference I: Estimation and Confidence Intervals Point estimation; confidence intervals for means and proportions; interpretation and limitations; assumptions and robustness. Statistical Inference II: Hypothesis Testing Null and alternative hypotheses; test statistics; p‑values; Type I/II errors; power considerations and sample size intuition. Inference for Two Populations Two‑sample CIs and tests; paired designs; comparing means and variances; parametric vs. nonparametric choices. Analysis of Variance (ANOVA) One‑way ANOVA; model formulation; assumptions; multiple comparisons; practical interpretation and reporting. Analysis of Covariance (ANCOVA) Incorporating quantitative covariates; adjusted means; interpretation; assumption checks and diagnostics. Simple and Multiple Linear Regression Least squares estimation; interpretation of coefficients; model fit; inference for regression parameters. Regression Diagnostics and Model Assessment Residual analysis; outliers and influence; multicollinearity; remedial measures (transformations, variable selection). Logistic Regression and Categorical Data Analysis Binary response models; odds, odds ratios, and log‑odds; goodness‑of‑fit; interpreting software output. Nonparametric Methods and Alternatives Rank‑based tests and other nonparametric procedures; when and why to choose nonparametric approaches. Experimental Design and Model Selection Principles of randomization, blocking, replication; power and sample size concepts; model selection strategies; reproducibility. Integration, Applications, and Communication of Results Case studies with real datasets; end‑to‑end analysis; ethical considerations; transparent reporting; course review. 1.2 Topical Learning Outcomes (Aligned with Bloom’s Taxonomy) Topic Bloom’s Level Learning Outcome Descriptive Statistics Understand / Apply Summarize and visualize datasets using appropriate numerical and graphical descriptive measures. Probability Concepts Understand Explain fundamental probability concepts and their role in statistical modeling and inference. Sampling Distributions Analyze Analyze sampling variability and illustrate key ideas via simulation. Confidence Intervals Apply / Analyze Construct and interpret confidence intervals; assess assumptions and practical implications. Hypothesis Testing Analyze / Evaluate Evaluate hypotheses using appropriate tests and interpret results in context. Two‑Sample Inference Apply / Analyze Compare two populations using parametric and nonparametric procedures. ANOVA / ANCOVA Analyze Analyze group differences with ANOVA/ANCOVA, incorporating covariates where appropriate. Linear Regression Apply / Analyze Develop and interpret simple and multiple linear regression models. Model Diagnostics Evaluate Assess model assumptions; identify influence and multicollinearity; implement remedial actions. Logistic Regression Analyze Model binary outcomes; interpret odds ratios and fitted model diagnostics. Nonparametric Methods Evaluate Justify and apply nonparametric alternatives when assumptions are violated. Experimental Design Create / Evaluate Design and critique basic experiments (randomization, blocking, replication). Statistical Software (R) Apply / Create Use R to perform analyses, run simulations, interpret output, and communicate results clearly. 1.3 Course Overview Introduction to Statistical Analysis is a graduate‑level course that develops statistical thinking as an integrated process of description, modeling, inference, and revision. The course is structured around the view that statistical analysis is not a linear application of techniques, but an iterative framework in which data are explored, probabilistic assumptions are imposed, conclusions are drawn under those assumptions, and models are reassessed in light of empirical evidence. Emphasis is placed on conceptual understanding, methodological coherence, and interpretation, rather than on formal mathematical derivations. The central organizing principle of the course is the relationship between data and probability models. Data are treated as realizations from an underlying stochastic process, and statistical methods are developed as tools for learning about that process. Throughout the course, students use statistical software (R) to connect theory with practice and to evaluate the adequacy of models when applied to real data. The course is organized into four major sections, each corresponding to a distinct stage of statistical reasoning. 1.3.1 Section I: Data — Description and Intrinsic Properties Purpose: Characterizing data without imposing a probabilistic model The first section focuses on understanding data in its raw form. Prior to modeling or inference, data must be described, explored, and contextualized. This section emphasizes the intrinsic properties of data that can be observed directly, without reference to stochastic assumptions. Key objectives include identifying structure, detecting anomalies, and understanding variability as an empirical phenomenon. Topics in this section include: - Nature and structure of data (measurement scales, data types) - Observational versus experimental data - Study design and sources of variation - Descriptive statistics (location, dispersion, shape) - Graphical summaries and data visualization - Exploratory Data Analysis (EDA) This section establishes descriptive analysis as a critical and non‑optional stage of any sound statistical investigation. 1.3.2 Section II: Data Modeling — Probability as a Generative Mechanism Purpose: Modeling data as realizations of a stochastic process In the second section, probability is introduced as a modeling language for data generation. Rather than focusing on probability as an abstract mathematical subject, it is framed as a tool for encoding assumptions about how data arise. Students study theoretical distributions and sampling behavior implied by different probabilistic models, and they examine the consequences of these assumptions. Topics in this section include: - Basic probability concepts and axioms - Random variables and probability distributions - Discrete and continuous models - Expectation and variance - Sampling distributions - The Central Limit Theorem - Simulation‑based exploration of theoretical results This section provides the probabilistic foundation upon which all subsequent inferential procedures are built. 1.3.3 Section III: Statistical Inference — Learning from Data Under a Model Purpose: Drawing conclusions using probabilistic models The third section develops formal statistical inference as a logical extension of probability modeling. Conditional on an assumed data‑generating mechanism, students derive and apply methods for estimation, hypothesis testing, and uncertainty quantification. Inference is presented as fundamentally model‑dependent, and interpretation is emphasized over mechanical computation. Topics in this section include: - Point estimation and estimators - Confidence intervals and coverage interpretation - Hypothesis testing frameworks - Test statistics and p‑values - Type I and Type II errors - Power and sample size considerations - Inference for one and two populations - Introduction to ANOVA as a comparative inferential framework Students learn to answer substantive research questions while explicitly acknowledging the assumptions that justify their conclusions. 1.3.4 Section IV: Data Remodeling — Model Assessment, Revision, and Alternatives Purpose: Validating conclusions and revisiting assumptions The final section returns to the modeling stage with a critical perspective. After conducting inference, students evaluate whether the probabilistic assumptions underlying their analyses are empirically reasonable. Model adequacy is assessed using diagnostics, and alternative approaches are considered when assumptions fail. This section emphasizes that statistical modeling is iterative and subject to revision rather than a one‑time choice. Topics in this section include: - Linear and multiple regression models - Logistic regression and categorical responses - Residual analysis and goodness‑of‑fit diagnostics - Influential observations and multicollinearity - Remedial measures and transformations - Nonparametric alternatives - Model selection strategies - Interpretation under updated or revised assumptions This section reinforces the idea that statistical conclusions are conditional statements whose validity rests on the suitability of the modeling framework. 1.4 Course Perspective Across all four sections, the course emphasizes coherence between data, models, inference, and validation. By the end of the course, students will be able to approach data analysis as a structured yet flexible process—one that integrates empirical evidence, probabilistic reasoning, and critical evaluation in order to draw defensible conclusions from data. "],["intro-to-r.html", "2 Intro to R 2.1 Basic R Operations and Concepts 2.2 Arithmetic 2.3 Assignment, Object Names, and Data Types 2.4 Vectors 2.5 Functions and Expressions 2.6 Getting Help", " 2 Intro to R These notes introduce core R concepts you’ll use throughout the course. Each section includes short explanations and runnable code. Tip: You can run code line-by-line in RStudio with Ctrl+Enter (Windows) / Cmd+Enter (Mac). 2.1 Basic R Operations and Concepts R works like a powerful calculator and a programming language. You type commands into the Console, and R evaluates them. 2.1.1 The Console and Scripts Console: where commands run immediately. Script (.R) / R Markdown (.Rmd): where you write reproducible work you can save and re-run. 2.1.2 Comments Use # to write comments. R ignores them. # This is a comment 2 + 2 ## [1] 4 2.1.3 Printing and Output Typing an object name prints it. You can also use print(). x &lt;- 10 x ## [1] 10 print(x) ## [1] 10 2.2 Arithmetic R supports standard arithmetic operators: Addition: + Subtraction: - Multiplication: * Division: / Exponentiation: ^ Integer division: %/% Remainder (mod): %% # Basic arithmetic 5 + 3 ## [1] 8 10 - 4 ## [1] 6 6 * 7 ## [1] 42 20 / 5 ## [1] 4 # Powers 2^5 ## [1] 32 # Integer division and remainder 17 %/% 3 ## [1] 5 17 %% 3 ## [1] 2 2.2.1 Order of Operations R follows standard order of operations. Use parentheses to be explicit. 3 + 2 * 5 # multiplication first ## [1] 13 (3 + 2) * 5 # parentheses first ## [1] 25 2.2.2 Logical Comparisons These return TRUE or FALSE. 5 &gt; 3 ## [1] TRUE 5 == 3 ## [1] FALSE 5 != 3 ## [1] TRUE 5 &gt;= 5 ## [1] TRUE 2.3 Assignment, Object Names, and Data Types 2.3.1 Assignment Use &lt;- (common in R code) or =. a &lt;- 5 b = 12 a + b ## [1] 17 2.3.2 Object Names Good names are readable and informative. Valid examples height exam_score x1 Avoid spaces (exam score) starting with numbers (1st) reserved words (if, for, TRUE, FALSE) exam_score &lt;- 92 final_grade &lt;- &quot;A&quot; 2.3.3 Common Data Types 2.3.3.1 Numeric (double) x &lt;- 3.14 typeof(x) ## [1] &quot;double&quot; class(x) ## [1] &quot;numeric&quot; 2.3.3.2 Integer y &lt;- 7L typeof(y) ## [1] &quot;integer&quot; class(y) ## [1] &quot;integer&quot; 2.3.3.3 Character (strings) name &lt;- &quot;Rene&quot; typeof(name) ## [1] &quot;character&quot; class(name) ## [1] &quot;character&quot; 2.3.3.4 Logical flag &lt;- TRUE typeof(flag) ## [1] &quot;logical&quot; class(flag) ## [1] &quot;logical&quot; 2.3.4 3.4 Missing Values R uses NA for missing data. z &lt;- c(1, NA, 3) z ## [1] 1 NA 3 is.na(z) ## [1] FALSE TRUE FALSE NA affects calculations unless you remove missing values. mean(z) # returns NA ## [1] NA mean(z, na.rm = TRUE) # removes NA before computing mean ## [1] 2 2.4 Vectors Vectors are the basic building block in R: an ordered collection of values. 2.4.1 Creating Vectors with c() v &lt;- c(2, 4, 6, 8) v ## [1] 2 4 6 8 2.4.2 Vector Length length(v) ## [1] 4 2.4.3 Vector Arithmetic (Vectorized Operations) Operations apply element-by-element. v + 1 ## [1] 3 5 7 9 v * 2 ## [1] 4 8 12 16 v^2 ## [1] 4 16 36 64 2.4.4 Sequences with : and seq() 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(from = 0, to = 1, by = 0.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 seq(from = 1, to = 10, length.out = 5) ## [1] 1.00 3.25 5.50 7.75 10.00 2.4.5 Repetition with rep() rep(5, times = 4) ## [1] 5 5 5 5 rep(c(&quot;A&quot;,&quot;B&quot;), times = 3) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; rep(1:3, each = 2) ## [1] 1 1 2 2 3 3 2.4.6 Indexing Vectors 2.4.6.1 By position v &lt;- c(10, 20, 30, 40, 50) v[1] # first element ## [1] 10 v[3] # third element ## [1] 30 v[c(2,5)] # second and fifth ## [1] 20 50 2.4.6.2 By negative indexing (remove elements) v[-1] # all but first ## [1] 20 30 40 50 v[-c(2,4)] ## [1] 10 30 50 2.4.6.3 By logical indexing v[v &gt; 25] # keep values greater than 25 ## [1] 30 40 50 v[v == 20] # values equal to 20 ## [1] 20 2.4.7 Named Vectors Names make code easier to read. grades &lt;- c(Midterm = 88, Final = 93, Project = 90) grades ## Midterm Final Project ## 88 93 90 grades[&quot;Final&quot;] ## Final ## 93 2.5 Functions and Expressions 2.5.1 Functions R has many built-in functions: mean(), sum(), sd(), etc. x &lt;- c(2, 4, 6, 8, 10) sum(x) ## [1] 30 mean(x) ## [1] 6 sd(x) ## [1] 3.162278 min(x) ## [1] 2 max(x) ## [1] 10 2.5.1.1 Function arguments Arguments control how the function behaves. round(3.14159, digits = 2) ## [1] 3.14 mean(c(1, NA, 3), na.rm = TRUE) ## [1] 2 2.5.2 Expressions and Nesting You can combine (nest) functions. x &lt;- c(1, 2, 3, 4, 5) sqrt(sum(x^2)) ## [1] 7.416198 2.5.3 Creating Your Own Function Use function() to define reusable code. # A simple function: compute z-scores zscore &lt;- function(x) { (x - mean(x)) / sd(x) } zscore(c(10, 12, 15, 20)) ## [1] -0.9771621 -0.5173211 0.1724404 1.3220429 2.5.3.1 Another example: a summary function quick_summary &lt;- function(x) { c( n = length(x), mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), min = min(x, na.rm = TRUE), max = max(x, na.rm = TRUE) ) } quick_summary(c(1, 2, 3, NA, 5)) ## n mean sd min max ## 5.000000 2.750000 1.707825 1.000000 5.000000 2.6 Getting Help R has excellent built-in help tools. 2.6.1 Help Pages Use ? or help(). ?mean help(sd) 2.6.2 Examples in Help Files Many help pages include examples you can run: example(mean) ## ## mean&gt; x &lt;- c(0:10, 50) ## ## mean&gt; xm &lt;- mean(x) ## ## mean&gt; c(xm, mean(x, trim = 0.10)) ## [1] 8.75 5.50 2.6.3 Searching for a Function If you don’t know the exact name, use help.search() or ??. ??regression help.search(&quot;histogram&quot;) 2.6.4 Inspecting Objects x &lt;- rnorm(5) str(x) ## num [1:5] 0.21207 0.79246 -1.34433 -0.74447 -0.00604 class(x) ## [1] &quot;numeric&quot; typeof(x) ## [1] &quot;double&quot; ls() # list objects in your environment ## [1] &quot;a&quot; &quot;age&quot; &quot;b&quot; &quot;breaks&quot; &quot;category&quot; ## [6] &quot;die&quot; &quot;dieRol&quot; &quot;disease&quot; &quot;eduPar&quot; &quot;eduPer&quot; ## [11] &quot;exam_score&quot; &quot;final_grade&quot; &quot;flag&quot; &quot;gra&quot; &quot;grades&quot; ## [16] &quot;graPer&quot; &quot;griSta&quot; &quot;i&quot; &quot;incLev&quot; &quot;incPer&quot; ## [21] &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;matSco&quot; &quot;matScoBin&quot; ## [26] &quot;maxScoMat&quot; &quot;maxScoVer&quot; &quot;minScoMat&quot; &quot;minScoVer&quot; &quot;n&quot; ## [31] &quot;name&quot; &quot;namEduPar&quot; &quot;namIncLev&quot; &quot;namSch&quot; &quot;numGra&quot; ## [36] &quot;numSch&quot; &quot;numSel&quot; &quot;numStu&quot; &quot;numStuClu&quot; &quot;numStuGra&quot; ## [41] &quot;p&quot; &quot;pA&quot; &quot;populationGroth&quot; &quot;populationGrowth&quot; &quot;pro1&quot; ## [46] &quot;pro12&quot; &quot;pro2&quot; &quot;proCon&quot; &quot;proInf&quot; &quot;proMar&quot; ## [51] &quot;quick_summary&quot; &quot;relFreHea&quot; &quot;samCoi&quot; &quot;samPop&quot; &quot;sch&quot; ## [56] &quot;schDat&quot; &quot;schPer&quot; &quot;sel&quot; &quot;selGra&quot; &quot;selInc&quot; ## [61] &quot;selPar&quot; &quot;selSch&quot; &quot;sen&quot; &quot;spe&quot; &quot;test&quot; ## [66] &quot;v&quot; &quot;verSco&quot; &quot;x&quot; &quot;xm&quot; &quot;y&quot; ## [71] &quot;z&quot; &quot;zscore&quot; rm(x) # remove x ls() ## [1] &quot;a&quot; &quot;age&quot; &quot;b&quot; &quot;breaks&quot; &quot;category&quot; ## [6] &quot;die&quot; &quot;dieRol&quot; &quot;disease&quot; &quot;eduPar&quot; &quot;eduPer&quot; ## [11] &quot;exam_score&quot; &quot;final_grade&quot; &quot;flag&quot; &quot;gra&quot; &quot;grades&quot; ## [16] &quot;graPer&quot; &quot;griSta&quot; &quot;i&quot; &quot;incLev&quot; &quot;incPer&quot; ## [21] &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;matSco&quot; &quot;matScoBin&quot; ## [26] &quot;maxScoMat&quot; &quot;maxScoVer&quot; &quot;minScoMat&quot; &quot;minScoVer&quot; &quot;n&quot; ## [31] &quot;name&quot; &quot;namEduPar&quot; &quot;namIncLev&quot; &quot;namSch&quot; &quot;numGra&quot; ## [36] &quot;numSch&quot; &quot;numSel&quot; &quot;numStu&quot; &quot;numStuClu&quot; &quot;numStuGra&quot; ## [41] &quot;p&quot; &quot;pA&quot; &quot;populationGroth&quot; &quot;populationGrowth&quot; &quot;pro1&quot; ## [46] &quot;pro12&quot; &quot;pro2&quot; &quot;proCon&quot; &quot;proInf&quot; &quot;proMar&quot; ## [51] &quot;quick_summary&quot; &quot;relFreHea&quot; &quot;samCoi&quot; &quot;samPop&quot; &quot;sch&quot; ## [56] &quot;schDat&quot; &quot;schPer&quot; &quot;sel&quot; &quot;selGra&quot; &quot;selInc&quot; ## [61] &quot;selPar&quot; &quot;selSch&quot; &quot;sen&quot; &quot;spe&quot; &quot;test&quot; ## [66] &quot;v&quot; &quot;verSco&quot; &quot;xm&quot; &quot;y&quot; &quot;z&quot; ## [71] &quot;zscore&quot; 2.6.5 Getting Package Help If you use a package, load it first (if installed). # install.packages(&quot;ggplot2&quot;) # run once if needed library(ggplot2) ?ggplot "],["observational-and-experimental-studies.html", "3 Observational and Experimental Studies 3.1 Introduction 3.2 Surveys 3.3 Experimental Studies 3.4 Observational vs Experimental Studies 3.5 Data Quality and Measurement 3.6 Summary", " 3 Observational and Experimental Studies 3.1 Introduction Reliable statistical inference depends critically on the quality and structure of the data collected. This chapter introduces two major frameworks for gathering data: surveys and experimental studies. These methods differ in their level of control, susceptibility to bias, and the strength of the conclusions permitted. This chapter corresponds to the Data stage of the course workflow: collecting, describing data, understanding structure, and identifying intrinsic properties before modeling. 3.2 Surveys Surveys are systematic procedures for obtaining information from a sample that represents a broader population. Effective survey design minimizes bias and ensures appropriate representation. 3.2.1 Populations and Sampling Frames Population: the complete set of units of interest. Sampling frame: the operational list from which the sample is drawn. A mismatch between population and frame leads to coverage error. 3.2.2 Sampling Methods 3.2.2.1 Simple Random Sampling (SRS) Every subset of a given size has an equal chance of being selected. Advantages: - Unbiased representation. We will see what do we mean by this later. - Straightforward design and analysis. 3.2.2.2 Stratified Sampling The population is divided into homogeneous groups (strata), and SRS is applied within each. - Reduces sampling variability - Ensures representation of important subpopulations 3.2.2.3 Cluster Sampling Naturally occurring groups (clusters) are sampled instead of individuals. - Efficient for large geographic populations - Requires careful design to ensure representativeness 3.2.2.4 Systematic Sampling Select every kth element after a random start. - Easy to implement - Sensitive to hidden periodic patterns 3.2.3 Sources of Survey Bias 3.2.3.1 Selection Bias The sample differs systematically from the population. 3.2.3.2 Nonresponse Bias Failure to obtain measurements from selected units creates distortion. 3.2.3.3 Response Bias Arises from poor question wording, respondent discomfort, or interviewer influence. 3.2.4 Questionnaire Design Principles Use clear, neutral language Avoid leading or double‑barreled questions Choose appropriate measurement scales: Nominal Ordinal Interval Ratio 3.3 Experimental Studies Experiments involve deliberate manipulation of conditions to identify causal effects. They provide control over sources of variability through specific design features. 3.3.1 Key Components of Experimental Design 3.3.1.1 Experimental Units Entities to which treatments are applied. 3.3.1.2 Treatments and Control Interventions or conditions imposed on units; control group provides a baseline. 3.3.1.3 Random Assignment Distributes potential confounders across treatment groups. 3.3.1.4 Replication Multiple units exposed to each treatment enable the estimation of experimental variability. 3.3.1.5 Blocking Grouping similar units to reduce the influence of nuisance factors. 3.3.2 Confounding A confounding variable affects both treatment assignment and response, obscuring causal relationships. Good experimental design aims to reduce confounding through: - Randomization - Control - Proper blocking 3.4 Observational vs Experimental Studies 3.4.1 Comparative Characteristics Feature Observational Study Experimental Study Researcher control None High Confounding Often present Reduced Causal claims Weak Strong Difficulty Usually easier to conduct May be expensive or unethical 3.4.2 Interpretational Considerations Observational studies support associational conclusions. Experiments support causal conclusions, conditional on valid design. 3.5 Data Quality and Measurement 3.5.1 Validity and Reliability 3.5.1.1 Validity Whether measurement reflects the concept intended. 3.5.1.2 Reliability Consistency of repeated measurements under identical conditions. 3.5.2 Sources of Variability Natural biological or environmental variation Measurement error Experimental or observational conditions Sampling variability Reducing uncontrolled variation increases the precision and reliability of statistical findings. 3.6 Summary This chapter establishes essential principles for gathering high‑quality data. Surveys and experiments differ in control structure, design complexity, and inferential capability, but both aim to produce data suitable for statistical reasoning. In the next chapter, we transition to Data Modeling, where probability is introduced as a generative mechanism to describe theoretical data behavior. "],["data-description.html", "4 Data Description 4.1 Data Simulation 4.2 Types of Data 4.3 Why Do We Have to Describe Data? 4.4 Types of Data Description 4.5 Techniques to Describe Categorical Data 4.6 Techniques to Describe Quantitative Data 4.7 Frequency Table 4.8 General Guidelines for Successful Graphics 4.9 Measures of Central Tendency 4.10 Measures of Variability 4.11 Techniques for Two Variables", " 4 Data Description In this section, we explore how data can be classified, summarized, and interpreted. We introduce key concepts for both categorical and quantitative data, and we examine graphical and numerical tools used in statistics. 4.1 Data Simulation For this section we will simulate a data set for student performance in math and verbal tests accross 10 schools. For each student we will include the following variables: Variables: Student ID Student School Student Grade Student Age Income Level Education Level Parents Math Score Verbal Score 4.2 Types of Data 4.2.1 Quantitative vs. Qualitative Data Definition 4.1 (Types of Data) Data can be classified into quantitative (numerical) and qualitative (categorical) types. Quantitative data consist of numerical measurements; qualitative data describe categories or attributes. 4.2.2 Quantitative Data Quantitative data represent numerical values that can be measured. Examples: Heights of students Weights of packages Exam scores They can be: Discrete: countable (e.g., number of children) Continuous: measured on a continuum (e.g., height, time) 4.2.3 Qualitative Data Qualitative (categorical) data represent non‑numeric labels or groups. Examples: Sex Favorite color Yes/No responses Satisfaction Level These categories may or may not have a natural order. 4.3 Why Do We Have to Describe Data? Describing data allows us to: Explain data. Identify patterns or trends Summarize large datasets compactly Compare groups or populations Detect outliers or unusual observations Prepare for inferential analysis Raw data alone rarely reveal useful insight without proper summarization. 4.4 Types of Data Description 4.4.1 Numerical Description Focuses on numerical summaries such as: Mean Median Standard deviation Quantiles 4.4.2 Graphical Description Focuses on visualizations such as: Histograms Bar charts Pie charts Boxplots Both approaches are essential for understanding data behavior. 4.5 Techniques to Describe Categorical Data 4.5.1 Graphical Techniques 4.5.1.1 Bar Charts Bar charts display the frequency of each category using rectangular bars. Example in R: barplot(table(schDat$School), col=&quot;steelblue&quot;) 4.5.2 Pie Charts Pie charts show proportions as slices of a circle. pie(table(schDat$Income_Level)) 4.5.3 Numerical Techniques The main numerical summaries for categorical data include: Counts (frequencies) Proportions Relative frequencies Example: # Frequency Table table(schDat$Income_Level) ## ## High Low Middle ## 937 2747 5316 # Proportional Table prop.table(table(schDat$Income_Level)) ## ## High Low Middle ## 0.1041111 0.3052222 0.5906667 4.6 Techniques to Describe Quantitative Data 4.6.1 Displaying All the Data Tools: Raw data lists Dotplots Stem‑and‑leaf plots Example: stem(c(23,25,25,30,31,33,35,40)) ## ## The decimal point is 1 digit(s) to the right of the | ## ## 2 | 3 ## 2 | 55 ## 3 | 013 ## 3 | 5 ## 4 | 0 4.7 Frequency Table A frequency table groups quantitative data into intervals (classes). 4.7.1 Class Frequency Definition 4.2 (Class Frequency) The class frequency is the number of observations that fall within a specific interval. 4.7.2 Relative Frequency Definition 4.3 (Relative Frequency) The relative frequency is the class frequency divided by the total number of observations. Example: # Frequency Classes breaks &lt;- seq(0, 800, by = 100) matScoBin &lt;- cut(matSco, breaks = breaks, right = TRUE) table(matScoBin) ## matScoBin ## (0,100] (100,200] (200,300] (300,400] (400,500] (500,600] (600,700] (700,800] ## 756 1163 1132 1156 1166 1203 1225 1198 prop.table(table(matScoBin)) ## matScoBin ## (0,100] (100,200] (200,300] (300,400] (400,500] (500,600] (600,700] (700,800] ## 0.08400933 0.12923658 0.12579175 0.12845872 0.12956995 0.13368152 0.13612624 0.13312590 4.7.3 Histograms Histograms partition quantitative data into bins and display frequencies as bars. 4.7.3.1 How to Build Them Steps: Choose number of intervals (bins). Determine bin width. Count frequencies per bin. Draw adjacent bars. 4.7.3.2 Number of Intervals Common guidelines: Square-root rule: \\[k = \\sqrt{n}\\] Sturges’ rule: \\[k = 1 + \\log_2(n)\\] Example: hist(schDat$Math_Score[schDat$Grade == 1], col=&quot;skyblue&quot;, main = &quot;Histogram of Math Scores for First Grade&quot;, xlab = &quot;Math Scores&quot;) 4.7.3.3 Characteristics of Histograms 4.7.3.3.1 Unimodal One peak. 4.7.3.3.2 Bimodal Two distinct peaks. hist(schDat$Math_Score[schDat$Grade == 3 | schDat$Grade == 5], col=&quot;skyblue&quot;, main = &quot;Histogram of Math Scores for Third and Fifth Grade&quot;, xlab = &quot;Math Scores&quot;, breaks = 20) 4.7.3.3.3 Symmetry Symmetric: left and right sides mirror Skewed Right: long right tail Skewed Left: long left tail 4.8 General Guidelines for Successful Graphics Label axes clearly Use consistent scales Avoid unnecessary decoration Ensure readability Highlight key trends 4.9 Measures of Central Tendency 4.9.1 Mode Definition 4.4 (Mode) The mode is the value that occurs most frequently in a dataset. 4.9.2 Median Definition 4.5 (Median) The median is the middle value when data are ordered. 4.9.3 Mean Definition 4.6 (Mean) The mean is the arithmetic average of all observations \\(x_1, x_2, \\ldots x_n\\). Denoted by: \\[ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} \\] Example: mean(schDat$Math_Score[schDat$Grade == 1]); median(schDat$Math_Score[schDat$Grade == 1]) ## [1] 81.79429 ## [1] 82 4.10 Measures of Variability 4.10.1 Range Definition 4.7 (Range) The range is the difference between the maximum and minimum values in a dataset. c(max(schDat$Math_Score[schDat$Grade == 3]), min(schDat$Math_Score[schDat$Grade == 3])) ## [1] 354 156 max(schDat$Math_Score[schDat$Grade == 3]) - min(schDat$Math_Score[schDat$Grade == 3]) ## [1] 198 # Average score 3rd Grade mean(schDat$Math_Score[schDat$Grade == 3]) ## [1] 251.898 # Average score 4th Grade mean(schDat$Math_Score[schDat$Grade == 4]) ## [1] 335.5399 4.10.2 Quantiles Quantiles divide data into equal-sized groups. 4.10.3 Quantile Plot plot(sort(schDat$Math_Score[schDat$Age == 11]), ppoints(length(schDat$Math_Score[schDat$Age == 11])), xlab = &quot;Math Score&quot;, ylab = &quot;Quantile&quot;) 4.10.4 Percentiles The p-th percentile is the value below which p% of the data fall. 4.10.5 Quartiles Quartiles divide data into four equal parts: Q1 Q2 (median) Q3 4.10.6 Deviation A deviation is the difference between an observation and the mean. That is \\[ x_i - \\bar{x} \\] 4.10.7 Variance Definition 4.8 (Variance) The variance measures the average squared deviation from the mean. \\[ s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} \\] 4.10.8 Standard Deviation Definition 4.9 (Standard Deviation) The standard deviation is the square root of the variance. Example: sd(schDat$Math_Score) ## [1] 221.3948 var(schDat$Math_Score) ## [1] 49015.67 4.10.9 Absolute Deviation Average of absolute deviations from the mean. 4.10.10 Median Absolute Deviation (MAD) Robust measure of variability. mad(matSco) ## [1] 281.694 4.10.11 68%, 95%, and 99.7% Empirical Rule For bell‑shaped distributions: 68% of values fall within 1 SD of mean 95% within 2 SD 99.7% within 3 SD 4.10.12 Coefficient of Variation The CV measures spread relative to the mean: \\[ CV = \\frac{SD}{\\text{mean}} \\] 4.10.13 The Boxplot Displays: Median Quartiles Whiskers Outliers Example: boxplot(schDat$Math_Score[schDat$Grade == 3], schDat$Math_Score[schDat$Grade == 4], schDat$Math_Score[schDat$Grade == 5]) 4.10.14 Outliers Observations unusually far from the rest: Often defined as: Below Q1 – 1.5(IQR) Above Q3 + 1.5(IQR) 4.11 Techniques for Two Variables 4.11.1 Contingency Tables Summarize two categorical variables. table(schDat$Income_Level, schDat$Parent_Education) ## ## College Graduate School Highschool No Highschool ## High 225 219 256 237 ## Low 692 648 671 736 ## Middle 1330 1294 1339 1353 4.11.2 Stacked Bar Graph barplot(table(schDat$Income_Level, schDat$Parent_Education), beside=FALSE) 4.11.3 Cluster Bar Graph barplot(table(schDat$Income_Level, schDat$Parent_Education), beside=TRUE) 4.11.4 Scatter Plot Used for two quantitative variables. plot(schDat$Math_Score[schDat$Grade == 8], schDat$Verbal_Score[schDat$Grade == 8], xlab = &quot;Math Score&quot;, ylab = &quot;Verbal Score&quot;) 4.11.5 Correlation Coefficient Measures linear association. cor(schDat$Math_Score, schDat$Verbal_Score) ## [1] 0.9979339 4.11.6 Block Boxplots Grouped boxplots. boxplot(Math_Score ~ Income_Level, data = schDat) 4.11.7 Block Scatter Plots Scatter plots grouped by category. "],["introduction-to-probability.html", "5 Introduction to Probability 5.1 Probability as a Model for Uncertain and Complex Phenomena 5.2 Interpretations of Probability 5.3 How to Build Probability Models 5.4 Summary", " 5 Introduction to Probability Probability provides a mathematical framework for reasoning about uncertainty and complexity. In this lecture, we develop probability ideas using simulation and computation, treating probability models as data‑producing mechanisms. This approach emphasizes understanding over memorization and mirrors how probability is used in modern data analysis. 5.1 Probability as a Model for Uncertain and Complex Phenomena 5.1.1 Probability as a Data‑Producing Machine Definition 5.1 (Probability Model) A probability model is a mathematical description of a process that produces data under uncertainty, assigning probabilities to possible outcomes. Rather than viewing probability as an abstract formula, we interpret it as a machine that generates data. Once a model is specified, we can simulate outcomes and study their long‑run behavior. 5.1.1.1 Example: A Simple Data‑Producing Machine set.seed(2026) sample(c(&quot;Heads&quot;, &quot;Tails&quot;), size = 10, replace = TRUE, prob = c(0.5, 0.5)) ## [1] &quot;Heads&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Tails&quot; &quot;Heads&quot; &quot;Tails&quot; &quot;Heads&quot; Each run produces different data, yet the underlying mechanism remains fixed. 5.1.2 Example of an Uncertain Phenomenon Uncertain phenomena are characterized by random variation, even when conditions appear identical. Example: Tossing a fair coin Rolling a die Drawing a card from a shuffled deck 5.1.3 Example of a Complex Phenomenon Complex phenomena involve many interacting components: Student performance Financial markets Weather systems Even if deterministic rules exist, the combined system behaves unpredictably. # Function Describing Population Growth populationGrowth &lt;- function(x = 1, r = 1, t = 1){ for(i in 1:t){ x &lt;- r * x * (1 - x) } return(x) } # Samples obtained at different initial states for 10 periods griSta &lt;- seq(0.1,0.9, by = 0.001) samPop &lt;- populationGrowth(x = griSta, r = 4, t = 100) # Print Table names(samPop) &lt;- griSta print(round(samPop, 2)) ## 0.1 0.101 0.102 0.103 0.104 0.105 0.106 0.107 0.108 0.109 0.11 0.111 0.112 0.113 0.114 0.115 0.116 ## 0.37 0.44 1.00 0.15 0.68 0.01 0.17 0.43 0.52 0.09 0.88 0.39 0.09 0.65 0.56 0.00 0.86 ## 0.117 0.118 0.119 0.12 0.121 0.122 0.123 0.124 0.125 0.126 0.127 0.128 0.129 0.13 0.131 0.132 0.133 ## 0.55 0.45 0.29 0.27 0.07 0.99 0.84 0.43 0.78 0.82 1.00 0.66 0.93 0.96 0.96 0.26 1.00 ## 0.134 0.135 0.136 0.137 0.138 0.139 0.14 0.141 0.142 0.143 0.144 0.145 0.146 0.147 0.148 0.149 0.15 ## 0.19 0.48 0.54 1.00 0.06 0.08 0.77 0.52 0.16 0.89 0.12 0.14 0.96 0.71 0.94 0.75 0.64 ## 0.151 0.152 0.153 0.154 0.155 0.156 0.157 0.158 0.159 0.16 0.161 0.162 0.163 0.164 0.165 0.166 0.167 ## 0.87 0.00 0.35 0.38 0.80 0.93 0.10 0.27 0.83 0.74 0.22 0.03 0.16 0.41 0.79 0.00 0.70 ## 0.168 0.169 0.17 0.171 0.172 0.173 0.174 0.175 0.176 0.177 0.178 0.179 0.18 0.181 0.182 0.183 0.184 ## 0.37 0.14 0.50 0.94 0.14 0.32 0.31 0.59 0.58 0.99 0.33 0.71 0.92 0.96 0.60 0.86 0.09 ## 0.185 0.186 0.187 0.188 0.189 0.19 0.191 0.192 0.193 0.194 0.195 0.196 0.197 0.198 0.199 0.2 0.201 ## 0.59 0.90 0.97 0.02 0.17 0.14 0.28 0.01 0.46 0.82 1.00 0.51 0.96 0.97 1.00 0.88 0.20 ## 0.202 0.203 0.204 0.205 0.206 0.207 0.208 0.209 0.21 0.211 0.212 0.213 0.214 0.215 0.216 0.217 0.218 ## 0.72 0.60 0.02 0.02 0.13 0.01 0.56 0.94 0.12 0.96 0.32 0.86 0.34 0.27 0.79 0.34 0.22 ## 0.219 0.22 0.221 0.222 0.223 0.224 0.225 0.226 0.227 0.228 0.229 0.23 0.231 0.232 0.233 0.234 0.235 ## 0.26 0.71 0.94 0.27 0.00 0.78 0.59 0.24 0.78 0.03 0.04 0.07 0.22 0.79 1.00 0.32 0.43 ## 0.236 0.237 0.238 0.239 0.24 0.241 0.242 0.243 0.244 0.245 0.246 0.247 0.248 0.249 0.25 0.251 0.252 ## 0.08 0.23 0.99 0.18 0.76 0.78 0.32 0.92 0.00 0.45 0.65 0.73 1.00 0.97 0.75 0.84 0.86 ## 0.253 0.254 0.255 0.256 0.257 0.258 0.259 0.26 0.261 0.262 0.263 0.264 0.265 0.266 0.267 0.268 0.269 ## 0.16 0.85 0.00 0.44 0.46 1.00 0.34 0.69 1.00 0.04 0.00 0.76 0.37 0.00 0.94 0.76 0.01 ## 0.27 0.271 0.272 0.273 0.274 0.275 0.276 0.277 0.278 0.279 0.28 0.281 0.282 0.283 0.284 0.285 0.286 ## 0.99 0.43 0.02 0.78 0.42 0.97 0.83 0.96 0.97 0.02 0.54 0.37 0.34 0.48 0.90 0.17 0.37 ## 0.287 0.288 0.289 0.29 0.291 0.292 0.293 0.294 0.295 0.296 0.297 0.298 0.299 0.3 0.301 0.302 0.303 ## 0.80 0.88 0.01 0.29 0.92 0.81 1.00 0.44 0.24 0.07 0.42 0.93 0.80 0.75 0.16 0.08 0.16 ## 0.304 0.305 0.306 0.307 0.308 0.309 0.31 0.311 0.312 0.313 0.314 0.315 0.316 0.317 0.318 0.319 0.32 ## 0.04 0.18 0.30 0.85 1.00 1.00 0.02 0.83 0.03 0.03 0.95 0.51 0.03 0.97 0.81 0.33 0.34 ## 0.321 0.322 0.323 0.324 0.325 0.326 0.327 0.328 0.329 0.33 0.331 0.332 0.333 0.334 0.335 0.336 0.337 ## 0.84 0.11 0.16 0.40 0.46 0.03 0.53 0.02 0.05 0.00 0.63 0.18 0.70 0.45 0.99 0.41 0.49 ## 0.338 0.339 0.34 0.341 0.342 0.343 0.344 0.345 0.346 0.347 0.348 0.349 0.35 0.351 0.352 0.353 0.354 ## 0.39 0.24 0.10 0.38 0.84 1.00 0.90 0.00 0.13 0.05 0.76 0.73 1.00 0.58 0.31 0.00 0.56 ## 0.355 0.356 0.357 0.358 0.359 0.36 0.361 0.362 0.363 0.364 0.365 0.366 0.367 0.368 0.369 0.37 0.371 ## 0.41 0.56 0.81 1.00 0.61 0.93 0.88 0.23 1.00 0.36 0.00 0.98 0.01 0.36 0.67 0.97 0.26 ## 0.372 0.373 0.374 0.375 0.376 0.377 0.378 0.379 0.38 0.381 0.382 0.383 0.384 0.385 0.386 0.387 0.388 ## 0.00 0.79 0.98 0.29 0.48 0.87 0.15 0.08 0.86 0.61 0.30 0.12 0.07 0.17 0.14 0.69 0.79 ## 0.389 0.39 0.391 0.392 0.393 0.394 0.395 0.396 0.397 0.398 0.399 0.4 0.401 0.402 0.403 0.404 0.405 ## 0.47 0.76 0.44 0.58 1.00 0.69 0.08 0.73 1.00 0.23 0.92 0.24 0.86 0.07 0.52 0.91 0.91 ## 0.406 0.407 0.408 0.409 0.41 0.411 0.412 0.413 0.414 0.415 0.416 0.417 0.418 0.419 0.42 0.421 0.422 ## 0.20 0.56 0.47 0.40 0.21 0.60 0.41 0.11 0.44 0.97 0.30 0.87 0.67 0.93 0.11 0.70 0.43 ## 0.423 0.424 0.425 0.426 0.427 0.428 0.429 0.43 0.431 0.432 0.433 0.434 0.435 0.436 0.437 0.438 0.439 ## 0.74 0.23 0.66 0.52 0.85 0.83 0.64 0.68 0.10 0.15 0.91 0.74 0.03 0.90 0.97 1.00 0.11 ## 0.44 0.441 0.442 0.443 0.444 0.445 0.446 0.447 0.448 0.449 0.45 0.451 0.452 0.453 0.454 0.455 0.456 ## 0.86 0.01 0.58 0.61 0.59 0.17 0.68 0.28 0.27 0.13 0.06 0.66 0.94 0.92 0.07 0.01 0.17 ## 0.457 0.458 0.459 0.46 0.461 0.462 0.463 0.464 0.465 0.466 0.467 0.468 0.469 0.47 0.471 0.472 0.473 ## 0.35 0.26 0.07 0.45 0.04 0.00 0.00 0.99 0.86 0.98 0.95 0.61 0.69 0.17 0.59 0.53 0.03 ## 0.474 0.475 0.476 0.477 0.478 0.479 0.48 0.481 0.482 0.483 0.484 0.485 0.486 0.487 0.488 0.489 0.49 ## 0.46 0.02 0.05 0.25 0.97 0.95 0.79 0.55 0.04 0.59 0.02 0.00 0.39 0.00 0.29 0.89 0.92 ## 0.491 0.492 0.493 0.494 0.495 0.496 0.497 0.498 0.499 0.5 0.501 0.502 0.503 0.504 0.505 0.506 0.507 ## 0.11 0.60 0.27 1.00 0.16 0.93 0.40 0.94 0.01 0.00 0.01 0.94 0.40 0.93 0.16 1.00 0.27 ## 0.508 0.509 0.51 0.511 0.512 0.513 0.514 0.515 0.516 0.517 0.518 0.519 0.52 0.521 0.522 0.523 0.524 ## 0.60 0.11 0.92 0.89 0.29 0.00 0.39 0.00 0.02 0.59 0.04 0.55 0.79 0.95 0.97 0.25 0.05 ## 0.525 0.526 0.527 0.528 0.529 0.53 0.531 0.532 0.533 0.534 0.535 0.536 0.537 0.538 0.539 0.54 0.541 ## 0.02 0.46 0.03 0.53 0.59 0.17 0.69 0.61 0.95 0.98 0.86 0.99 0.00 0.00 0.04 0.45 0.07 ## 0.542 0.543 0.544 0.545 0.546 0.547 0.548 0.549 0.55 0.551 0.552 0.553 0.554 0.555 0.556 0.557 0.558 ## 0.26 0.35 0.17 0.01 0.07 0.92 0.94 0.75 0.06 0.13 0.61 0.28 0.68 0.17 0.59 0.61 0.58 ## 0.559 0.56 0.561 0.562 0.563 0.564 0.565 0.566 0.567 0.568 0.569 0.57 0.571 0.572 0.573 0.574 0.575 ## 0.96 0.04 0.11 0.54 0.97 0.58 0.03 0.74 0.32 0.34 0.06 0.94 0.44 0.89 0.85 0.50 0.18 ## 0.576 0.577 0.578 0.579 0.58 0.581 0.582 0.583 0.584 0.585 0.586 0.587 0.588 0.589 0.59 0.591 0.592 ## 0.23 0.63 0.43 0.68 0.11 0.93 0.67 0.87 0.30 0.97 0.44 0.11 0.41 0.60 0.21 0.40 0.47 ## 0.593 0.594 0.595 0.596 0.597 0.598 0.599 0.6 0.601 0.602 0.603 0.604 0.605 0.606 0.607 0.608 0.609 ## 0.56 0.20 0.91 0.91 0.52 0.07 0.86 0.24 0.92 0.23 1.00 0.73 0.08 0.69 1.00 0.58 0.44 ## 0.61 0.611 0.612 0.613 0.614 0.615 0.616 0.617 0.618 0.619 0.62 0.621 0.622 0.623 0.624 0.625 0.626 ## 0.76 0.47 0.79 0.69 0.14 0.17 0.07 0.12 0.30 0.61 0.86 0.08 0.15 0.87 0.48 0.29 0.98 ## 0.627 0.628 0.629 0.63 0.631 0.632 0.633 0.634 0.635 0.636 0.637 0.638 0.639 0.64 0.641 0.642 0.643 ## 0.79 0.00 0.26 0.97 0.67 0.36 0.01 0.98 0.00 0.36 1.00 0.23 0.88 0.93 0.61 1.00 0.81 ## 0.644 0.645 0.646 0.647 0.648 0.649 0.65 0.651 0.652 0.653 0.654 0.655 0.656 0.657 0.658 0.659 0.66 ## 0.56 0.41 0.56 0.00 0.31 0.58 1.00 0.73 0.76 0.05 0.13 0.00 0.90 1.00 0.84 0.38 0.10 ## 0.661 0.662 0.663 0.664 0.665 0.666 0.667 0.668 0.669 0.67 0.671 0.672 0.673 0.674 0.675 0.676 0.677 ## 0.41 0.39 0.71 0.41 0.82 0.45 0.38 0.18 0.00 0.00 0.97 0.02 0.98 0.03 0.46 0.40 0.16 ## 0.678 0.679 0.68 0.681 0.682 0.683 0.684 0.685 0.686 0.687 0.688 0.689 0.69 0.691 0.692 0.693 0.694 ## 0.02 0.84 0.34 0.33 0.02 0.97 0.09 0.51 0.25 0.03 0.21 0.83 1.00 1.00 1.00 0.85 0.30 ## 0.695 0.696 0.697 0.698 0.699 0.7 0.701 0.702 0.703 0.704 0.705 0.706 0.707 0.708 0.709 0.71 0.711 ## 0.18 0.04 0.16 0.08 0.16 0.75 0.80 0.93 0.42 0.07 0.24 0.44 1.00 0.81 0.92 0.29 0.01 ## 0.712 0.713 0.714 0.715 0.716 0.717 0.718 0.719 0.72 0.721 0.722 0.723 0.724 0.725 0.726 0.727 0.728 ## 0.88 0.80 0.37 0.17 0.90 0.48 0.34 0.37 0.54 0.02 0.97 0.96 0.83 0.97 0.42 0.78 0.02 ## 0.729 0.73 0.731 0.732 0.733 0.734 0.735 0.736 0.737 0.738 0.739 0.74 0.741 0.742 0.743 0.744 0.745 ## 0.43 0.99 0.01 0.76 0.94 0.00 0.37 0.76 0.00 0.04 1.00 0.69 0.34 1.00 0.46 0.44 0.00 ## 0.746 0.747 0.748 0.749 0.75 0.751 0.752 0.753 0.754 0.755 0.756 0.757 0.758 0.759 0.76 0.761 0.762 ## 0.85 0.16 0.86 0.84 0.75 0.97 1.00 0.73 0.65 0.45 0.00 0.69 0.68 0.56 0.76 0.92 0.74 ## 0.763 0.764 0.765 0.766 0.767 0.768 0.769 0.77 0.771 0.772 0.773 0.774 0.775 0.776 0.777 0.778 0.779 ## 0.20 0.24 1.00 0.78 0.78 0.79 0.88 0.17 0.03 1.00 0.84 0.24 0.71 0.73 0.00 0.77 0.69 ## 0.78 0.781 0.782 0.783 0.784 0.785 0.786 0.787 0.788 0.789 0.79 0.791 0.792 0.793 0.794 0.795 0.796 ## 0.01 0.26 0.22 1.00 0.00 0.14 0.73 0.00 0.87 0.96 0.75 0.02 0.10 0.01 0.43 0.70 0.54 ## 0.797 0.798 0.799 0.8 0.801 0.802 0.803 0.804 0.805 0.806 0.807 0.808 0.809 0.81 0.811 0.812 0.813 ## 0.84 0.11 0.20 0.55 1.00 0.56 1.00 0.06 0.79 0.93 0.43 0.01 0.54 0.92 0.14 0.89 0.97 ## 0.814 0.815 0.816 0.817 0.818 0.819 0.82 0.821 0.822 0.823 0.824 0.825 0.826 0.827 0.828 0.829 0.83 ## 0.81 0.28 0.26 0.86 0.60 0.30 1.00 0.81 0.96 0.53 0.99 0.88 0.31 0.89 0.14 0.94 0.00 ## 0.831 0.832 0.833 0.834 0.835 0.836 0.837 0.838 0.839 0.84 0.841 0.842 0.843 0.844 0.845 0.846 0.847 ## 0.17 0.21 0.70 0.00 0.79 0.80 0.16 0.69 0.22 0.88 0.09 0.27 1.00 0.93 0.80 0.38 0.35 ## 0.848 0.849 0.85 0.851 0.852 0.853 0.854 0.855 0.856 0.857 0.858 0.859 0.86 0.861 0.862 0.863 0.864 ## 0.00 0.87 0.64 0.75 0.94 0.71 0.96 0.14 0.12 0.89 0.16 0.52 0.77 0.08 0.06 1.00 0.54 ## 0.865 0.866 0.867 0.868 0.869 0.87 0.871 0.872 0.873 0.874 0.875 0.876 0.877 0.878 0.879 0.88 0.881 ## 0.48 0.19 1.00 0.26 0.96 0.96 0.93 0.66 1.00 0.82 0.78 0.43 0.84 0.99 0.00 0.60 0.29 ## 0.882 0.883 0.884 0.885 0.886 0.887 0.888 0.889 0.89 0.891 0.892 0.893 0.894 0.895 0.896 0.897 0.898 ## 0.45 0.55 0.86 0.60 0.56 0.15 1.00 0.20 0.94 0.87 0.52 0.04 0.00 0.00 0.42 0.02 0.99 ## 0.899 0.9 ## 0.44 0.37 # plot(x = griSta, y = samPop, type = &#39;l&#39;) 5.1.4 Using Models to Explain Phenomena Models simplify reality while retaining essential structure. A good probability model: Explains observed variability Produces realistic simulated data Can be refined as new information becomes available 5.1.5 Final Idea: Inference Without a Model When you don’t have a model it is really hard to do inference. All of probability and statistics rely, either explicitly or implicitly, on the existence of a model that describes how data are generated. A model provides a structured description of uncertainty. It specifies: - what outcomes are possible, - how these outcomes are related, - and how frequently they are expected to occur. Without such a structure, observed data are simply a collection of numbers or categories with no principled way to generalize beyond what has already been seen. In probability theory, models play the role of an idealized data‑generating mechanism. They allow us to ask meaningful questions such as: - How surprising is this outcome? - What should we expect to see if the process repeats? - How does changing assumptions affect long‑run behavior? When a model is clearly defined, inference becomes possible because we can compare observed data to what the model predicts. Differences between data and model expectations can then be interpreted as evidence about the underlying process. By contrast, without a model: - There is no clear notion of what outcomes are “likely” or “unlikely.” - Uncertainty cannot be quantified in a principled way. - There is no reliable mechanism for extrapolation beyond the observed data. - Conclusions become informal, subjective, or anecdotal. Definition 5.2 (Statistical Inference) Statistical inference is the process of drawing conclusions about an underlying process, population, or mechanism by comparing observed data to a probabilistic model. 5.2 Interpretations of Probability 5.2.1 Classical Interpretation Definition 5.3 (Classical Probability) Classical probability defines the probability of an event as the ratio of the number of favorable outcomes to the total number of equally likely outcomes. The classical interpretation is the earliest formal approach to probability. It views probability as a logical consequence of symmetry in a well‑defined experiment. When all outcomes are equally plausible before the experiment is performed, probability can be computed by simple counting. This approach treats probability as an a priori quantity: probabilities are determined entirely by the structure of the experiment, not by observed data. 5.2.1.1 History The classical interpretation emerged in the 17th and 18th centuries, primarily through the study of games of chance. Key contributors include: Blaise Pascal and Pierre de Fermat, who studied fair division of gambling stakes. Pierre‑Simon Laplace, who formalized probability as a mathematical discipline. During this period, probability problems typically involved: dice, cards, coins, lottery‑type mechanisms. These systems were appealing because they naturally possessed symmetry, making the assumption of equally likely outcomes reasonable. 5.2.1.2 Key Assumption: Equally Likely Outcomes The classical definition requires that all outcomes in the sample space are equally likely. This assumption is critical and restrictive. Consequences: Classical probability works well for idealized systems with symmetry. It becomes difficult or impossible to apply when outcomes are not equally likely or when the sample space is not finite and well defined. Thus, the classical interpretation is best viewed as a modeling choice, not a universal definition. 5.2.1.3 Example: Rolling a Fair Die Consider rolling a well‑balanced six‑sided die. The sample space contains 6 outcomes: {1, 2, 3, 4, 5, 6} Each outcome is assumed equally likely. The probability of rolling a 4 is therefore: 1 / 6 ## [1] 0.1666667 This probability is computed before any data are collected and does not depend on repeated trials. 5.2.1.4 Outcomes and Events Definition 5.4 (Outcome) An outcome is a single possible result of a random experiment. Examples: Rolling a 4 on a die Drawing the Ace of Spades from a deck Getting Heads in a coin toss Outcomes represent the most granular level of description of a random experiment. Definition 5.5 (Event) An event is a collection of one or more outcomes. Examples: Rolling an even number: {2, 4, 6} Drawing a face card from a deck Getting at least one Head in two coin tosses An event can be interpreted as a question about the experiment whose answer is either yes (the event occurs) or no (the event does not occur). 5.2.1.5 Strengths and Limitations of the Classical Interpretation Strengths Simple and intuitive Requires no data Provides exact probabilities in symmetric settings Limitations Depends critically on the equally likely assumption Not suitable for complex, real‑world phenomena Does not explain how probabilities change or emerge with repeated observations These limitations motivate alternative interpretations of probability, especially those based on long‑run behavior and data‑driven modeling. In summary, the classical interpretation introduces probability as a counting rule based on symmetry. While historically foundational and mathematically elegant, it is best understood as a special case of probability modeling, applicable only when symmetry and equally likely outcomes are plausible assumptions. 5.2.2 Relative Frequency Interpretation Definition 5.6 (Relative Frequency Probability) Probability is defined as the long‑run proportion of times an event occurs when a random experiment is repeated under identical conditions. The relative frequency interpretation views probability not as a purely theoretical quantity, but as a property that emerges through repetition. Under this interpretation, probability is tied directly to observable data and is meaningful only in contexts where an experiment can, at least in principle, be repeated many times. Rather than assuming equally likely outcomes, this approach defines probability through empirical behavior. Probabilities are not assigned in advance; they are revealed through repeated trials. 5.2.2.1 History The relative frequency interpretation developed primarily in the 19th and early 20th centuries, alongside advances in experimental science and statistics. Key features of its historical development: Closely associated with scientists such as John Venn and Richard von Mises Motivated by the need to describe probabilities in physical and social phenomena Emphasized objectivity through repeatable experiments This interpretation arose as a response to the limitations of classical probability, especially in situations lacking symmetry or equally likely outcomes. 5.2.2.2 Core Idea: Long‑Run Behavior Under the relative frequency interpretation: Probability is not about a single trial Probability is about what happens in the long run A single coin toss provides no meaningful probability information. Thousands of tosses, however, reveal stable patterns. This interpretation implicitly assumes: The experiment can be repeated indefinitely The underlying mechanism remains stable over time 5.2.2.3 Probability as an Approximation In practice, probabilities are approximated, not known exactly. The relative frequency of an event after \\(n\\) trials is: \\[ \\text{Relative Frequency} = \\frac{\\text{Number of times the event occurs}}{n} \\] Simulation provides a natural way to observe this idea. p &lt;- 0.4 n &lt;- 1000 samCoi &lt;- sample(c(&quot;Heads&quot;, &quot;Tails&quot;), size = n, replace = TRUE, prob = c(p, 1 - p)) relFreHea &lt;- sum(samCoi == &quot;Heads&quot;) / n print(relFreHea) ## [1] 0.412 Each execution of this code generates a different result, but as the number of trials increases, the relative frequency tends to stabilize near a constant value. This stabilization is an empirical manifestation of the Law of Large Numbers that we will talk about later. 5.2.2.4 Interpretation and Modeling Perspective From a modeling standpoint, the relative frequency interpretation treats probability as a feature of the data‑generating process. The model is validated not by symmetry or logic alone, but by consistency between: simulated behavior, observed data, and long‑run frequencies. Probability models under this interpretation are often assessed by asking: Do simulated frequencies resemble observed frequencies? Does the model reproduce stable long‑run patterns? 5.2.2.5 Strengths and Limitations Strengths Grounded in observable data Does not require equally likely outcomes Naturally compatible with simulation and computation Limitations Requires repeatable experiments Not applicable to one‑time events (e.g., a specific election outcome) Does not directly address uncertainty in small samples In summary, the relative frequency interpretation reframes probability as a long‑run empirical regularity. It forms the conceptual foundation for simulation‑based probability and provides a bridge between abstract models and observed data. Here is an expanded, textbook‑style version of the Subjective Interpretation, written fully in R Markdown, consistent with your formatting rules and the level of depth used for the Classical and Relative Frequency sections. It emphasizes modeling, information, and belief, with a clear conceptual narrative. 5.2.3 Subjective Interpretation Definition 5.7 (Subjective Probability) Subjective probability represents an individual’s degree of belief about the occurrence of an event, given the information available to them. The subjective interpretation treats probability as a measure of uncertainty based on information, judgment, and belief, rather than symmetry or long‑run frequency. Under this view, probability quantifies how strongly an individual believes an event will occur, conditional on what they currently know. Unlike classical or relative frequency interpretations, subjective probability does not require repeatable experiments or equally likely outcomes. Instead, it is fundamentally conditional on information and is allowed to change as new information becomes available. 5.2.3.1 History The subjective interpretation developed primarily in the 20th century and is closely associated with the rise of Bayesian statistics. Key contributors include: Frank Ramsey, who linked probability to rational decision‑making Bruno de Finetti, who argued that “probability does not exist” outside personal belief Later formalization within Bayesian inference and decision theory This interpretation arose from the need to reason probabilistically about: unique or one‑time events, decision‑making under uncertainty, situations where long‑run frequencies are unavailable or meaningless. 5.2.3.2 Core Idea: Probability as Information‑Dependent Belief Under the subjective interpretation: Probability is always conditional on available information. Different individuals may assign different probabilities to the same event. Updating beliefs in light of new data is a central feature. From a modeling perspective, a subjective probability model represents the analyst’s state of knowledge rather than an objective property of the physical world. As information changes, probabilities are updated accordingly, typically using Bayes’ theorem. 5.2.3.3 Example: Betting Interpretation One way to operationalize subjective probability is through betting behavior. If you are willing to bet $70 to win $100 on an event occurring, your implied probability is: 70 / 100 ## [1] 0.7 This value represents the minimum probability at which you consider the bet fair, given your beliefs and risk tolerance. In this framework: Probabilities reflect willingness to accept risk Inconsistent probabilities can lead to guaranteed losses (Dutch books) Rational probability assignments are those that avoid sure loss 5.2.3.4 Subjective Probability as a Model In subjective probability, a probability model represents beliefs about how the world operates, not just observed frequencies. These models are often used when: events are unique (e.g., an election outcome), data are scarce or incomplete, decisions must be made before data are observed. Such models are evaluated not by long‑run frequencies, but by: internal coherence, consistency with observed evidence, usefulness for decision‑making. 5.2.3.5 Strengths and Limitations Strengths Applicable to one‑time or non‑repeatable events Naturally incorporates prior knowledge and expert judgment Provides a coherent framework for updating beliefs Limitations Depends on subjective judgment Different individuals may disagree Requires careful justification of assumptions In summary, the subjective interpretation views probability as a quantitative expression of uncertainty given information. It plays a central role in Bayesian modeling, where probability models evolve as data are observed, and inference is understood as a process of belief updating rather than long‑run stabilization. 5.3 How to Build Probability Models 5.3.1 Finding the Probability of an Event Definition 5.8 (Probability of an Event) The probability of an event is the sum of the probabilities of the outcomes that compose the event. In probability theory, events are not primitive objects. Instead, they are built from outcomes, which represent the most basic possible results of a random experiment. A probability model assigns probabilities to these outcomes, and the probability of any event is obtained by aggregating the probabilities of the outcomes that form the event. From a modeling perspective, this definition emphasizes that probability is additive over disjoint outcomes. Once the model specifies probabilities at the outcome level, probabilities for more complex events follow automatically. In classical settings, this is achieved through counting. In simulation‑based approaches, this aggregation is approximated empirically. 5.3.1.1 Simulation Approach to Event Probability Rather than computing probabilities analytically, we can estimate probabilities through repeated simulation. For example, consider estimating the probability of rolling a 6 on a fair die. mean(sample(1:6, size = 10000, replace = TRUE) == 6) ## [1] 0.1629 Each simulation run produces a slightly different value, but as the number of trials increases, the estimate stabilizes. This illustrates how probability emerges as a long‑run average behavior of a model, rather than a single deterministic calculation. This computational viewpoint aligns naturally with the relative frequency interpretation of probability. 5.3.2 Basic Event Relations and Probability Laws The power of probability models lies not only in assigning probabilities to single events, but in understanding how events combine and relate to one another. These relationships are governed by fundamental probability laws that hold regardless of the specific model used. 5.3.2.1 Mutually Exclusive Events Definition 5.9 (Mutually Exclusive Events) Two events are mutually exclusive if they cannot occur at the same time. Mutually exclusive events share no common outcomes. If one event occurs, the other cannot. Examples: Rolling a die and obtaining a 1 versus obtaining a 2 Drawing a card that is a heart versus drawing a card that is a spade Because these events do not overlap, their probabilities combine in a particularly simple way. Definition 5.10 (Probability of Mutually Exclusive Events) If A and B are mutually exclusive, then P(A ∪ B) = P(A) + P(B). This property follows directly from the definition of probability as additive over disjoint outcomes. 5.3.2.2 Simulation Example Estimate the probability that a fair die roll results in either a 1 or a 2. dieRol &lt;- sample(1:6, 10000, TRUE) pro12 &lt;- mean(dieRol %in% c(1, 2)) pro1 &lt;- mean(dieRol == 1) pro2 &lt;- mean(dieRol == 2) print(paste0(&quot;Probability of 1: &quot;, pro1)) ## [1] &quot;Probability of 1: 0.1712&quot; print(paste0(&quot;Probability of 2: &quot;, pro2)) ## [1] &quot;Probability of 2: 0.1663&quot; print(paste0(&quot;Probability of 1 and 2: &quot;, pro12)) ## [1] &quot;Probability of 1 and 2: 0.3375&quot; This probability is approximately twice the probability of a single outcome because the events are mutually exclusive. 5.3.2.3 Complement Events Definition 5.11 (Complement of an Event) The complement of an event A consists of all outcomes that are not in A. Every event has a complement, representing the event “not A.” Together, an event and its complement exhaust the entire sample space. Definition 5.12 (Probability of the Complement) P(Aᶜ) = 1 − P(A) This identity is fundamental and does not depend on assumptions such as equal likelihood or independence. It follows from the fact that the total probability of the sample space is 1. 5.3.2.4 Simulation Example Estimate the probability of not rolling a 1 on a fair die. pA &lt;- mean(sample(1:6, 10000, TRUE) == 1) print(paste0(&quot;Probability of rolling a 1 : &quot;, pA)) ## [1] &quot;Probability of rolling a 1 : 0.1655&quot; print(paste0(&quot;Probability of not rolling a 1 : &quot;, 1 - pA)) ## [1] &quot;Probability of not rolling a 1 : 0.8345&quot; Using the complement is often computationally simpler, especially when the event of interest is complicated but its complement is not. 5.3.3 Properties of Probability Probability models are governed by a small set of foundational properties that ensure internal consistency. Definition 5.13 (Probability Model Properties) A probability model must satisfy: 0 ≤ P(A) ≤ 1 P(S) = 1 If A and B are mutually exclusive, P(A ∪ B) = P(A) + P(B) These axioms define what it means for a function to be a valid probability measure. All other probability rules can be derived from them. 5.3.3.1 Unions and Intersections of Events Definition 5.14 (Union of Events) The union of events A and B consists of all outcomes that are in A or B (or both). Definition 5.15 (Intersection of Events) The intersection of events A and B consists of all outcomes that are in both A and B. Unlike mutually exclusive events, most events overlap, meaning their intersection is not empty. In such cases, naïvely adding probabilities would double‑count shared outcomes. Definition 5.16 (Probability of the Union) P(A ∪ B) = P(A) + P(B) − P(A ∩ B) This formula corrects for double counting and is essential for building probability models in realistic settings. 5.3.3.2 Modeling Perspective From a modeling standpoint, these properties: ensure internal consistency of probability assignments, allow complex event probabilities to be constructed from simpler ones, connect analytical probability laws with simulation‑based estimation. In computational probability, unions, intersections, and complements are implemented using logical operations on simulated data, reinforcing the interpretation of probability models as data‑generating mechanisms. 5.3.4 Conditional Probability Definition 5.17 (Conditional Probability) The conditional probability of an event A given that event B has occurred is \\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad \\text{provided } P(B) &gt; 0. \\] Conditional probability formalizes the idea of updating probabilities when additional information is available. Rather than asking whether event A occurs in general, we now ask whether A occurs given that B has already occurred. From a modeling perspective, conditioning restricts attention to a reduced sample space: only those outcomes consistent with event B are considered possible. Probabilities are then rescaled so that this restricted space has total probability 1. In this way, conditional probability connects probability models to the flow of information. 5.3.4.1 Interpretation via Simulation Consider a fair die. Let: A = {the outcome is 4} B = {the outcome is greater than 3} Rather than reasoning abstractly, we estimate this conditional probability by simulation. dieRol &lt;- sample(1:6, 10000, TRUE) proCon &lt;- mean(dieRol == 4 &amp; dieRol &gt; 3) / mean(dieRol &gt; 3) print(paste0(&quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: &quot;, proCon)) ## [1] &quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: 0.322678535302448&quot; Here: dieRol &gt; 3 identifies the outcomes consistent with the condition B, dieRol == 4 &amp; dieRol &gt; 3 identifies outcomes where both A and B occur. The ratio estimates the probability of rolling a 4 given that the roll exceeds 3. This computational approach makes the conditioning mechanism explicit: we focus only on simulated outcomes where B occurs and examine how often A appears within that subset. 5.3.4.2 Conditional Probability as a Modeling Tool Conditional probabilities are central to probabilistic modeling because they allow us to express structured dependence between events. In complex systems, probabilities rarely exist in isolation; they reflect assumptions about what is known and unknown. Practically, conditional probabilities help answer questions of the form: What is the chance of A when we know B has happened? How does information change our expectations? Definition 5.18 (Marginal Probability) A marginal (or unconditional) probability is the probability of an event considered without conditioning on any other event. The marginal probability describes the baseline likelihood of an event across the entire sample space, before incorporating additional information. For the same die example, the marginal probability of rolling a 4 is: proMar &lt;- mean(die == 4) print(paste0(&quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: &quot;, proCon)) ## [1] &quot;Conditional probability of rolling a 4 given that a number greater than 3 is rolled: 0.322678535302448&quot; print(paste0(&quot;Marginal probability of rolling a 4: &quot;, proMar)) ## [1] &quot;Marginal probability of rolling a 4: 0.1595&quot; Comparing marginal and conditional probabilities highlights how information can increase, decrease, or leave unchanged the likelihood of an event. 5.3.5 Law of Total Probability Definition 5.19 (Law of Total Probability) If events \\(B_1, B_2, \\dots, B_n\\) form a partition of the sample space, then \\[ P(A) = \\sum_{i=1}^{n} P(A \\mid B_i)\\,P(B_i). \\] The Law of Total Probability provides a way to compute the probability of an event by decomposing it into simpler scenarios. Each event \\[B_i\\] represents a mutually exclusive and exhaustive case under which A might occur. From a modeling perspective, this law expresses how overall probability emerges from conditional structure. It is especially useful when direct computation of \\[P(A)\\] is difficult, but conditional probabilities are easier to specify or estimate. 5.3.6 Bayes’ Formula Definition 5.20 (Bayes' Formula) For events A and B with P(A) &gt; 0, \\[ P(B \\mid A) = \\frac{P(A \\mid B)\\,P(B)}{P(A)}. \\] Bayes’ formula reverses the direction of conditioning. Instead of computing the probability of observing A given B, it computes the probability of B given that A has been observed. This inversion is fundamental to Bayesian modeling, where probabilities are updated as data arrive. Prior beliefs \\[P(B)\\] are combined with evidence \\[P(A \\mid B)\\] to produce updated beliefs \\[P(B \\mid A)\\]. 5.3.6.1 Simulation Example: Medical Testing We illustrate Bayes’ formula using a diagnostic testing scenario. # Bayes example via simulation proInf &lt;- 0.01 ## Sample disease status disease &lt;- sample(c(TRUE, FALSE), 100000, TRUE, prob = c(proInf, 1 - proInf)) ## Sample test results given disease status sen &lt;- 0.95 spe &lt;- 0.05 test &lt;- ifelse(disease, sample(c(TRUE,FALSE), 100000, TRUE,prob=c(sen, 1 - sen)), sample(c(TRUE,FALSE), 100000, TRUE,prob=c(1- spe, spe))) mean(disease[test]) ## [1] 0.01005025 In this simulation: Only 1% of individuals have the disease, The test is accurate but imperfect, We compute the probability of disease given a positive test result. Despite high test accuracy, the conditional probability may be much lower than expected due to the rarity of the disease. This illustrates how conditioning can dramatically change intuitive conclusions. Definition 5.21 (States of Nature and Observable Events) States of nature represent the true but unobserved condition of a system, while observable events are the data or measurements produced by that system. Bayesian reasoning explicitly distinguishes between: states of nature, which are unknown but real, observable events, which provide indirect information about those states. Conditional probability provides the mathematical bridge between what we observe and what we wish to infer. 5.3.7 Modeling Perspective Summary Conditional probability is not merely a computational rule—it is a modeling principle. It encodes how information alters uncertainty, enables the construction of hierarchical and Bayesian models, and connects probability theory directly to inference. In simulation‑based approaches, conditioning is implemented by subsetting data and renormalizing, reinforcing the idea that probability models are best understood as data‑generating mechanisms shaped by information. 5.4 Summary Probability models are data‑generating mechanisms Simulation connects theory to observable behavior Different interpretations serve different purposes Conditional probability and Bayes’ formula link belief, data, and inference "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
