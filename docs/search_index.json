[["index.html", "STAT 5428/6428 Spring 2026 1 Stat 5428/6428 Spring 2026 1.1 Weekly Course Outline 1.2 Topical Learning Outcomes (Aligned with Bloom’s Taxonomy) 1.3 Course Overview 1.4 Course Perspective", " STAT 5428/6428 Spring 2026 Rene Gutierrez University of Texas at El Paso 1 Stat 5428/6428 Spring 2026 This is the website for STAT 5428/6428. It will contain the relevant information for the course and lecture notes and code seen in class. Find the syllabus here: Syllabus Find the github page for the course (with the data sets) here: Github Find the 1.1 Weekly Course Outline Introduction to Statistical Thinking and Data Analysis Role of statistics in research; types of data; study design; observational versus experimental studies; introduction to statistical software (R). Descriptive Statistics and Exploratory Data Analysis (EDA) Numerical summaries; measures of center and variability; graphical methods; data visualization; exploratory data analysis workflow. Probability Concepts and Random Variables Basic probability rules; discrete and continuous random variables; common distributions; expectation and variance. Sampling Distributions and Simulation Concept of sampling distributions; Central Limit Theorem; simulation‑based illustration; intuition for confidence levels and statistical power. Statistical Inference I: Estimation and Confidence Intervals Point estimation; confidence intervals for means and proportions; interpretation and limitations; assumptions and robustness. Statistical Inference II: Hypothesis Testing Null and alternative hypotheses; test statistics; p‑values; Type I/II errors; power considerations and sample size intuition. Inference for Two Populations Two‑sample CIs and tests; paired designs; comparing means and variances; parametric vs. nonparametric choices. Analysis of Variance (ANOVA) One‑way ANOVA; model formulation; assumptions; multiple comparisons; practical interpretation and reporting. Analysis of Covariance (ANCOVA) Incorporating quantitative covariates; adjusted means; interpretation; assumption checks and diagnostics. Simple and Multiple Linear Regression Least squares estimation; interpretation of coefficients; model fit; inference for regression parameters. Regression Diagnostics and Model Assessment Residual analysis; outliers and influence; multicollinearity; remedial measures (transformations, variable selection). Logistic Regression and Categorical Data Analysis Binary response models; odds, odds ratios, and log‑odds; goodness‑of‑fit; interpreting software output. Nonparametric Methods and Alternatives Rank‑based tests and other nonparametric procedures; when and why to choose nonparametric approaches. Experimental Design and Model Selection Principles of randomization, blocking, replication; power and sample size concepts; model selection strategies; reproducibility. Integration, Applications, and Communication of Results Case studies with real datasets; end‑to‑end analysis; ethical considerations; transparent reporting; course review. 1.2 Topical Learning Outcomes (Aligned with Bloom’s Taxonomy) Topic Bloom’s Level Learning Outcome Descriptive Statistics Understand / Apply Summarize and visualize datasets using appropriate numerical and graphical descriptive measures. Probability Concepts Understand Explain fundamental probability concepts and their role in statistical modeling and inference. Sampling Distributions Analyze Analyze sampling variability and illustrate key ideas via simulation. Confidence Intervals Apply / Analyze Construct and interpret confidence intervals; assess assumptions and practical implications. Hypothesis Testing Analyze / Evaluate Evaluate hypotheses using appropriate tests and interpret results in context. Two‑Sample Inference Apply / Analyze Compare two populations using parametric and nonparametric procedures. ANOVA / ANCOVA Analyze Analyze group differences with ANOVA/ANCOVA, incorporating covariates where appropriate. Linear Regression Apply / Analyze Develop and interpret simple and multiple linear regression models. Model Diagnostics Evaluate Assess model assumptions; identify influence and multicollinearity; implement remedial actions. Logistic Regression Analyze Model binary outcomes; interpret odds ratios and fitted model diagnostics. Nonparametric Methods Evaluate Justify and apply nonparametric alternatives when assumptions are violated. Experimental Design Create / Evaluate Design and critique basic experiments (randomization, blocking, replication). Statistical Software (R) Apply / Create Use R to perform analyses, run simulations, interpret output, and communicate results clearly. 1.3 Course Overview Introduction to Statistical Analysis is a graduate‑level course that develops statistical thinking as an integrated process of description, modeling, inference, and revision. The course is structured around the view that statistical analysis is not a linear application of techniques, but an iterative framework in which data are explored, probabilistic assumptions are imposed, conclusions are drawn under those assumptions, and models are reassessed in light of empirical evidence. Emphasis is placed on conceptual understanding, methodological coherence, and interpretation, rather than on formal mathematical derivations. The central organizing principle of the course is the relationship between data and probability models. Data are treated as realizations from an underlying stochastic process, and statistical methods are developed as tools for learning about that process. Throughout the course, students use statistical software (R) to connect theory with practice and to evaluate the adequacy of models when applied to real data. The course is organized into four major sections, each corresponding to a distinct stage of statistical reasoning. 1.3.1 Section I: Data — Description and Intrinsic Properties Purpose: Characterizing data without imposing a probabilistic model The first section focuses on understanding data in its raw form. Prior to modeling or inference, data must be described, explored, and contextualized. This section emphasizes the intrinsic properties of data that can be observed directly, without reference to stochastic assumptions. Key objectives include identifying structure, detecting anomalies, and understanding variability as an empirical phenomenon. Topics in this section include: - Nature and structure of data (measurement scales, data types) - Observational versus experimental data - Study design and sources of variation - Descriptive statistics (location, dispersion, shape) - Graphical summaries and data visualization - Exploratory Data Analysis (EDA) This section establishes descriptive analysis as a critical and non‑optional stage of any sound statistical investigation. 1.3.2 Section II: Data Modeling — Probability as a Generative Mechanism Purpose: Modeling data as realizations of a stochastic process In the second section, probability is introduced as a modeling language for data generation. Rather than focusing on probability as an abstract mathematical subject, it is framed as a tool for encoding assumptions about how data arise. Students study theoretical distributions and sampling behavior implied by different probabilistic models, and they examine the consequences of these assumptions. Topics in this section include: - Basic probability concepts and axioms - Random variables and probability distributions - Discrete and continuous models - Expectation and variance - Sampling distributions - The Central Limit Theorem - Simulation‑based exploration of theoretical results This section provides the probabilistic foundation upon which all subsequent inferential procedures are built. 1.3.3 Section III: Statistical Inference — Learning from Data Under a Model Purpose: Drawing conclusions using probabilistic models The third section develops formal statistical inference as a logical extension of probability modeling. Conditional on an assumed data‑generating mechanism, students derive and apply methods for estimation, hypothesis testing, and uncertainty quantification. Inference is presented as fundamentally model‑dependent, and interpretation is emphasized over mechanical computation. Topics in this section include: - Point estimation and estimators - Confidence intervals and coverage interpretation - Hypothesis testing frameworks - Test statistics and p‑values - Type I and Type II errors - Power and sample size considerations - Inference for one and two populations - Introduction to ANOVA as a comparative inferential framework Students learn to answer substantive research questions while explicitly acknowledging the assumptions that justify their conclusions. 1.3.4 Section IV: Data Remodeling — Model Assessment, Revision, and Alternatives Purpose: Validating conclusions and revisiting assumptions The final section returns to the modeling stage with a critical perspective. After conducting inference, students evaluate whether the probabilistic assumptions underlying their analyses are empirically reasonable. Model adequacy is assessed using diagnostics, and alternative approaches are considered when assumptions fail. This section emphasizes that statistical modeling is iterative and subject to revision rather than a one‑time choice. Topics in this section include: - Linear and multiple regression models - Logistic regression and categorical responses - Residual analysis and goodness‑of‑fit diagnostics - Influential observations and multicollinearity - Remedial measures and transformations - Nonparametric alternatives - Model selection strategies - Interpretation under updated or revised assumptions This section reinforces the idea that statistical conclusions are conditional statements whose validity rests on the suitability of the modeling framework. 1.4 Course Perspective Across all four sections, the course emphasizes coherence between data, models, inference, and validation. By the end of the course, students will be able to approach data analysis as a structured yet flexible process—one that integrates empirical evidence, probabilistic reasoning, and critical evaluation in order to draw defensible conclusions from data. "],["intro-to-r.html", "2 Intro to R 2.1 Basic R Operations and Concepts 2.2 Arithmetic 2.3 Assignment, Object Names, and Data Types 2.4 Vectors 2.5 Functions and Expressions 2.6 Getting Help", " 2 Intro to R These notes introduce core R concepts you’ll use throughout the course. Each section includes short explanations and runnable code. Tip: You can run code line-by-line in RStudio with Ctrl+Enter (Windows) / Cmd+Enter (Mac). 2.1 Basic R Operations and Concepts R works like a powerful calculator and a programming language. You type commands into the Console, and R evaluates them. 2.1.1 The Console and Scripts Console: where commands run immediately. Script (.R) / R Markdown (.Rmd): where you write reproducible work you can save and re-run. 2.1.2 Comments Use # to write comments. R ignores them. # This is a comment 2 + 2 ## [1] 4 2.1.3 Printing and Output Typing an object name prints it. You can also use print(). x &lt;- 10 x ## [1] 10 print(x) ## [1] 10 2.2 Arithmetic R supports standard arithmetic operators: Addition: + Subtraction: - Multiplication: * Division: / Exponentiation: ^ Integer division: %/% Remainder (mod): %% # Basic arithmetic 5 + 3 ## [1] 8 10 - 4 ## [1] 6 6 * 7 ## [1] 42 20 / 5 ## [1] 4 # Powers 2^5 ## [1] 32 # Integer division and remainder 17 %/% 3 ## [1] 5 17 %% 3 ## [1] 2 2.2.1 Order of Operations R follows standard order of operations. Use parentheses to be explicit. 3 + 2 * 5 # multiplication first ## [1] 13 (3 + 2) * 5 # parentheses first ## [1] 25 2.2.2 Logical Comparisons These return TRUE or FALSE. 5 &gt; 3 ## [1] TRUE 5 == 3 ## [1] FALSE 5 != 3 ## [1] TRUE 5 &gt;= 5 ## [1] TRUE 2.3 Assignment, Object Names, and Data Types 2.3.1 Assignment Use &lt;- (common in R code) or =. a &lt;- 5 b = 12 a + b ## [1] 17 2.3.2 Object Names Good names are readable and informative. Valid examples height exam_score x1 Avoid spaces (exam score) starting with numbers (1st) reserved words (if, for, TRUE, FALSE) exam_score &lt;- 92 final_grade &lt;- &quot;A&quot; 2.3.3 Common Data Types 2.3.3.1 Numeric (double) x &lt;- 3.14 typeof(x) ## [1] &quot;double&quot; class(x) ## [1] &quot;numeric&quot; 2.3.3.2 Integer y &lt;- 7L typeof(y) ## [1] &quot;integer&quot; class(y) ## [1] &quot;integer&quot; 2.3.3.3 Character (strings) name &lt;- &quot;Rene&quot; typeof(name) ## [1] &quot;character&quot; class(name) ## [1] &quot;character&quot; 2.3.3.4 Logical flag &lt;- TRUE typeof(flag) ## [1] &quot;logical&quot; class(flag) ## [1] &quot;logical&quot; 2.3.4 3.4 Missing Values R uses NA for missing data. z &lt;- c(1, NA, 3) z ## [1] 1 NA 3 is.na(z) ## [1] FALSE TRUE FALSE NA affects calculations unless you remove missing values. mean(z) # returns NA ## [1] NA mean(z, na.rm = TRUE) # removes NA before computing mean ## [1] 2 2.4 Vectors Vectors are the basic building block in R: an ordered collection of values. 2.4.1 Creating Vectors with c() v &lt;- c(2, 4, 6, 8) v ## [1] 2 4 6 8 2.4.2 Vector Length length(v) ## [1] 4 2.4.3 Vector Arithmetic (Vectorized Operations) Operations apply element-by-element. v + 1 ## [1] 3 5 7 9 v * 2 ## [1] 4 8 12 16 v^2 ## [1] 4 16 36 64 2.4.4 Sequences with : and seq() 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(from = 0, to = 1, by = 0.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 seq(from = 1, to = 10, length.out = 5) ## [1] 1.00 3.25 5.50 7.75 10.00 2.4.5 Repetition with rep() rep(5, times = 4) ## [1] 5 5 5 5 rep(c(&quot;A&quot;,&quot;B&quot;), times = 3) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; rep(1:3, each = 2) ## [1] 1 1 2 2 3 3 2.4.6 Indexing Vectors 2.4.6.1 By position v &lt;- c(10, 20, 30, 40, 50) v[1] # first element ## [1] 10 v[3] # third element ## [1] 30 v[c(2,5)] # second and fifth ## [1] 20 50 2.4.6.2 By negative indexing (remove elements) v[-1] # all but first ## [1] 20 30 40 50 v[-c(2,4)] ## [1] 10 30 50 2.4.6.3 By logical indexing v[v &gt; 25] # keep values greater than 25 ## [1] 30 40 50 v[v == 20] # values equal to 20 ## [1] 20 2.4.7 Named Vectors Names make code easier to read. grades &lt;- c(Midterm = 88, Final = 93, Project = 90) grades ## Midterm Final Project ## 88 93 90 grades[&quot;Final&quot;] ## Final ## 93 2.5 Functions and Expressions 2.5.1 Functions R has many built-in functions: mean(), sum(), sd(), etc. x &lt;- c(2, 4, 6, 8, 10) sum(x) ## [1] 30 mean(x) ## [1] 6 sd(x) ## [1] 3.162278 min(x) ## [1] 2 max(x) ## [1] 10 2.5.1.1 Function arguments Arguments control how the function behaves. round(3.14159, digits = 2) ## [1] 3.14 mean(c(1, NA, 3), na.rm = TRUE) ## [1] 2 2.5.2 Expressions and Nesting You can combine (nest) functions. x &lt;- c(1, 2, 3, 4, 5) sqrt(sum(x^2)) ## [1] 7.416198 2.5.3 Creating Your Own Function Use function() to define reusable code. # A simple function: compute z-scores zscore &lt;- function(x) { (x - mean(x)) / sd(x) } zscore(c(10, 12, 15, 20)) ## [1] -0.9771621 -0.5173211 0.1724404 1.3220429 2.5.3.1 Another example: a summary function quick_summary &lt;- function(x) { c( n = length(x), mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), min = min(x, na.rm = TRUE), max = max(x, na.rm = TRUE) ) } quick_summary(c(1, 2, 3, NA, 5)) ## n mean sd min max ## 5.000000 2.750000 1.707825 1.000000 5.000000 2.6 Getting Help R has excellent built-in help tools. 2.6.1 Help Pages Use ? or help(). ?mean help(sd) 2.6.2 Examples in Help Files Many help pages include examples you can run: example(mean) ## ## mean&gt; x &lt;- c(0:10, 50) ## ## mean&gt; xm &lt;- mean(x) ## ## mean&gt; c(xm, mean(x, trim = 0.10)) ## [1] 8.75 5.50 2.6.3 Searching for a Function If you don’t know the exact name, use help.search() or ??. ??regression help.search(&quot;histogram&quot;) 2.6.4 Inspecting Objects x &lt;- rnorm(5) str(x) ## num [1:5] 0.3345 -0.9626 -1.5977 -0.8596 -0.0139 class(x) ## [1] &quot;numeric&quot; typeof(x) ## [1] &quot;double&quot; ls() # list objects in your environment ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;exam_score&quot; ## [5] &quot;final_grade&quot; &quot;flag&quot; &quot;grades&quot; &quot;height1&quot; ## [9] &quot;height2&quot; &quot;name&quot; &quot;quick_summary&quot; &quot;v&quot; ## [13] &quot;x&quot; &quot;x_hat&quot; &quot;x_hat_1&quot; &quot;xm&quot; ## [17] &quot;y&quot; &quot;z&quot; &quot;zscore&quot; rm(x) # remove x ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;exam_score&quot; ## [5] &quot;final_grade&quot; &quot;flag&quot; &quot;grades&quot; &quot;height1&quot; ## [9] &quot;height2&quot; &quot;name&quot; &quot;quick_summary&quot; &quot;v&quot; ## [13] &quot;x_hat&quot; &quot;x_hat_1&quot; &quot;xm&quot; &quot;y&quot; ## [17] &quot;z&quot; &quot;zscore&quot; 2.6.5 Getting Package Help If you use a package, load it first (if installed). # install.packages(&quot;ggplot2&quot;) # run once if needed library(ggplot2) ?ggplot "],["observational-and-experimental-studies.html", "3 Observational and Experimental Studies 3.1 Introduction 3.2 Surveys 3.3 Experimental Studies 3.4 Observational vs Experimental Studies 3.5 Data Quality and Measurement 3.6 Summary", " 3 Observational and Experimental Studies 3.1 Introduction Reliable statistical inference depends critically on the quality and structure of the data collected. This chapter introduces two major frameworks for gathering data: surveys and experimental studies. These methods differ in their level of control, susceptibility to bias, and the strength of the conclusions permitted. This chapter corresponds to the Data stage of the course workflow: collecting, describing data, understanding structure, and identifying intrinsic properties before modeling. 3.2 Surveys Surveys are systematic procedures for obtaining information from a sample that represents a broader population. Effective survey design minimizes bias and ensures appropriate representation. 3.2.1 Populations and Sampling Frames Population: the complete set of units of interest. Sampling frame: the operational list from which the sample is drawn. A mismatch between population and frame leads to coverage error. 3.2.2 Sampling Methods 3.2.2.1 Simple Random Sampling (SRS) Every subset of a given size has an equal chance of being selected. Advantages: - Unbiased representation. We will see what do we mean by this later. - Straightforward design and analysis. 3.2.2.2 Stratified Sampling The population is divided into homogeneous groups (strata), and SRS is applied within each. - Reduces sampling variability - Ensures representation of important subpopulations 3.2.2.3 Cluster Sampling Naturally occurring groups (clusters) are sampled instead of individuals. - Efficient for large geographic populations - Requires careful design to ensure representativeness 3.2.2.4 Systematic Sampling Select every kth element after a random start. - Easy to implement - Sensitive to hidden periodic patterns 3.2.3 Sources of Survey Bias 3.2.3.1 Selection Bias The sample differs systematically from the population. 3.2.3.2 Nonresponse Bias Failure to obtain measurements from selected units creates distortion. 3.2.3.3 Response Bias Arises from poor question wording, respondent discomfort, or interviewer influence. 3.2.4 Questionnaire Design Principles Use clear, neutral language Avoid leading or double‑barreled questions Choose appropriate measurement scales: Nominal Ordinal Interval Ratio 3.3 Experimental Studies Experiments involve deliberate manipulation of conditions to identify causal effects. They provide control over sources of variability through specific design features. 3.3.1 Key Components of Experimental Design 3.3.1.1 Experimental Units Entities to which treatments are applied. 3.3.1.2 Treatments and Control Interventions or conditions imposed on units; control group provides a baseline. 3.3.1.3 Random Assignment Distributes potential confounders across treatment groups. 3.3.1.4 Replication Multiple units exposed to each treatment enable the estimation of experimental variability. 3.3.1.5 Blocking Grouping similar units to reduce the influence of nuisance factors. 3.3.2 Confounding A confounding variable affects both treatment assignment and response, obscuring causal relationships. Good experimental design aims to reduce confounding through: - Randomization - Control - Proper blocking 3.4 Observational vs Experimental Studies 3.4.1 Comparative Characteristics Feature Observational Study Experimental Study Researcher control None High Confounding Often present Reduced Causal claims Weak Strong Difficulty Usually easier to conduct May be expensive or unethical 3.4.2 Interpretational Considerations Observational studies support associational conclusions. Experiments support causal conclusions, conditional on valid design. 3.5 Data Quality and Measurement 3.5.1 Validity and Reliability 3.5.1.1 Validity Whether measurement reflects the concept intended. 3.5.1.2 Reliability Consistency of repeated measurements under identical conditions. 3.5.2 Sources of Variability Natural biological or environmental variation Measurement error Experimental or observational conditions Sampling variability Reducing uncontrolled variation increases the precision and reliability of statistical findings. 3.6 Summary This chapter establishes essential principles for gathering high‑quality data. Surveys and experiments differ in control structure, design complexity, and inferential capability, but both aim to produce data suitable for statistical reasoning. In the next chapter, we transition to Data Modeling, where probability is introduced as a generative mechanism to describe theoretical data behavior. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
