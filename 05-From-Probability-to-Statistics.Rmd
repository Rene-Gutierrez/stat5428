# From Probability to Statistics

Probability provides a mathematical language for describing uncertainty. Statistics uses that language to learn from data.

The transition from probability to statistics occurs when we reinterpret probability models as descriptions of populations and data-generating processes. Instead of asking:

> “What is the probability of an event?”

we begin asking:

> “What can we learn about an unknown population from a random sample?”

This shift is conceptual but fundamental.

---

## Population as a Distribution

A coherent way to unify probability and statistics is to model the **population** as a probability distribution.

In this framework:

* The population is not just a collection of numbers.
* It is a **data-generating mechanism** governed by a probability distribution.
* Observations are random outcomes generated by that mechanism.

Formally, we think of a population as a distribution $F$ with parameters $\theta$.

Each observation is a random variable:

$$
X \sim F.
$$

A sample of size $n$ is:

$$
X_1, X_2, \dots, X_n \sim F.
$$

A statistic is a function of these random variables:

$$
T = g(X_1, \dots, X_n).
$$

The sampling distribution describes how $T$ behaves across repeated samples.

---

### The Structural Framework

We summarize the statistical framework as follows:

1. The population is modeled by a probability distribution with parameters.
2. Samples are random draws from this distribution.
3. Statistics are computed from samples.
4. Sampling distributions describe the variability of statistics.
5. Inference uses sampling distributions to learn about parameters.

This framework explains:

* Why statistics vary from sample to sample.
* Why uncertainty is unavoidable.
* Why larger samples produce more stable conclusions.

---

## Examples

### Example 1: A Normal Population

Suppose a population is modeled as:

$$
X \sim N(\mu = 10, \sigma = 2).
$$

The parameter $\mu = 10$ is fixed but unknown in practice. A sample mean $\bar{X}$ will vary across samples.

```{r}
set.seed(123)

means <- replicate(1000, mean(rnorm(20, mean = 10, sd = 2)))

hist(means, probability = TRUE, breaks = 30,
     main = "Sampling Distribution of the Mean")
abline(v = 10, col = 2, lwd = 2)
```

The vertical line marks the population mean. The histogram shows how sample means fluctuate around it.

---

### Example 2: A Discrete Population

Let the population be Bernoulli:

$$
X \sim \text{Bernoulli}(p = 0.3).
$$

```{r}
means <- replicate(2000, mean(rbinom(30, 1, 0.3)))
hist(means, probability = TRUE, breaks = 30)
```

Even though the population is discrete, the sampling distribution of the mean appears approximately continuous and bell-shaped.

---

## What Does Bias Mean Under This Framework?

Bias refers to systematic deviation from a parameter.

For an estimator $\hat{\theta}$:

$$
\text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta.
$$

An estimator is **unbiased** if:

$$
E[\hat{\theta}] = \theta.
$$

Example: The sample mean.

```{r}
true_mean <- 5
means <- replicate(5000, mean(rnorm(20, mean = true_mean)))
mean(means)
```

The average of the sample means is very close to the true mean. The sample mean is unbiased.

Bias is therefore defined in terms of the **sampling distribution**, not a single dataset.

---

## Role of Independence

Independence is foundational because it simplifies probability structure.

If $X_1, \dots, X_n$ are independent:

* Joint probabilities factorize.
* Variances add cleanly.
* Limit theorems apply.
* Sampling distributions behave predictably.

### Independent Sampling (With Replacement)

```{r}
sample1 <- sample(1:100, 50, replace = TRUE)
```

Each draw does not affect future draws.

### Deterministic Sequence (No Randomness)

```{r}
sample2 <- 1:50
```

There is no randomness here. No sampling distribution exists.

### Dependent Example: Random Walk

```{r}
x <- cumsum(rnorm(1000))
plot(x, type = "l")
mean(x)
```

Each observation depends on previous ones. The usual variance formulas for independent observations do not apply directly.

Dependence changes:

* Variance accumulation
* Convergence rates
* Shape of sampling distributions

---

## Sampling as Drawing Random Variables

We model the population as a distribution $F$.

A sample of size $n$:

$$
X_1, X_2, \dots, X_n \sim F.
$$

### i.i.d. Assumption

Most theoretical results assume observations are:

* Independent
* Identically distributed

This assumption corresponds to sampling with replacement or from an infinite population.

---

### Sampling Without Replacement

If sampling from a finite population of size $N$ without replacement:

* Observations are dependent.
* They are exchangeable.
* Variance includes a finite population correction:

$$
1 - \frac{n}{N}.
$$

When $n/N$ is small, the dependence is negligible.

---

## Parameters and Statistics

A **parameter** describes a population distribution.

Examples:

* $\mu$ (mean)
* $\sigma^2$ (variance)
* $p$ (proportion)
* $\lambda$ (Poisson rate)

A **statistic** is computed from a sample.

Examples:

* $\bar{X}$
* $S^2$
* $\hat{p}$

Parameters are fixed (but unknown).
Statistics are random variables.

---

### Simulation Illustration

```{r}
set.seed(1)

true_mean <- 5
means <- replicate(1000, mean(rnorm(20, mean = true_mean)))

mean(means)
sd(means)
```

* `true_mean` is a parameter.
* Each computed mean is a statistic.
* The collection forms the sampling distribution.

---

## Sampling Distribution of a Mean

Let $X_1,\dots,X_n$ be i.i.d. with:

$$
E[X_i] = \mu, \quad \mathbb{V}(X_i) = \sigma^2.
$$

The sample mean:

$$
\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i.
$$

---

### Fundamental Properties

$$
E[\bar{X}] = \mu
$$

$$
\mathbb{V}(\bar{X}) = \frac{\sigma^2}{n}
$$

Standard error:

$$
\frac{\sigma}{\sqrt{n}}.
$$

Averaging reduces variability.

---

### Small vs Large Sample Size

```r
means_small <- replicate(1000, mean(rnorm(5)))
means_large <- replicate(1000, mean(rnorm(50)))

par(mfrow=c(1,2))
hist(means_small, probability=TRUE, main="n=5")
hist(means_large, probability=TRUE, main="n=50")
par(mfrow=c(1,1))
```

Larger samples → smaller variance.

---

### Big vs Small Variance

```r
means1 <- replicate(1000, mean(rnorm(20, sd=1)))
means2 <- replicate(1000, mean(rnorm(20, sd=5)))

par(mfrow=c(1,2))
hist(means1, probability=TRUE, main="sd=1")
hist(means2, probability=TRUE, main="sd=5")
par(mfrow=c(1,1))
```

Larger population variance → wider sampling distribution.

---

## Shape of the Sampling Distribution

If the population is normal:

* $\bar{X}$ is exactly normal.

If not:

* The Central Limit Theorem implies approximate normality for large $n$.

---

### Binomial Example

```r
means <- replicate(5000, mean(rbinom(20,1,0.5)))
hist(means, probability=TRUE)
```

---

### Poisson Example

```r
means <- replicate(5000, mean(rpois(20,3)))
hist(means, probability=TRUE)
```

---

### Normal Example

```r
means <- replicate(5000, mean(rnorm(20,0,1)))
hist(means, probability=TRUE)
```

---

## Sampling Distribution of a Variance

The sample variance:

$$
S^2 = \frac{1}{n-1}\sum (X_i-\bar{X})^2.
$$

It is also a random variable.

If the population is normal:

$$
\frac{(n-1)S^2}{\sigma^2}
$$

follows a chi-squared distribution with $n-1$ degrees of freedom.

---

### Small vs Large Sample Size

```r
vars_small <- replicate(2000, var(rnorm(5)))
vars_large <- replicate(2000, var(rnorm(50)))

par(mfrow=c(1,2))
hist(vars_small, probability=TRUE, main="n=5")
hist(vars_large, probability=TRUE, main="n=50")
par(mfrow=c(1,1))
```

---

### Big vs Small Variance

```r
vars1 <- replicate(2000, var(rnorm(20, sd=1)))
vars2 <- replicate(2000, var(rnorm(20, sd=5)))

par(mfrow=c(1,2))
hist(vars1, probability=TRUE, main="sd=1")
hist(vars2, probability=TRUE, main="sd=5")
par(mfrow=c(1,1))
```

---

## Concentration with Larger Samples

As $n$ increases, statistics stabilize.

```{r}
means <- replicate(1000, mean(rnorm(30, 0, 1)))
hist(means, probability = TRUE, breaks = 30)
```

The distribution tightens around the true mean.

---

## Central Insight

Statistics are random variables.

A statistic has:

* An expectation
* A variance
* A distribution

Statistical inference studies these sampling distributions to:

* Estimate parameters
* Quantify uncertainty
* Construct confidence intervals
* Perform hypothesis tests

Probability describes randomness.
Statistics uses that description to learn from data.

This is the conceptual bridge from probability to statistics.
